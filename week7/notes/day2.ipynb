{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4db0cc6",
   "metadata": {},
   "source": [
    "# Day 2 - How to Choose the Best Base Model for Fine-Tuning Large Language Models\n",
    "\n",
    "### Summary\n",
    "This lecture outlines the strategy for building a proprietary, specialized AI model for a price prediction task. The core idea is to fine-tune a smaller, open-source model (around 8 billion parameters) to outperform large, general-purpose \"frontier\" models like GPT-4 on this single, focused business problem. The most critical initial step is the strategic selection of a base model, weighing the trade-offs between a raw \"base\" version and a chat-optimized \"instruct\" variant.\n",
    "\n",
    "### Highlights\n",
    "* **Strategy: Specialization Over Size**: A smaller model (e.g., 8B parameters) can outperform a massive frontier model (e.g., GPT-4) on a narrow task when fine-tuned on a large, high-quality, domain-specific dataset. This is a practical and cost-effective approach for creating high-value, proprietary AI solutions.\n",
    "* **Constraint-Driven Model Selection**: The choice of an 8-billion-parameter model is a practical decision based on hardware memory limitations. This highlights a real-world data science constraint where model size is determined by available compute resources, not just desired performance.\n",
    "* **Base Model as a Hyperparameter**: The initial choice of which pre-trained model to use for fine-tuning is one of the most pivotal decisions, functioning like a \"massive hyperparameter.\" This choice will fundamentally influence the final model's performance and capabilities.\n",
    "* **Model Variants: Base vs. Instruct**: It's crucial to distinguish between two types of starting models. \"Base\" models are the raw, pre-trained versions, while \"instruct\" models have been further fine-tuned to follow conversational prompts (e.g., system/user/assistant roles).\n",
    "* **When to Use a Base Model**: The \"base\" model is generally the better starting point for a single, highly structured, and repetitive task like price prediction. It can be specialized entirely for that one purpose without the overhead of conversational instruction-following.\n",
    "* **When to Use an Instruct Model**: The \"instruct\" variant is advantageous when you want to leverage its pre-existing ability to understand roles and follow complex instructions via system prompts. This can be a shortcut to frame a task or give the model a specific persona.\n",
    "* **Importance of Benchmarking**: Before investing in a full fine-tuning process, it's essential to evaluate the off-the-shelf base model's performance on the task. This establishes a baseline and confirms that building a custom model provides a worthwhile return on investment compared to using open-source models as-is or paying for API access to frontier models.\n",
    "\n",
    "### Conceptual Understanding\n",
    "* **Base vs. Instruct Models for Fine-Tuning**\n",
    "    1.  **Why is this concept important?** The choice between a \"base\" and an \"instruct\" model is a fundamental strategic decision that dictates the format of your training data, the nature of your prompts, and the overall fine-tuning approach. Choosing incorrectly can lead to suboptimal performance and unnecessary complexity.\n",
    "    2.  **How does it connect to real-world tasks?**\n",
    "        * **Base Models** are ideal for building specialized, non-conversational tools. Examples include text classifiers, data extractors, or, as in this case, a price predictor that takes structured input and produces a structured output. You are molding the raw capabilities of the model for a single purpose.\n",
    "        * **Instruct Models** are better suited for building applications that require interaction, instruction-following, or persona-based responses. Examples include customer service chatbots, creative writing assistants, or tools that need to respond to nuanced human commands.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?**\n",
    "        * **Prompt Engineering**: Understanding how to structure prompts for both base (e.g., simple input-output examples) and instruct models (e.g., using chat templates with system/user roles).\n",
    "        * **Parameter-Efficient Fine-Tuning (PEFT)**: Techniques like LoRA are crucial for efficiently fine-tuning these large models on limited hardware.\n",
    "        * **Hugging Face Hub & Leaderboards**: Learning to navigate these resources is key to discovering, evaluating, and selecting the best possible base models for a given task.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** Which specific dataset or project could benefit from this concept?\n",
    "    * *Answer*: A project to classify legal documents into specific categories (e.g., contract, motion, pleading) could benefit. By fine-tuning a base model on a large dataset of company-specific legal documents, it could achieve higher accuracy in classifying industry-specific jargon and sentiment than a general-purpose model.\n",
    "\n",
    "2.  **Teaching:** How would you explain the difference between a base and an instruct model to a junior colleague, using one concrete example?\n",
    "    * *Answer*: Think of a base model as a raw engine you can tune for one specific purpose, like building a Formula 1 car that only knows how to go fast on a track. An instruct model is like a standard car that already knows how to follow traffic signs and a GPS (system prompts), making it better for general driving but less specialized.\n",
    "\n",
    "3.  **Extension:** What related technique or area should you explore next, and why?\n",
    "    * *Answer*: Explore **Parameter-Efficient Fine-Tuning (PEFT)** techniques like LoRA. Because fine-tuning an entire 8B parameter model is computationally expensive, PEFT allows you to achieve similar results by training only a small fraction of the model's parameters, making the process much faster and more affordable.\n",
    "\n",
    "# Day 2 - Selecting the Best Base Model: Analyzing HuggingFace's LLM Leaderboard\n",
    "\n",
    "### Summary\n",
    "This lecture details the nuanced process of selecting a base model for fine-tuning using the Hugging Face Open LLM Leaderboard. It demonstrates that while overall benchmark scores are a good starting point, a deeper analysis reveals that task-specific features, like a model's tokenizer, can be the deciding factor. For the specific problem of price prediction, Llama 3.1 8B is chosen because its tokenizer efficiently represents three-digit prices as single tokens, which simplifies the model's learning objective.\n",
    "\n",
    "### Highlights\n",
    "* **Nuanced Leaderboard Analysis**: It's wise to assess a base model's potential by looking at the benchmark performance of its \"instruct\" variant. The instruct model is better primed for tests, and its high score can indicate strong underlying capabilities in the base model, even if the base model itself scores lower on the leaderboard.\n",
    "* **Strategic Model Choice: Base over Instruct**: For a single, highly structured task, it's preferable to fine-tune the \"base\" model. This avoids dedicating the model's learning capacity to understanding conversational structures (like system prompts) that are irrelevant to the specific, non-conversational task.\n",
    "* **The Decisive Factor: Tokenization Strategy**: The choice of Llama 3.1 was ultimately driven by its tokenizer, which uniquely maps any number from 0-999 to a single token. This is a critical advantage for the price prediction task.\n",
    "* **Simplifying the Problem**: By representing the entire price as one token, the model's task is simplified from predicting a sequence of digits to predicting a single correct token. This makes the learning objective more direct and efficient.\n",
    "* **Task-Model Fit Over Raw Score**: This selection process highlights that the \"best\" model isn't always the one with the highest overall score. A model with architectural features that are uniquely suited to the specific problem can be a more effective choice.\n",
    "* **A Holistic Selection Process**: The final decision integrates multiple factors: filtering by hardware constraints (parameter count), interpreting leaderboard scores with nuance, weighing the base vs. instruct trade-off, and investigating low-level model features like tokenization.\n",
    "\n",
    "### Conceptual Understanding\n",
    "* **The Impact of Tokenization on Task Performance**\n",
    "    1.  **Why is this concept important?** Tokenization is the foundational step of converting text into a format a model can process. How a tokenizer breaks down text—especially numbers, code, or specialized jargon—can significantly help or hinder a model's performance on a specific task.\n",
    "    2.  **How does it connect to real-world tasks?** For this price prediction task, a single token for the price simplifies the objective to a classification-like problem (predicting one item out of many). For a code generation task, a tokenizer that excels at handling syntax and whitespace would be superior. The ideal tokenizer is always task-dependent.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Subword tokenization algorithms (like BPE and WordPiece), understanding a model's vocabulary size, and the practical skill of analyzing a model's `tokenizer.json` file to see how it handles specific text.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** Which specific project could benefit from this deep analysis of a model's tokenizer before selection?\n",
    "    * *Answer*: A project designed to extract and validate ISBNs from book descriptions would greatly benefit. A model with a tokenizer that can represent the common 10- or 13-digit ISBN structure efficiently (with few tokens) would be more accurate than one that splits the numbers into many individual digits.\n",
    "\n",
    "2.  **Teaching:** How would you explain the importance of Llama's single-token number representation to a junior colleague?\n",
    "    * *Answer*: Imagine teaching a model the price \"$599.\" If the model can learn this as a single \"word,\" the task is straightforward. If it has to learn to say \"5,\" then \"9,\" then another \"9\" in the correct sequence, there are more opportunities for it to make a mistake.\n",
    "\n",
    "3.  **Extension:** What is a practical next step to verify that the Llama 3.1 tokenizer is indeed better for your price prediction task?\n",
    "    * *Answer*: A practical next step is to write a script that takes a sample of your price data and tokenizes it using the tokenizers from both Llama 3.1 and a competing model like Phi-3. By comparing the average number of tokens generated per price, you can quantitatively confirm that Llama's tokenizer provides a more compact and efficient representation for your specific data.\n",
    "\n",
    "# Day 2 - Exploring Tokenizers: Comparing LLAMA, QWEN, and Other LLM Models\n",
    "\n",
    "### Summary\n",
    "This Colab session provides a practical code demonstration of how different large language models tokenize numbers, visually confirming the theoretical basis for selecting Llama 3.1. An `investigate_tokenizer` function is used to show that Llama 3.1 maps three-digit numbers to a single token, unlike competitors like Qwen or Gemma which use multiple tokens. This \"convenient\" property simplifies the learning task for the target price prediction problem, making Llama 3.1 an advantageous choice.\n",
    "\n",
    "### Highlights\n",
    "* **Practical Verification of Tokenization**: The notebook uses a hands-on Python function to move from theory to practice, showing exactly how different models convert number strings into tokens. This is a key step in MLOps for validating model architecture assumptions.\n",
    "* **Llama 3.1's Single-Token Advantage**: The code output confirms that Llama 3.1 represents any number up to 999 (e.g., a potential price) as one unique token. This is a significant advantage for the price prediction task, as the model only needs to predict one item correctly.\n",
    "* **Multi-Token Representation in Other Models**: In contrast, models like Qwen 2.5, Gemma 2, and Phi-3 are shown to use a sequence of tokens for multi-digit numbers (e.g., the string \"100\" becomes three separate tokens). This would make the prediction task a more complex sequential problem for these models.\n",
    "* **Isolating Raw Tokens**: The use of the `add_special_tokens=False` argument is a key technical detail. It ensures that the output is not cluttered with metadata like start-of-sentence tokens, allowing for a clean and direct comparison of how the core text is represented.\n",
    "* **A \"Convenience,\" Not a \"Disqualifier\"**: The speaker emphasizes that while Llama's tokenization is advantageous, the multi-token approach of other models is not a deal-breaker. It simply means those models would need to solve a slightly more complex sequence generation problem, whereas Llama's task is simplified to single-token classification.\n",
    "\n",
    "### Conceptual Understanding\n",
    "* **Why Single-Token Representation Simplifies the Task**\n",
    "    1.  **Why is this important?** By mapping a price like \"999\" to a single token, the fine-tuning task is reframed from a sequence prediction problem (harder) into a single classification problem (easier).\n",
    "    2.  **Connection to real-world tasks:** The model's objective becomes \"predict the correct class (token) out of the vocabulary,\" which is a more direct and computationally simpler objective than \"predict the first digit, then the second digit given the first, then the third digit given the first two.\" This can lead to faster convergence during training and more reliable outputs.\n",
    "    3.  **Related concepts:** This relates to framing regression-style problems for LLMs, output parsing, and managing the model's output logits to favor desired tokens.\n",
    "\n",
    "### Code Examples\n",
    "```python\n",
    "def investigate_tokenizer(model_name):\n",
    "    \"\"\"\n",
    "    Loads a tokenizer for the given model name and prints how it tokenizes\n",
    "    a series of numbers.\n",
    "    \"\"\"\n",
    "    # Load the tokenizer for the specified model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    # List of numbers to test\n",
    "    numbers_to_investigate = [0, 1, 10, 100, 999, 1000]\n",
    "\n",
    "    print(f\"Investigating tokenizer for: {model_name}\")\n",
    "    for number in numbers_to_investigate:\n",
    "        # Convert the number to its string representation\n",
    "        text = str(number)\n",
    "        \n",
    "        # Tokenize the text without adding special start/end tokens\n",
    "        tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "        \n",
    "        # Print the original text and its token representation\n",
    "        print(f\"Text: '{text}' -> Tokens: {tokens}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "# Example usage:\n",
    "# investigate_tokenizer(\"meta-llama/Meta-Llama-3.1-8B\")\n",
    "```\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** In what other type of project would a similar tokenizer investigation be critical?\n",
    "    * *Answer*: This investigation would be critical for a project involving medical data, specifically for parsing dosage instructions like \"2.5mg.\" A tokenizer that splits \"2.5\" into multiple tokens might struggle to understand the numerical value correctly, so finding a model that handles decimal numbers or common dosages efficiently would be a key first step.\n",
    "\n",
    "2.  **Teaching:** How would you explain the purpose of the `investigate_tokenizer` function to a non-technical stakeholder?\n",
    "    * *Answer*: \"This function helps us understand how our AI reads numbers. We found that one AI reads a price like '$125' as a single word, while another reads it as three separate characters '1', '2', '5'. For predicting prices, it's much easier to teach an AI that reads the whole price at once.\"\n",
    "\n",
    "# Day 2 - Optimizing LLM Performance: Loading and Tokenizing Llama 3.1 Base Model\n",
    "\n",
    "### Summary\n",
    "This session details the process of setting up and testing a 4-bit quantized Llama 3.1 base model in Google Colab for a price prediction task. After loading a specialized dataset from Hugging Face, the un-trained model is used for inference on a test example. The base model's prediction is wildly inaccurate ($1,800 vs. an actual price of $374), clearly demonstrating that even a capable foundation model performs poorly on a specialized task without fine-tuning and establishing a baseline that justifies the need for training.\n",
    "\n",
    "### Highlights\n",
    "* **Data Sourcing from Hugging Face Hub**: The workflow begins by loading a pre-processed dataset directly from the Hugging Face Hub. This is a standard practice for ensuring reproducibility and easy access to data in cloud environments like Colab.\n",
    "* **Task Simplification via Prompt Engineering**: The prompt is explicitly framed with \"to the nearest dollar\" to simplify the task for the 8B parameter model. This is a pragmatic adjustment to guide the smaller model, a step that is often unnecessary for much larger frontier models.\n",
    "* **4-Bit Quantization for Accessibility**: The Llama 3.1 8B model is loaded using 4-bit quantization via the `BitsAndBytes` library. This technique dramatically reduces the model's memory footprint (to ~5.6 GB), making it feasible to run on free-tier GPUs, thereby democratizing access to powerful LLMs.\n",
    "* **Standard Inference Pipeline**: The code implements a standard Hugging Face inference pipeline: the prompt is tokenized (`tokenizer.encode`), fed into the model for text generation (`model.generate`), and the resulting tokens are converted back to text (`tokenizer.decode`).\n",
    "* **Controlling Generation Length**: The `max_new_tokens=4` parameter is used in the `generate` call. This is an efficient choice because the task only requires predicting a single price token, with a small buffer for any extra characters, preventing the model from generating unnecessary text.\n",
    "* **Establishing a Performance Baseline**: The first prediction from the off-the-shelf base model is significantly incorrect. This is a crucial step in the machine learning lifecycle, as it establishes a baseline performance and provides a clear justification for the resources required for fine-tuning.\n",
    "\n",
    "### Conceptual Understanding\n",
    "* **4-Bit Quantization**\n",
    "    1.  **Why is this concept important?** Quantization is a compression technique that reduces the precision of a model's weights (from 16 or 32 bits down to 4 bits). This dramatically decreases the memory (VRAM) and storage required to run the model, with only a minor impact on performance. It is a critical enabler for running large models on consumer-grade hardware.\n",
    "    2.  **How does it connect to real-world tasks?** This allows developers and researchers to prototype, fine-tune, and run inference on powerful models like Llama 3.1 8B without needing expensive, enterprise-grade GPUs. It lowers the barrier to entry for building custom AI applications.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Other quantization methods like 8-bit, GPTQ, and AWQ, as well as the trade-offs between model size, inference speed, and potential performance degradation.\n",
    "\n",
    "* **The `model.generate()` Inference Pipeline**\n",
    "    1.  **Why is this concept important?** The `.generate()` method is the core engine for autoregressive text generation in the Hugging Face ecosystem. Understanding its parameters is essential for controlling the model's output in any application.\n",
    "    2.  **How does it connect to real-world tasks?** By controlling parameters like `max_new_tokens`, `temperature`, `top_k`/`top_p` sampling, and `num_return_sequences`, you can tailor the model's output to be short or long, deterministic or creative, and focused or diverse, which is fundamental for building chatbots, summarizers, or any generative AI tool.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Different decoding strategies such as greedy search, beam search, and nucleus sampling.\n",
    "\n",
    "### Code Examples\n",
    "```python\n",
    "import re\n",
    "\n",
    "# Function to extract a numerical price from the model's text output\n",
    "def extract_price(text):\n",
    "    # Search for a number (integer or float) possibly preceded by a dollar sign\n",
    "    match = re.search(r'\\$?(\\d+\\.?\\d*)', text)\n",
    "    if match:\n",
    "        try:\n",
    "            # Convert the found number to a float\n",
    "            return float(match.group(1))\n",
    "        except (ValueError, IndexError):\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Function to run inference with the loaded model\n",
    "def model_predict(prompt):\n",
    "    # Encode the prompt text into input IDs and move to the GPU\n",
    "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    # Create an attention mask to avoid warnings (standard practice)\n",
    "    attention_mask = (inputs != tokenizer.pad_token_id).long()\n",
    "\n",
    "    # Generate output tokens from the model\n",
    "    outputs = base_model.generate(\n",
    "        inputs,\n",
    "        max_new_tokens=4,       # Only need a few tokens for the price\n",
    "        num_return_sequences=1, # We only want one answer\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens back into a string, skipping special tokens\n",
    "    reply_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the numerical price from the generated reply\n",
    "    return extract_price(reply_text)\n",
    "\n",
    "# Example usage:\n",
    "# prompt_text = test_data[0]['text']\n",
    "# predicted_price = model_predict(prompt_text)\n",
    "# print(f\"Predicted Price: ${predicted_price}\")\n",
    "```\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** Which project could benefit from this 4-bit quantization and inference pipeline?\n",
    "    * *Answer*: A project to create a real-time customer support suggestion tool running on a local desktop. By using a 4-bit quantized model, the application could provide instant responses to support agents without relying on slow and expensive API calls to a cloud service.\n",
    "\n",
    "2.  **Teaching:** How would you explain the poor initial result ($1,800 vs. $374) to a project manager?\n",
    "    * *Answer*: \"The initial result shows that the general-purpose AI, like a new employee, doesn't yet understand our specific task of pricing products. We've confirmed it's running correctly, and this poor performance is our expected starting point; now we begin the actual training to teach it the specific patterns in our data.\"\n",
    "\n",
    "3.  **Extension:** What is the most logical next step after seeing the base model fail at this task?\n",
    "    * *Answer*: The next logical step is to implement a fine-tuning strategy, specifically using a **Parameter-Efficient Fine-Tuning (PEFT)** method like **LoRA**. Since the base model lacks the specialized knowledge, we need to train it on our data, and LoRA allows us to do this efficiently on the same memory-constrained hardware by updating only a small fraction of the model's weights.\n",
    "\n",
    "# Day 2 - Quantization Impact on LLMs: Analyzing Performance Metrics and Errors\n",
    "\n",
    "### Summary\n",
    "This document outlines the process of evaluating a small, open-source language model (Llama, 8 billion parameters) on a price prediction task before fine-tuning. The evaluation reveals that both the 4-bit and 8-bit quantized versions of the model perform poorly, with mean errors of $395 and $301 respectively, which is worse than simple heuristics. This establishes a critical baseline and sets up the primary challenge: to use fine-tuning to transform this underperforming, efficient model into a system that can compete with massive, proprietary models like GPT-4.\n",
    "\n",
    "### Highlights\n",
    "- **Baseline Performance is Crucial**: The first step was to test the pre-trained model on a 250-item test set. This establishes a baseline metric to measure the effectiveness of future fine-tuning, confirming the model does not yet understand the specific task.\n",
    "- **4-Bit Quantized Model Fails Significantly**: The heavily compressed 4-bit model produced a very high average error of $395. Its predictions were often clustered at specific high values, indicating it learned a flawed pattern and lacked the nuance for accurate price estimation.\n",
    "- **Quantization Impacts Accuracy**: A direct comparison showed that reducing the model's precision through quantization has a tangible negative effect. The 8-bit quantized model, while still poor, performed better with a $301 error, demonstrating a clear trade-off between model size/efficiency and predictive accuracy.\n",
    "- **Model Size vs. Performance**: The 8-billion parameter model's poor initial results highlight the performance gap between smaller, accessible open-source models and massive, trillion-parameter \"frontier\" models. The goal is to bridge this gap using cost-effective fine-tuning rather than relying on expensive, large-scale pre-training.\n",
    "- **The Goal of Fine-Tuning**: The core challenge is now to take this poorly performing base model and, using a custom training dataset, fine-tune it. The ultimate aim is to achieve performance comparable to or better than a human expert and potentially even approach the accuracy of much larger models, but without the associated API costs.\n",
    "\n",
    "### Conceptual Understanding\n",
    "- **Model Quantization (4-bit vs. 8-bit)**\n",
    "  1.  **Why is this concept important?** Quantization is a technique to reduce the memory footprint and computational cost of a neural network by representing its weights with fewer bits (e.g., 8-bit or 4-bit integers instead of 32-bit floating-point numbers). This is critical for deploying large models on consumer-grade hardware or in resource-constrained environments.\n",
    "  2.  **How does it connect to real-world tasks?** For applications like real-time price prediction on a local server or mobile device, a full-sized model is often impractical. Quantization allows developers to use powerful models that would otherwise be too large or slow, but as shown, it comes at the cost of reduced precision and accuracy.\n",
    "  3.  **Which related techniques or areas should be studied alongside this concept?** You should explore **QLoRA (Quantized Low-Rank Adaptation)**, a specific fine-tuning method designed to efficiently train quantized models. Also, researching **quantization-aware training (QAT)** is beneficial, as it involves fine-tuning the model to be more robust to the precision loss from quantization.\n",
    "\n",
    "- **Baseline Model Evaluation**\n",
    "  1.  **Why is this concept important?** Establishing a baseline with an \"untrained\" (i.e., not yet fine-tuned) model is the most critical first step in any transfer learning project. It provides a quantitative measure of how much the model knows about your specific task out-of-the-box and serves as the benchmark against which all fine-tuning improvements are measured.\n",
    "  2.  **How does it connect to real-world tasks?** In any commercial data science project, you must justify the cost and effort of training. If a baseline model already performs well, minimal fine-tuning might be needed. If it performs horribly (as in this case), it proves the necessity of the training phase and helps set realistic expectations for stakeholders.\n",
    "  3.  **Which related techniques or areas should be studied alongside this concept?** This directly leads to **fine-tuning** and **transfer learning**. It's also related to **model selection**, as you might evaluate several different baseline models (e.g., Llama, Mistral, Gemma) to see which one provides the best starting point for your specific task before committing to fine-tuning.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** Which specific dataset or project could benefit from this concept? Provide a one-sentence explanation.\n",
    "    - *Answer*: This approach is ideal for an e-commerce platform wanting to build an internal tool for predicting the market price of second-hand electronics, where running a small, fine-tuned open-source model would be far more cost-effective than paying for a high-volume API.\n",
    "\n",
    "2.  **Teaching:** How would you explain the impact of 4-bit vs. 8-bit quantization to a junior colleague, using one concrete example?\n",
    "    - *Answer*: Think of it like saving a photograph: saving it as a high-quality JPEG (8-bit) retains most details, but saving it as a highly compressed GIF (4-bit) makes the file much smaller but can create color banding and artifacts, losing important nuance—similarly, the 4-bit model loses numerical precision, leading to worse predictions.\n",
    "\n",
    "3.  **Extension:** What related technique or area should you explore next, and why?\n",
    "    - *Answer*: The next logical step is to explore **QLoRA (Quantized Low-Rank Adaptation)**, because it is a highly efficient fine-tuning technique specifically designed to train quantized models like this one, allowing for significant performance improvements without requiring massive computational resources.\n",
    "\n",
    "# Day 2 - Comparing LLMs: GPT-4 vs LLAMA 3.1 in Parameter-Efficient Tuning\n",
    "\n",
    "### Summary\n",
    "This session recaps the performance of various models on a price prediction task, revealing that the un-fine-tuned, quantized Llama 3.1 model performs devastatingly poorly (with errors of $396 for 4-bit and $301 for 8-bit), significantly worse than even simple baselines. This sets the stage for the next crucial phase: using supervised fine-tuning (SFT) to train this small, open-source model. The goal is to transform its performance and attempt to compete with leading proprietary models like GPT-4 on this specific task.\n",
    "\n",
    "### Highlights\n",
    "- **Performance Hierarchy Established**: A clear ranking places GPT-4 at the top with a $76 error, followed by Random Forest ($97) and a simple average guess ($146). The base Llama 3.1 model is at the very bottom, demonstrating it is not yet suitable for this specialized task without further training.\n",
    "- **Base Llama Model Fails**: The 8-billion parameter Llama model, especially when quantized to 4-bits, is the worst-performing model tested. This highlights that general pre-trained models require specific adaptation to excel at niche tasks.\n",
    "- **The Core Challenge**: The key objective is to take a free, open-source, and efficient model (Llama 3.1) and, through fine-tuning, make it competitive with a massive, costly, state-of-the-art model (GPT-4). Success would be a significant achievement in democratizing powerful AI.\n",
    "- **Next Step is Supervised Fine-Tuning (SFT)**: The upcoming lessons will focus on the most critical part of the process: training the model. This involves configuring training-specific hyperparameters and using a `Supervised Fine-Tuning (SFT) Trainer` to adapt the model to the price prediction dataset.\n",
    "\n",
    "### Conceptual Understanding\n",
    "- **Supervised Fine-Tuning (SFT)**\n",
    "  1.  **Why is this concept important?** SFT is the standard process for teaching a general-purpose, pre-trained model how to perform a specific, desired task. It adapts the model's weights by training it on a labeled dataset (e.g., a set of product descriptions and their correct prices), making it an expert in that narrow domain.\n",
    "  2.  **How does it connect to real-world tasks?** This is the fundamental technique used to create specialized AI tools. For example, SFT is used to turn a base model into a customer service chatbot that follows a specific script, a medical report summarizer, or, in this case, an accurate price estimator.\n",
    "  3.  **Which related techniques or areas should be studied alongside this concept?** After mastering SFT, it's useful to study `Parameter-Efficient Fine-Tuning (PEFT)` methods like LoRA, which make training even more memory and compute-efficient. Another advanced area is `Reinforcement Learning from Human Feedback (RLHF)`, which further refines model behavior based on qualitative feedback.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** Which specific project could benefit from applying Supervised Fine-Tuning? Provide a one-sentence explanation.\n",
    "    - *Answer*: A legal tech company could use SFT to fine-tune a base model on a dataset of legal documents and their summaries, creating a highly accurate tool for generating case briefs automatically.\n",
    "\n",
    "2.  **Teaching:** How would you explain Supervised Fine-Tuning to a junior colleague, using one concrete example?\n",
    "    - *Answer*: Imagine you have a brilliant recent graduate who knows a lot about everything (the base model); SFT is like giving them on-the-job training with specific examples of your company's tasks until they become an expert performer in their specific role.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
