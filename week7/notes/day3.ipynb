{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05445d2a",
   "metadata": {},
   "source": [
    "# Day 3 - QLoRA Hyperparameters: Mastering Fine-Tuning for Large Language Models\n",
    "\n",
    "### Summary\n",
    "This text outlines the preparatory steps for training a proprietary Large Language Model (LLM), focusing on the critical role of hyperparameter tuning for QLoRA. It explains five essential hyperparameters—target modules, rank (`r`), alpha, quantization, and dropout—as levers for optimizing model performance and preventing common issues like overfitting. The core idea is that mastering these settings is fundamental to the research and development process of building high-performing, customized models.\n",
    "\n",
    "### Highlights\n",
    "- **Target Modules**: Instead of fine-tuning an entire large model, you select specific layers (e.g., attention layers) to apply low-rank adapters to. This drastically reduces computational cost, making it feasible to train on consumer-grade hardware.\n",
    "- **R (Rank of the Adapter)**: This hyperparameter defines the dimensionality (and thus the number of trainable parameters) of the low-rank adapter matrices. A common starting point is 8, but it can be increased (e.g., to 32) for complex tasks with large datasets, though higher values offer diminishing returns and consume more memory.\n",
    "- **LoRA Alpha**: This is a scaling factor applied to the outputs of the adapter matrices (`change in weights = alpha * LoRA_A * LoRA_B`). A common rule of thumb is to set alpha to twice the value of `r` (e.g., if `r=32`, `alpha=64`), balancing the influence of the newly trained weights.\n",
    "- **Quantization**: This technique reduces the precision of the base model's weights (e.g., from 32-bit to 4-bit) to decrease memory usage. While it allows large models to fit into memory, there is a trade-off, as lower precision can slightly degrade the model's baseline performance.\n",
    "- **Dropout**: A regularization technique used to prevent overfitting, where the model memorizes the training data instead of learning general patterns. It works by randomly setting a fraction of neuron activations to zero during each training step, forcing the network to build more robust and generalized representations.\n",
    "\n",
    "### Conceptual Understanding\n",
    "- **Dropout**\n",
    "    1.  **Why is this concept important?** Dropout is a crucial technique for improving a model's ability to generalize to new, unseen data. Without it, a model trained for too long on a specific dataset can become \"overfitted,\" performing exceptionally well on that data but failing badly on any new data, making it useless for real-world applications.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** In any production model (e.g., a chatbot, a sentiment analyzer, a code generator), the goal is to handle a wide variety of user inputs, not just the examples it was trained on. Dropout helps ensure this by preventing the model from becoming too dependent on any single neuron or feature, making its knowledge more distributed and robust.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** To fully grasp regularization, you should also study other techniques like **L1 and L2 regularization**, which add a penalty to the loss function based on the magnitude of the model weights, and **early stopping**, where training is halted once the model's performance on a validation set starts to degrade.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** Which specific dataset or project could benefit from applying dropout?\n",
    "    - *Answer*: A project to fine-tune a model for medical diagnosis on a limited dataset of patient records would greatly benefit from dropout, as it would prevent the model from memorizing specific patient cases and help it learn more generalizable diagnostic patterns.\n",
    "2.  **Teaching:** How would you explain dropout to a junior colleague, using one concrete example?\n",
    "    - *Answer*: Imagine a team of experts trying to solve a problem, but for every meeting, you randomly ask 10% of them to stay silent. This forces the remaining experts to collaborate more broadly and prevents any single expert from dominating the solution, leading to a more robust final decision that doesn't rely on just one person's niche knowledge.\n",
    "3.  **Extension:** What related technique or area should you explore next, and why?\n",
    "    - *Answer*: After understanding dropout, exploring **learning rate scheduling** is a logical next step. While dropout controls model complexity, the learning rate controls how quickly the model adapts its weights, and using a scheduler to adjust this rate during training can significantly improve convergence and final performance.\n",
    "\n",
    "# Day 3 - Understanding Epochs and Batch Sizes in Model Training\n",
    "\n",
    "### Summary\n",
    "This text explains two fundamental hyperparameters for the machine learning training process: epochs and batch size. It details how epochs, or full passes over the training data, allow a model to iteratively refine its parameters, while batch size determines how many data points are processed simultaneously for performance. The key takeaway is that running multiple epochs is beneficial up to a point, after which the model may begin to overfit, making it critical to save and evaluate the model at each stage to select the best version.\n",
    "\n",
    "### Highlights\n",
    "- **Epochs**: An epoch represents one complete iteration through the entire training dataset. Training a model for multiple epochs allows it to see the data repeatedly, making small, iterative improvements to its weights each time and potentially learning more refined patterns.\n",
    "- **Batch Size**: Instead of processing one data point at a time, data is grouped into \"batches\" (e.g., of 4, 8, or 16 samples) that are fed through the model together. This is primarily done for computational efficiency, as it leverages parallel processing capabilities on GPUs to speed up training.\n",
    "- **Interaction between Epochs and Batches**: At the beginning of each new epoch, the training data is typically shuffled. This means that while the model sees the same overall data, the composition of the batches is different in each epoch, which helps the model learn more robust and generalized features.\n",
    "- **Finding the Optimal Epoch**: Model performance generally improves with each epoch up to a certain point. After this peak, the model may start to overfit—memorizing the training data and performing worse on unseen data. A common strategy is to save the model after each epoch, test its performance, and select the version from the epoch that yielded the best results.\n",
    "\n",
    "### Conceptual Understanding\n",
    "- **Epochs and Overfitting**\n",
    "    1.  **Why is this concept important?** The number of epochs is a direct control for how much the model learns. Too few, and the model is \"underfit\" (hasn't learned the patterns); too many, and it's \"overfit\" (has memorized the noise). Understanding this trade-off is fundamental to training any effective machine learning model.\n",
    "    2.  **Connection to real-world tasks, problems, or applications?** When fine-tuning a language model for a specific task like legal document analysis, you must find the optimal number of epochs. Overtraining could cause the model to perform poorly on new legal contracts because it has memorized the exact phrasing of the contracts in your training set instead of learning the general structure and clauses.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** To properly manage the epoch-overfitting relationship, you must learn about **validation sets** (a separate dataset used to check model performance after each epoch) and **early stopping** (an automated technique that stops the training process once performance on the validation set stops improving).\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** Which specific dataset or project could benefit from carefully tuning the number of epochs?\n",
    "    - *Answer*: When fine-tuning a language model on a small, specialized dataset, such as a collection of a single author's poems, carefully tuning epochs is critical to capture the author's style without memorizing specific poems.\n",
    "2.  **Teaching:** How would you explain epochs and batches to a junior colleague, using one concrete example?\n",
    "    - *Answer*: Think of the entire dataset as a textbook. One epoch is like reading the entire textbook from start to finish once, while the batch size is how many pages you read at a time before pausing to think and take notes.\n",
    "\n",
    "# Day 3 - Learning Rate, Gradient Accumulation, and Optimizers Explained\n",
    "\n",
    "### Summary\n",
    "This text covers three critical hyperparameters that control the core mechanics of the model training process: learning rate, gradient accumulation, and the optimizer. It explains that the learning rate dictates the size of the adjustments made to the model's weights, gradient accumulation offers a technique to speed up training by batching updates, and the optimizer is the specific algorithm that applies these changes. Understanding these levers is essential for efficiently guiding a model towards a high-quality solution and managing computational resources.\n",
    "\n",
    "### Highlights\n",
    "- **Learning Rate**: This is the step size the model takes to adjust its weights in the right direction after calculating the error (loss). A high learning rate can cause the model to overshoot the optimal solution, while a very low one can make training incredibly slow.\n",
    "- **Learning Rate Scheduler**: This is an advanced technique where the learning rate is not fixed but is gradually decreased over the course of training. This allows the model to make large adjustments at the beginning when it knows very little and smaller, more refined adjustments as it gets closer to the optimal solution.\n",
    "- **Gradient Accumulation**: A performance-enhancing technique where gradients (the signals for how to update weights) are collected over several batches *before* an update step is performed. This allows for a larger effective batch size without a proportional increase in memory usage, which can speed up training.\n",
    "- **Optimizer**: This is the specific algorithm used to update the model's weights based on the calculated gradients. Different optimizers (like Adam, SGD, etc.) use different mathematical formulas to perform this update, each with its own trade-offs between computational cost, memory usage, and the final quality of the model.\n",
    "\n",
    "### Conceptual Understanding\n",
    "- **The Optimization Trio (Learning Rate, Gradients, Optimizer)**\n",
    "    1.  **Why is this concept important?** This trio forms the engine of the learning process. The gradients tell you the *direction* to improve, the learning rate tells you the *size of the step* to take in that direction, and the optimizer is the *method* or vehicle you use to take that step. Misconfiguring any of these can lead to failed training or suboptimal models.\n",
    "    2.  **Connection to real-world tasks, problems, or applications?** In any fine-tuning task, from building a custom chatbot to a financial forecasting model, these settings directly control the training budget (time and cost) and the final result. A well-chosen optimizer and learning rate schedule can mean the difference between a state-of-the-art model and one that fails to converge.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** To go deeper, one should study different types of optimizers (e.g., **AdamW**, **RMSprop**, **SGD with Momentum**) to understand their specific behaviors. Additionally, exploring the concept of the **loss landscape** (the multi-dimensional surface the optimizer is trying to navigate) helps build intuition for why these settings are so crucial.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** In which project would the choice of optimizer be particularly important?\n",
    "    - *Answer*: Training a very large model like a GPT variant on a massive dataset would make the optimizer choice critical, as memory-efficient optimizers (like `paged_adamw_8bit`) could be the only way to make the training feasible on available hardware.\n",
    "2.  **Teaching:** How would you explain the learning rate and optimizer to a junior colleague, using one concrete example?\n",
    "    - *Answer*: Imagine you're blindfolded on a mountainside, trying to get to the lowest valley. The optimizer is your strategy for walking (e.g., \"always take a step in the steepest downward direction\"), and the learning rate is the size of each step you take—too big and you might step over the valley, too small and it will take forever to get there.\n",
    "\n",
    "# Day 3 - Setting Up the Training Process for Fine-Tuning\n",
    "\n",
    "### Summary\n",
    "This text provides a comprehensive walkthrough of setting up a Google Colab environment and configuring all necessary parameters to fine-tune a Llama 3.1 8B model using Hugging Face's TRL library. It covers practical considerations like GPU selection and cost, project naming for experiment tracking, and a detailed breakdown of the two main groups of hyperparameters: those for QLoRA (e.g., `r`, `alpha`, `dropout`) and those for the training process itself (e.g., `epochs`, `batch_size`, `learning_rate`, `optimizer`). The guide emphasizes the trade-offs between performance, cost, and model quality, preparing the user to launch their first supervised fine-tuning (SFT) job.\n",
    "\n",
    "### Highlights\n",
    "- **Environment Setup**: The user can choose between a powerful but costly A100 GPU for fast experimentation or a standard T4 GPU for a more budget-friendly approach. The choice directly impacts settings like `batch_size`.\n",
    "- **TRL and SFT Trainer**: The core of the training process is handled by the `SFTTrainer` from Hugging Face's TRL (Transformer Reinforcement Learning) library, which abstracts away much of the complexity of writing a training loop.\n",
    "- **Systematic Experiment Tracking**: A robust naming convention is introduced, using the project name (`Pricer`) and a timestamp (`run_name`) to create unique model names for each run. This is crucial for organizing experiments and comparing the results of different hyperparameter configurations.\n",
    "- **Data Loading**: The setup allows for flexibility, enabling the user to either load their own curated dataset from the Hugging Face Hub or use a provided public dataset.\n",
    "- **QLoRA Configuration**: The parameters for parameter-efficient fine-tuning are set, including an adapter rank (`r`) of 32, a scaling factor (`lora_alpha`) of 64, and a `lora_dropout` of 0.1 to prevent overfitting.\n",
    "- **Batch Size and Memory**: A key trade-off is highlighted: a larger `batch_size` (e.g., 16 on an A100) speeds up training but requires significant GPU memory. For memory-constrained GPUs like the T4, a `batch_size` of 1 is recommended.\n",
    "- **Dynamic Learning Rate**: Instead of a fixed learning rate, a `cosine` learning rate scheduler is used. This starts with a higher rate and gradually decreases it, which helps the model converge more effectively and avoid getting stuck in suboptimal solutions.\n",
    "- **Warmup Ratio**: The training starts with an even lower learning rate for a small portion of the initial steps (the \"warmup\"). This stabilizes the training process at the beginning when the model is making large, potentially erratic adjustments.\n",
    "- **The Local Minimum Problem**: A core challenge in optimization is explained: a learning rate that is too low can cause the model to get \"stuck\" in a small valley (a local minimum) and fail to find the much deeper valley (the global minimum) that represents the best possible solution.\n",
    "- **Optimizer Choice**: The `paged_adamw_32bit` optimizer is chosen for its excellent performance in finding optimal model weights. However, it is memory-intensive, and the text notes that less greedy alternatives exist if memory becomes an issue.\n",
    "\n",
    "### Conceptual Understanding\n",
    "- **Learning Rate Scheduling and Warmup**\n",
    "    1.  **Why is this concept important?** A fixed learning rate is a blunt instrument. A scheduler acts like a sophisticated strategy: start with large, confident steps when you are far from the goal, and take smaller, more careful steps as you get closer to the final solution. The initial warmup period prevents the model from \"tripping\" at the starting line by taking too large a step when it is most unstable.\n",
    "    2.  **Connection to real-world tasks, problems, or applications?** This is directly tied to training efficiency and model quality. Using a scheduler can significantly reduce the total training time needed to reach a good solution and often results in a final model with lower loss (better performance) than one trained with a static learning rate.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** You should explore different scheduler types (`linear`, `constant_with_warmup`) and understand their shapes. Advanced concepts include **cyclical learning rates**, where the rate goes up and down in cycles to help escape local minima.\n",
    "- **Optimizer Memory vs. Performance Trade-off**\n",
    "    1.  **Why is this concept important?** The optimizer is a major consumer of GPU memory because some of the best-performing ones (like AdamW) need to store extra information (e.g., rolling averages of past gradients) for each model parameter. Understanding this trade-off is critical for successfully training large models on limited hardware.\n",
    "    2.  **Connection to real-world tasks, problems, or applications?** If you are trying to fine-tune a 70-billion parameter model on a single GPU, you cannot use a memory-hungry optimizer. You must choose a more memory-efficient one (like `adafactor` or an 8-bit Adam variant), even if it means the training might be slightly slower or converge to a slightly less optimal result. It makes the difference between a training run that succeeds and one that fails with an out-of-memory error.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Study the specifics of different optimizers like **Adam**, **AdamW**, and **SGD**. Also, research techniques for memory reduction like **gradient accumulation** and **mixed-precision training**, which work alongside the optimizer to make training more feasible.\n",
    "\n",
    "### Code Examples\n",
    "The following parameters are configured in the Colab notebook for the training run:\n",
    "\n",
    "**Project and Model Configuration**\n",
    "```python\n",
    "base_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "project_name = \"pricer\"\n",
    "max_seq_length = 182\n",
    "# Hub model name is dynamically created, e.g., \"your-username/pricer-20240612123000\"\n",
    "```\n",
    "\n",
    "**QLoRA Hyperparameters**\n",
    "```python\n",
    "r = 32\n",
    "lora_alpha = 64\n",
    "target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"]\n",
    "lora_dropout = 0.1\n",
    "```\n",
    "\n",
    "**Training Hyperparameters**\n",
    "```python\n",
    "num_train_epochs = 3\n",
    "# For A100 GPU\n",
    "per_device_train_batch_size = 16 \n",
    "# For T4 GPU, this should be 1\n",
    "gradient_accumulation_steps = 1\n",
    "learning_rate = 0.0001 # or 1e-4\n",
    "lr_scheduler_type = \"cosine\"\n",
    "warmup_ratio = 0.1\n",
    "optimizer = \"paged_adamw_32bit\"\n",
    "```\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** If you were adapting this script to perform text classification on a small, domain-specific dataset (e.g., 2,000 legal clauses), which two hyperparameters would you change first?\n",
    "    - *Answer*: I would first decrease `num_train_epochs` to 1 or 2 to avoid overfitting on the small dataset, and I would also consider decreasing the `learning_rate` (e.g., to `5e-5`) to make smaller, more careful updates suitable for a more narrow task.\n",
    "2.  **Teaching:** How would you explain a cosine learning rate scheduler with warmup to a junior colleague using an analogy?\n",
    "    - *Answer*: Imagine you're carefully pushing a car to a precise spot. You start by giving it a gentle nudge to get it rolling (the warmup), then you push hard and steadily to cover most of the distance, and as you get close to the parking spot, you gradually ease off the pressure to make sure you don't overshoot (the cosine curve).\n",
    "3.  **Extension:** If your model's training loss is fluctuating wildly and not decreasing, which hyperparameter is the most likely culprit and what would you do?\n",
    "    - *Answer*: The `learning_rate` is the most likely culprit. Wild fluctuations suggest it is too high, causing the optimizer to repeatedly overshoot the optimal solution. I would try reducing it by a factor of 5 or 10 (e.g., from `1e-4` to `1e-5`) and restart the training run.\n",
    "\n",
    "# Day 3 - Configuring SFTTrainer for 4-Bit Quantized LoRA Fine-Tuning of LLMs\n",
    "\n",
    "### Summary\n",
    "This text provides a practical, step-by-step guide to preparing the final components for a supervised fine-tuning (SFT) run in a Google Colab notebook. It covers the initial setup of logging into Hugging Face and Weights & Biases, loading and verifying the dataset, and configuring the model and tokenizer. The key technical step introduced is the use of a `DataCollatorForCompletionOnlyLM`, a specialized Hugging Face utility that focuses the model's training exclusively on predicting the target completion (the price), making the fine-tuning process more efficient and targeted.\n",
    "\n",
    "### Highlights\n",
    "- **Environment Initialization**: The process begins by logging into essential services: Hugging Face (to push the final model) and Weights & Biases (to log metrics and track experiments in real-time).\n",
    "- **Data Verification**: Before training, the script confirms the dataset has loaded correctly, checking for the expected 400,000 training examples and verifying that each entry contains the prompt text followed by the price completion.\n",
    "- **Model and Tokenizer Setup**: The 4-bit quantized Llama 3.1 8B model and its corresponding tokenizer are loaded. The tokenizer is configured to right-pad all sequences to the maximum length of 182 tokens using the end-of-sentence token.\n",
    "- **Focused Training with `DataCollatorForCompletionOnlyLM`**: This is the most critical new component. It's a utility that automatically masks the loss function for the prompt part of the input, ensuring the model only learns from its errors in predicting the completion. This prevents the model from wasting capacity on relearning how to write the prompt.\n",
    "- **Response Template**: To use the data collator, a `response_template` is defined (in this case, `\"Price is $\"`). This string acts as a signal, telling the collator that the model should only be trained on predicting the tokens that appear *after* this template.\n",
    "- **Configuration Objects**: All the hyperparameters are organized into dedicated configuration objects. A `LoraConfig` holds the QLoRA parameters (`r`, `alpha`, `dropout`), and a set of training arguments holds the general training settings (`epochs`, `batch_size`, `learning_rate`).\n",
    "- **`SFTTrainer` Instantiation**: The final step is to create an instance of the `SFTTrainer`. This object brings everything together: the base model, tokenizer, training data, LoRA config, training arguments, and the specialized data collator, making it ready for the final `trainer.train()` command.\n",
    "\n",
    "### Conceptual Understanding\n",
    "- **DataCollatorForCompletionOnlyLM**\n",
    "    1.  **Why is this concept important?** In completion-style fine-tuning, the goal isn't to teach the model to regenerate the prompt, but to teach it how to generate the desired output *given* the prompt. This collator makes the training process highly efficient by ignoring errors made on the prompt tokens and focusing the model's updates solely on getting the completion part correct.\n",
    "    2.  **Connection to real-world tasks, problems, or applications?** This is essential for any instruction-tuning or request-response task. For example, if you're fine-tuning a model to be a chatbot, your data might look like `[INST]User: Hello![/INST]Assistant: Hi, how can I help?`. You would use a response template of `[/INST]Assistant:` to ensure the model only learns to generate the assistant's reply, not the user's query.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** This is a practical application of **loss masking**, a fundamental technique in training transformers. Understanding how loss is calculated only on specific tokens is key. It's also closely related to **prompt engineering** and defining clear input/output formats for fine-tuning.\n",
    "\n",
    "### Code Examples\n",
    "The following snippets represent the key steps described in the text for setting up the trainer.\n",
    "\n",
    "**1. Login to Weights & Biases**\n",
    "```python\n",
    "import wandb\n",
    "wandb.login(key=\"YOUR_WANDB_API_KEY\") \n",
    "```\n",
    "\n",
    "**2. Create the Data Collator**\n",
    "```python\n",
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "# The string that signals the start of the part to predict\n",
    "response_template = \"Price is $\"\n",
    "\n",
    "# Initialize the collator\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template=response_template, \n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "```\n",
    "\n",
    "**3. Define LoRA and Training Configurations**\n",
    "```python\n",
    "from peft import LoraConfig\n",
    "# Note: In the video, SFTConfig is used, but TrainingArguments is the standard class.\n",
    "from transformers import TrainingArguments \n",
    "\n",
    "# LoRA specific parameters\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# General training parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"pricer-run-1\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16, # Or 1 for T4 GPU\n",
    "    learning_rate=1e-4,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    # ... other training parameters\n",
    "    push_to_hub=True,\n",
    "    hub_model_id=\"username/pricer-run-1\",\n",
    "    hub_private_repo=True\n",
    ")\n",
    "```\n",
    "\n",
    "**4. Instantiate the SFT Trainer**\n",
    "```python\n",
    "from trl import SFTTrainer\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    peft_config=lora_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    max_seq_length=182\n",
    ")\n",
    "```\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** If you wanted to fine-tune a model to act as a SQL query generator, where the prompt is a natural language question, what would be a good `response_template`?\n",
    "    - *Answer*: A good `response_template` would be a unique phrase that always precedes the SQL code, such as `### SQL Query:` or `[SQL]`, to clearly separate the natural language question from the SQL code to be generated.\n",
    "2.  **Teaching:** How would you explain what the `DataCollatorForCompletionOnlyLM` does to a non-technical manager?\n",
    "    - *Answer*: Imagine we're teaching a student to finish sentences. We give them \"The capital of France is ___.\" We don't grade them on rewriting \"The capital of France is\"; we only grade them on whether they correctly write \"Paris.\" This tool does exactly that for our AI model, focusing its learning on only the answer part.\n",
    "3.  **Extension:** What would be the negative consequence of *not* using this data collator for the \"Pricer\" fine-tuning task?\n",
    "    - *Answer*: Without it, the model would waste a significant portion of its training effort on learning to predict the product descriptions and boilerplate text in the prompt. This would slow down its learning of the actual pricing task and likely result in a less accurate model after the same number of training epochs, as its \"learning budget\" would have been diluted.\n",
    "\n",
    "# Day 3 - Fine-Tuning LLMs: Launching the Training Process with QLoRA\n",
    "\n",
    "### Summary\n",
    "This text describes the initiation of the model fine-tuning process by executing the `trainer.train()` command. It highlights the immediate and significant consumption of GPU resources as the training begins, and points to the initial log outputs which show the training loss and provide a lengthy time estimate for completion. The author also notes a deliberate trade-off, skipping a formal validation step during training to maximize speed, while acknowledging that including one is a standard best practice.\n",
    "\n",
    "### Highlights\n",
    "- **Initiating Training**: The entire fine-tuning process is kicked off with a single command: `fine_tuning.train()`. The results, including the final model, are set to be pushed to the Hugging Face Hub upon completion.\n",
    "- **Resource Consumption**: Running the training immediately causes a dramatic spike in GPU memory usage, in this case shooting up from 6 GB to nearly the full 40 GB capacity of the A100 GPU. This confirms that the chosen `batch_size` of 16 fully utilizes the available hardware.\n",
    "- **Monitoring Progress**: The initial logs provide key information: an estimated total run time (over 24 hours for three epochs on this large dataset) and the training loss, which is reported every 50 steps and simultaneously logged to Weights & Biases for visualization.\n",
    "- **Best Practice Note (Evaluation)**: The author intentionally omits an evaluation step (`eval_strategy`) during training to prioritize speed. However, it's noted that using a held-out validation dataset to monitor performance throughout the training process is a standard best practice for more rigorous model development.\n",
    "\n",
    "# Day 3 - Monitoring and Managing Training with Weights & Biases\n",
    "\n",
    "### Summary\n",
    "This text concludes the session by postponing the analysis of the training results to the next lesson, which will cover examining the run in Weights & Biases and the model on the Hugging Face Hub. It emphasizes that spending significant money on high-end GPUs is optional and not required for the course, as effective training can be done for just a few cents with a smaller dataset. The user is congratulated on the major achievement of launching a training run and gaining a solid understanding of complex concepts like QLoRA and its various hyperparameters.\n",
    "\n",
    "### Highlights\n",
    "- **Next Steps**: The analysis of training results using Weights & Biases and viewing the saved model on the Hugging Face Hub will be covered in the next session.\n",
    "- **Cost-Effective Training**: The author clarifies that spending significant money is not necessary. The training exercises can be completed for a very low cost (cents) by reducing the size of the training dataset and using standard, less expensive GPUs.\n",
    "- **Major Milestone Achieved**: The user is congratulated for successfully launching a fine-tuning run. They have now gained practical experience with and an understanding of advanced topics, including QLoRA, target modules, learning rates, dropout, and optimizers.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
