{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f66e66",
   "metadata": {},
   "source": [
    "# Day 4 - Hugging Face Model Class: Running Inference on Open-Source AI Models\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This text introduces the Hugging Face `model` class, a lower-level API component for running inference on open-source transformer models. It matters because it allows for more granular control over model loading and execution, enabling tasks like comparing different models (Llama 3.1, Phi 3, Gemma), applying techniques like quantization for efficiency, and streaming output.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- ‚ú® **Introduction to Hugging Face Model Class:** This is a lower-level API in the Hugging Face Transformers library, following tokenizers, used for loading and running transformer models to generate text. Its relevance lies in providing direct access to model functionalities beyond the high-level `pipeline` API, crucial for customized inference workflows.\n",
    "- üî¨ **Comparative Model Analysis:** The session will involve running and comparing results across several open-source models: Meta's Llama 3.1, Microsoft's Phi 3, and Google's Gemma, with an option to experiment with Mistral and Qwen2. This is useful for data scientists to understand the performance and output characteristics of different architectures on specific tasks, aiding in model selection for projects.\n",
    "- ‚öôÔ∏è **Quantization:** This technique reduces the precision of model weights, making models smaller, easier to fit into memory (especially on lower-end GPUs), and faster to run. It's highly relevant for deploying large models in resource-constrained environments and is critical for efficient training of large open-source models (e.g., using QLoRA).\n",
    "- üîé **Inspecting Model Internals:** The session will offer a glimpse into the PyTorch layers that constitute Hugging Face Transformer models. This is valuable for understanding the underlying architecture and mechanics of these models, beneficial for debugging, custom modifications, and advanced research.\n",
    "- üåä **Streaming Results:** The ability to stream output token by token from models will be covered. This is essential for creating interactive applications like chatbots or any system where users expect real-time feedback, improving user experience significantly.\n",
    "\n",
    "### **Conceptual Understanding**\n",
    "\n",
    "- **Introduction to Hugging Face Model Class**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It provides deeper control over the inference process compared to the high-level `pipeline` API, allowing for more tailored solutions and a better understanding of model operations.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Used in custom inference scripts, integrating models into complex applications, research on model behavior, and when needing specific configurations not exposed by the `pipeline` API.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Tokenization, model loading, tensor manipulation, inference endpoints, and the overall architecture of transformer models.\n",
    "- **Comparative Model Analysis**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Different models have varying strengths, weaknesses, and biases. Understanding these helps in selecting the most appropriate model for a given task, dataset, or performance requirement.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Directly applicable in scenarios like choosing a chatbot's base model, selecting a model for text summarization in a news app, or finding an efficient model for sentiment analysis on edge devices.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Benchmarking, model evaluation metrics (e.g., perplexity, BLEU score), task-specific fine-tuning, and understanding different model architectures (e.g., Llama, Phi, Gemma).\n",
    "- **Quantization**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It enables the use of large, powerful models on hardware with limited memory and computational capacity, democratizing access to SOTA models. It also speeds up inference.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Deploying large language models on consumer-grade GPUs, mobile devices, or edge computing hardware. Essential for reducing operational costs in cloud deployments and for efficient fine-tuning (e.g., QLoRA).\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Model compression, numerical precision (e.g., FP32, FP16, INT8), hardware acceleration (GPUs, TPUs), and fine-tuning techniques like LoRA and QLoRA.\n",
    "- **Inspecting Model Internals**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Provides a deeper understanding of how transformers work, layer by layer, which can be crucial for advanced customization, troubleshooting, or contributing to model development.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Useful for researchers developing new model architectures or layers, for engineers debugging unexpected model outputs, or for those wanting to implement custom model behaviors not available off-the-shelf.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Neural network architectures, PyTorch (or TensorFlow/JAX) deep learning frameworks, attention mechanisms, feed-forward networks, and embedding layers.\n",
    "- **Streaming Results**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It dramatically improves the perceived performance and user experience of applications that generate text sequentially, like chatbots or code generators.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Implementing responsive AI assistants, live translation services, interactive story generation tools, and any application where immediate partial output is better than waiting for the full generation.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Asynchronous programming, token-by-token generation, API design for real-time communication (e.g., WebSockets), and user interface design for interactive systems.\n",
    "\n",
    "### **Code Examples**\n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "- How can I apply this concept in my daily data science work or learning?\n",
    "    - By using the Hugging Face `model` class, I can directly load various open-source models, experiment with quantization to run them on my local machine, and analyze their outputs for specific tasks, which is more flexible than just using pipelines.\n",
    "- Can I explain this concept to a beginner in one sentence?\n",
    "    - The Hugging Face `model` class is like getting direct access to the engine of a car (the AI model), rather than just the steering wheel and pedals (the simpler pipeline), allowing you to fine-tune how it runs and observe its parts.\n",
    "- Which type of project or domain would this concept be most relevant to?\n",
    "    - This would be most relevant for projects requiring custom inference logic, model comparison studies, resource-constrained deployment (e.g., using quantization for mobile apps), or research into the internal workings of transformer models.\n",
    "\n",
    "# Day 4 - Hugging Face Transformers: Loading & Quantizing LLMs with Bits & Bytes\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This text details the practical steps for working with large language models (LLMs) in a Google Colab environment using Hugging Face libraries. It covers environment setup, model selection, and a deep dive into quantization‚Äîa technique to reduce model size and improve performance by lowering numerical precision‚Äîdemonstrating how to load a 4-bit quantized model and inspect its underlying PyTorch layers. This is crucial for data scientists to efficiently manage and run powerful open-source LLMs, like Llama 3.1, even with limited computational resources, and to gain a foundational understanding of their architecture.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- üõ†Ô∏è **Environment Setup & Model Selection:** The process begins with Pip installs, Hugging Face login, and defining constants for various open-source models (Llama 3.1, Phi 3, Gemma 2, Qwen2, Mistral). This is fundamental for organizing and executing model experiments in a reproducible manner.\n",
    "- üìâ **Quantization Explained:** A detailed explanation of quantization, the process of reducing the numerical precision of model weights (e.g., from 32-bit floats to 4-bit), to save memory and speed up inference with a tolerable trade-off in accuracy. This is highly relevant for deploying large models on consumer hardware or for efficient fine-tuning (as in QLoRA).\n",
    "- ‚öôÔ∏è **BitsAndBytesConfig for Quantization:** Introduction to using the `BitsAndBytesConfig` from the `bitsandbytes` library to specify quantization parameters like `load_in_4bit=True`, `bnb_4bit_use_double_quant=True`, `bnb_4bit_compute_dtype`, and `bnb_4bit_quant_type=\"nf4\"`. This provides practical control over how models are loaded and compressed.\n",
    "- üß© **Tokenizer and Chat Template Application:** Standard procedure of loading a tokenizer (`AutoTokenizer.from_pretrained`) and applying a chat template (`tokenizer.apply_chat_template`) to format input messages for the model. Setting `tokenizer.pad_token_id = tokenizer.eos_token_id` is mentioned as a common practice to avoid warnings. This ensures inputs are correctly structured for the model.\n",
    "- üöÄ **Loading Models with `AutoModelForCausalLM`:** Demonstrates loading a causal language model using `AutoModelForCausalLM.from_pretrained`, passing the model name, `device_map=\"auto\"` for GPU utilization, and the `quantization_config`. This is the core step for instantiating a model for inference.\n",
    "- üß† **Understanding Causal Language Models:** Clarifies that \"Causal LM\" is synonymous with \"autoregressive LM,\" meaning models that predict future tokens based on past tokens, which encompasses most generative AI models discussed. This conceptual understanding is vital for knowing the type of model being used.\n",
    "- üíæ **Model Caching and Memory Footprint:** Explanation of how models are downloaded from the Hugging Face Hub, cached locally on the Colab instance's disk, and loaded into memory. The `get_memory_footprint()` method is shown to check the model's memory usage. This is important for resource management.\n",
    "- üëÅÔ∏è **Inspecting Model Architecture:** Printing the loaded model object reveals its structure, showing the underlying PyTorch layers like `Embedding`, `LlamaAttention`, `LlamaMLP` (with activation functions like SiLU). This allows for a deeper understanding of the model's internal components and is useful for debugging or advanced customization.\n",
    "\n",
    "### **Conceptual Understanding**\n",
    "\n",
    "- **Environment Setup & Model Selection**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - A proper setup ensures all dependencies are met, and consistent model identifiers prevent errors, making experiments repeatable and organized.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Essential first step in any machine learning project, from research experiments comparing models to building production-level AI applications.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Virtual environments, package management (Pip), API authentication, version control, and selecting appropriate model architectures for specific tasks.\n",
    "- **Quantization Explained**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It makes large, powerful models accessible on less powerful hardware, democratizing AI development and deployment. It reduces costs and improves inference speed.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Deploying LLMs on edge devices (phones, IoT), running models on personal computers with limited GPU RAM, reducing cloud computing costs for inference, and enabling efficient fine-tuning of large models (e.g., QLoRA).\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Model compression, numerical precision (FP32, INT8, NF4), information theory, hardware limitations, and model performance trade-offs (speed/size vs. accuracy).\n",
    "- **BitsAndBytesConfig for Quantization**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It provides fine-grained control over the quantization process, allowing users to choose specific methods (e.g., 4-bit, double quantization, NF4 type) to balance performance and resource usage.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Used when implementing quantization in Python scripts with Hugging Face Transformers to optimize model loading and inference for specific hardware or performance targets.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - The `bitsandbytes` library, Hugging Face Transformers API, configuration objects in software development, and specific quantization algorithms like NF4 (Normalized Float 4).\n",
    "- **Tokenizer and Chat Template Application**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Models understand numbers (tokens), not raw text. Tokenization converts text to tokens, and chat templates structure conversational input correctly for dialogue models.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Fundamental for any application involving text input to an LLM, such as chatbots, question-answering systems, and text generation tools. The `pad_token_id` setting helps manage batch processing and avoids unnecessary warnings.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Natural Language Processing (NLP), tokenization algorithms (BPE, WordPiece), special tokens (EOS, BOS, PAD), and input formatting for neural networks.\n",
    "- **Loading Models with `AutoModelForCausalLM`**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - This is the primary Hugging Face class for loading pre-trained generative language models. `device_map=\"auto\"` simplifies distributing the model across available hardware (CPU/GPU).\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Used to instantiate models for text generation, summarization, translation, and other tasks requiring causal language modeling. It allows for easy switching between different model architectures.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Hugging Face Transformers library, pre-trained models, model hubs, GPU acceleration (CUDA), and an understanding of different model types (e.g., causal vs. masked language models).\n",
    "- **Understanding Causal Language Models**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Knowing that \"causal\" means \"autoregressive\" clarifies that the model generates text sequentially, predicting one token at a time based on previous tokens. This is characteristic of most large-scale text generation models.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Explains the generative process of models like GPT, Llama, and Gemma, which are used for chatbots, story writing, code generation, etc.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Autoregressive processes, sequence modeling, transformer architecture, and the distinction from other model types like masked language models (e.g., BERT) or encoder-decoder models.\n",
    "- **Model Caching and Memory Footprint**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Downloading large models takes time and bandwidth; caching speeds up subsequent loads. Monitoring memory footprint is crucial for avoiding out-of-memory errors and managing resources.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Relevant for efficient development workflows, managing disk space in cloud environments or local machines, and selecting appropriate hardware or quantization strategies based on available memory.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - File system caching, resource management, GPU memory, system monitoring, and debugging performance issues.\n",
    "- **Inspecting Model Architecture**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It demystifies the model by showing its constituent layers (embedding, attention, MLP, etc.) as PyTorch objects, offering insights into its complexity and operation.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Useful for researchers studying model behavior, developers looking to customize model components, or anyone curious about the practical implementation of theoretical deep learning concepts. Helps in comparing architectures.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Deep learning frameworks (PyTorch), neural network layers, transformer architecture specifics (attention mechanisms, feed-forward networks), activation functions (e.g., SiLU: x‚ãÖœÉ(x)), and model debugging.\n",
    "\n",
    "### **Code Examples**\n",
    "\n",
    "```python\n",
    "# Setting constants for model names\n",
    "LLAMA_MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "GEMMA_MODEL_NAME = \"google/gemma-2-9b-it\" # Example, original might be different\n",
    "QWEN_MODEL_NAME = \"qwen/Qwen1.5-7B-Chat\" # Example, original stated \"quanto\"\n",
    "MISTRAL_MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\" # Example, original mentions it might be too big\n",
    "\n",
    "# Messages list format\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful pirate chatbot who answers questions in the style of a pirate.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the best way to find treasure?\"}\n",
    "]\n",
    "\n",
    "# Quantization Configuration using BitsAndBytesConfig\n",
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\", # Normalized Float 4\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# Tokenizer setup\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(LLAMA_MODEL_NAME)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id # Common practice\n",
    "\n",
    "# Apply chat template\n",
    "inputs = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=True,\n",
    "    add_generation_prompt=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "inputs = inputs.to(\"cuda\") # Move inputs to GPU\n",
    "\n",
    "# Model loading with quantization\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    LLAMA_MODEL_NAME,\n",
    "    device_map=\"auto\", # Use GPU if available\n",
    "    quantization_config=quant_config\n",
    ")\n",
    "\n",
    "# Check memory footprint\n",
    "print(f\"Model memory footprint: {model.get_memory_footprint()} bytes\")\n",
    "\n",
    "# Print model architecture\n",
    "# print(model)\n",
    "# Output would be a description of PyTorch layers:\n",
    "# LlamaForCausalLM(\n",
    "#   (model): LlamaModel(\n",
    "#     (embed_tokens): Embedding(128256, 4096)\n",
    "#     (layers): ModuleList(\n",
    "#       (0-31): 32 x LlamaDecoderLayer(\n",
    "#         (self_attn): LlamaSdpaAttention(  # Or LlamaAttention depending on version\n",
    "#           (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
    "#           (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
    "#           (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
    "#           (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
    "#           (rotary_emb): LlamaRotaryEmbedding()\n",
    "#         )\n",
    "#         (mlp): LlamaMLP(\n",
    "#           (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
    "#           (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
    "#           (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
    "#           (act_fn): SiLU()  # Sigmoid Linear Unit (Swish)\n",
    "#         )\n",
    "#         (input_layernorm): LlamaRMSNorm()\n",
    "#         (post_attention_layernorm): LlamaRMSNorm()\n",
    "#       )\n",
    "#     )\n",
    "#     (norm): LlamaRMSNorm()\n",
    "#   )\n",
    "#   (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
    "# )\n",
    "\n",
    "```\n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "- How can I apply this concept in my daily data science work or learning?\n",
    "    - I can use quantization to run larger, more capable models on my local GPU for experimentation or small projects, which would otherwise be inaccessible due to memory constraints. This allows me to explore SOTA models more freely.\n",
    "- Can I explain this concept to a beginner in one sentence?\n",
    "    - Quantization is like making the numbers in an AI model less precise (e.g., using fewer decimal places) so the model takes up less space and runs faster, usually without losing too much of its smarts.\n",
    "- Which type of project or domain would this concept be most relevant to?\n",
    "    - This is highly relevant for deploying LLMs on resource-constrained devices (like mobile phones or edge computers), for hobbyists or researchers with limited GPU access wanting to experiment with large models, and for companies looking to reduce the inference costs of their AI services.\n",
    "\n",
    "# Day 4 - Hugging Face Transformers: Generating Jokes with Open-Source AI Models\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This text focuses on the practical application of the Hugging Face `model` class for generating text, specifically using the `model.generate()` method with open-source models like Llama 3.1, Phi 3, and Gemma 2. It emphasizes understanding model architecture by examining dimensionality, showcases how even heavily quantized models can produce decent results, introduces text streaming for real-time output using `TextStreamer`, and encourages experimentation with different models and prompts. This is important for data scientists to effectively leverage and compare open-source LLMs for various tasks, manage resources, and enhance user experience in interactive applications.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- üëÅÔ∏è **Inspecting Model Dimensionality:** Advises looking at input/output dimensions in the model architecture (e.g., vocab size) to understand data flow. This aids in grasping the model's structure and potential transformations.\n",
    "- üöÄ **Text Generation with `model.generate()`:** Demonstrates using the `model.generate(inputs, max_new_tokens=...)` method to produce text from tokenized inputs. This is the core function for inference with Hugging Face transformer models.\n",
    "- üí¨ **Decoding Output:** Shows how `tokenizer.decode()` is used to convert the model's output tokens back into human-readable text. This is a necessary step to interpret model generations.\n",
    "- ‚úÖ **Performance of Quantized Models:** Highlights that a heavily quantized (4-bit, double quantized) Llama 3.1 8B model can still generate coherent and contextually relevant jokes. This underscores the effectiveness of quantization for resource-efficient deployment.\n",
    "- üßπ **Memory Management:** Stresses the importance of cleaning up model and tokenizer objects (`del model`, `del tokenizer`, `torch.cuda.empty_cache()`) to free GPU memory when working with multiple models sequentially. This is crucial to prevent out-of-memory errors in environments like Google Colab.\n",
    "- üéÅ **Reusable Generation Function:** A Python function is created to encapsulate the entire process: loading the tokenizer, applying chat templates, loading the quantized model, generating text (with streaming), and cleaning up. This promotes modular, reusable code for model inference.\n",
    "- üåä **Streaming Output with `TextStreamer`:** Introduces the `TextStreamer` class from Hugging Face for streaming output token by token as it's generated. This significantly improves user experience in interactive applications by providing immediate feedback.\n",
    "- üîÑ **Comparative Model Behavior:** Compares the outputs of Llama 3.1, Phi 3, and Gemma 2 (a 2B model) on the same joke-telling task, noting variations in performance and adherence to prompts (e.g., Phi 3 struggled, Gemma 2 gave a nerdy joke). This illustrates the need for model-specific prompting and evaluation.\n",
    "- üìù **Model-Specific Prompting:** Notes that Gemma 2 doesn't support system prompts, requiring direct user prompts. This emphasizes that different models may have unique input requirements or behaviors.\n",
    "- üß™ **Encouragement for Exploration:** Motivates users to experiment with other models (Qwen2, Mistral, or smaller alternatives), try different prompts (e.g., for math questions), and explore the capabilities of open-source LLMs without API costs. This fosters hands-on learning and discovery.\n",
    "\n",
    "### **Conceptual Understanding**\n",
    "\n",
    "- **Inspecting Model Dimensionality**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Understanding the dimensions of layers (embedding, hidden states, output logits) helps in comprehending how information is transformed within the model and ensures compatibility between different components.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Useful for debugging model issues, custom model modifications, and advanced analysis of model architectures. For instance, knowing the vocabulary size dimension is key for the embedding and final output layers.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Neural network architecture, tensor shapes, vocabulary size, embedding layers, linear layers, and model configuration files.\n",
    "- **Text Generation with `model.generate()`**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It's the primary interface for eliciting responses from generative language models in the Hugging Face ecosystem. Understanding its parameters (`max_new_tokens`, `streamer`, etc.) allows for control over the generation process.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Used in chatbots, content creation tools, summarization services, machine translation, and any application requiring automated text generation.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Autoregressive decoding, sampling strategies (greedy search, beam search, top-k, nucleus sampling), tokenization, and model inference pipelines.\n",
    "- **Decoding Output**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Models operate on numerical tokens; decoding converts these back into human-understandable text, making the model's output useful.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - An essential final step in any LLM application that presents text to a user or uses text for further processing.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Tokenization (the reverse process), vocabulary mapping, handling of special tokens, and text processing.\n",
    "- **Performance of Quantized Models**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Demonstrates that quantization offers significant benefits in terms of model size and speed with often acceptable, minor impacts on performance for many tasks.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Enables the deployment of sophisticated LLMs on devices with limited resources (edge AI, mobile) and makes working with large models more accessible for researchers and developers.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Model optimization, quantization techniques (4-bit, 8-bit, NF4), accuracy vs. efficiency trade-offs, and evaluation of LLM performance.\n",
    "- **Memory Management**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - LLMs, especially large ones, consume significant GPU memory. Proper cleanup prevents crashes and allows for efficient use of available resources, particularly in shared or limited environments.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Critical when running multiple experiments, deploying models in production where resource contention can occur, or working within free-tier cloud notebook environments.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - GPU memory allocation, garbage collection (in Python), PyTorch CUDA utilities (`torch.cuda.empty_cache()`), and resource monitoring.\n",
    "- **Reusable Generation Function**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Encapsulating complex workflows into functions makes code cleaner, easier to understand, debug, and reuse across different models or experiments.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Standard software engineering practice applied to machine learning pipelines, facilitating rapid experimentation and building more complex applications.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Procedural programming, function abstraction, modular design, and software development best practices.\n",
    "- **Streaming Output with `TextStreamer`**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Provides immediate feedback to users in interactive applications, improving perceived performance and engagement, as users see text appearing token by token.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Essential for chatbots, live coding assistants, real-time translation, and any application where users interact with an LLM and expect progressive output.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Asynchronous processing, callback functions, user interface design, and real-time communication protocols.\n",
    "- **Comparative Model Behavior**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Different models, even of similar sizes or from different families, can have varied strengths, weaknesses, and responses to the same prompt. This necessitates empirical testing.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Informs model selection for specific tasks. If one model fails at a task (e.g., telling a joke), another might excel, or prompt engineering might be required.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Model evaluation, benchmarking, prompt engineering, and understanding the nuances of different LLM architectures and training data.\n",
    "- **Model-Specific Prompting**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - The way a model is trained influences how it expects input. Some models have specific prompt formats or sensitivities (e.g., system prompts, instruction tuning).\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Crucial for achieving optimal performance from a chosen LLM. Failure to use the correct prompting strategy can lead to suboptimal or irrelevant outputs.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Prompt engineering, instruction fine-tuning, model documentation, and chat templates.\n",
    "- **Encouragement for Exploration**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - The field of open-source LLMs is rapidly evolving. Hands-on experimentation is key to understanding model capabilities, limitations, and finding novel applications.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Drives innovation, helps in selecting the best tools for a job, and builds practical skills in working with cutting-edge AI. The low cost barrier of open-source models facilitates this.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Lifelong learning, research and development, community engagement (e.g., Hugging Face Hub), and adapting to new technologies.\n",
    "\n",
    "### **Code Examples**\n",
    "\n",
    "```python\n",
    "# Generating text with the loaded model\n",
    "# Assuming 'model' and 'tokenizer' are loaded, and 'inputs' are prepared\n",
    "# (inputs = tokens on GPU)\n",
    "\n",
    "# Example for Llama 3.1 (from previous context)\n",
    "# outputs = model.generate(inputs, max_new_tokens=80)\n",
    "# decoded_output = tokenizer.decode(outputs[0])\n",
    "# print(decoded_output)\n",
    "\n",
    "# Memory cleanup (conceptual)\n",
    "# import torch\n",
    "# del model\n",
    "# del tokenizer\n",
    "# torch.cuda.empty_cache()\n",
    "\n",
    "# Reusable function for generation with streaming\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# Assuming quant_config is defined as in the previous segment\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "def generate_with_model(model_name, messages_list):\n",
    "    print(f\"\\n--- Running model: {model_name} ---\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages_list,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=quant_config\n",
    "    )\n",
    "\n",
    "    _ = model.generate(inputs, max_new_tokens=80, streamer=streamer)\n",
    "\n",
    "    # Cleanup\n",
    "    del model\n",
    "    del tokenizer\n",
    "    del streamer # if it holds significant resources, though usually not critical\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"\\n--- Model run complete and cleaned up ---\")\n",
    "\n",
    "# Example messages (system prompt might be ignored by some models like Gemma)\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful pirate chatbot who answers questions in the style of a pirate.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a lighthearted joke appropriate for a room full of data scientists.\"}\n",
    "]\n",
    "\n",
    "# Calling the function for Phi 3\n",
    "PHI3_MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "# generate_with_model(PHI3_MODEL_NAME, messages)\n",
    "# (Output for Phi 3 was described as rambling, not a direct joke in the transcript)\n",
    "\n",
    "# Calling the function for Gemma 2 (noting it doesn't support system prompt well)\n",
    "GEMMA_MODEL_NAME = \"google/gemma-2-9b-it\" # Or \"google/gemma-2b-it\" if 2B version\n",
    "messages_for_gemma = [\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a lighthearted joke appropriate for a room full of data scientists.\"}\n",
    "]\n",
    "# generate_with_model(GEMMA_MODEL_NAME, messages_for_gemma)\n",
    "# Example output joke from transcript for Gemma:\n",
    "# \"Why did the data scientist break up with the statistician? Because they had too many disagreements about the p-value.\"\n",
    "\n",
    "```\n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "- How can I apply this concept in my daily data science work or learning?\n",
    "    - I can use the `TextStreamer` when building interactive demos or simple chatbots with Hugging Face models to make the experience more engaging by showing results as they are generated, rather than waiting for the full response.\n",
    "- Can I explain this concept to a beginner in one sentence?\n",
    "    - The `model.generate()` function is how you ask a loaded AI model to create new text based on your input, and you can even make it show you the text word by word as it thinks.\n",
    "\n",
    "# Day 4 - Mastering Hugging Face Transformers: Models, Pipelines, and Tokenizers\n",
    "\n",
    "### Summary\n",
    "\n",
    "This text serves as a wrap-up, congratulating the learner on mastering the Hugging Face Transformers library, including `pipelines`, `tokenizers`, and now the `model` class for loading, inspecting, and running open-source models for various text generation tasks. It emphasizes the significant skills acquired and previews the next session, which will involve a project combining frontier model APIs with open-source models to build a practical business application, further solidifying the week's learning.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- üéâ **Skill Consolidation:** Learners can now confidently use Hugging Face `pipelines` (high-level API), `tokenizers` (for text preprocessing), and the `model` class (for direct model interaction and inference). This comprehensive skill set is vital for working effectively with transformer models.\n",
    "- üõ†Ô∏è **Proficiency with Open-Source Models:** The ability to load different open-source models, inspect their architecture, and run them for text generation (beyond just jokes) has been achieved. This empowers users to leverage a wide array of freely available powerful AI models.\n",
    "- üöÄ **Broader AI Capabilities:** Reinforces that these new skills complement existing abilities in coding with frontier model APIs, building multimodal AI assistants, and utilizing external tools with LLMs. This creates a well-rounded expertise in applied AI.\n",
    "- üîú **Upcoming Integrated Project:** The next session will feature a hands-on project to implement an LLM solution that synergizes calls to both frontier (proprietary) models and open-source models. This is relevant for building sophisticated, cost-effective,"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
