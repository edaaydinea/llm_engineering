{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3a64404",
   "metadata": {},
   "source": [
    "# Day 3 - Exploring Tokenizers in Open-Source AI: Llama, Phi-2, Qwen, & Starcoder\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This content delves into the lower-level APIs of the Hugging Face Transformers library, specifically focusing on tokenizers. Tokenizers are crucial components that translate between human-readable text and numerical tokens that machine learning models can process, enabling the interaction with these models at a more granular level than pre-built pipelines. Understanding tokenizers is essential for customizing model behavior, preparing data for training, and interpreting model inputs and outputs accurately.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- ‚ú® **Tokenizer Functionality**: Tokenizers convert text strings into lists of numerical tokens (encoding) and vice-versa (decoding). This is fundamental for preparing text data for transformer models and understanding their outputs, enabling tasks like text generation, classification, and translation.\n",
    "- üîë **Model-Specificity**: Each transformer model typically has its own specific tokenizer used during its training. Using the correct tokenizer during inference is critical for accurate results, as mismatches can lead to poor performance or nonsensical outputs. This is vital for anyone working with pre-trained models to ensure compatibility.\n",
    "- üìö **Tokenizer Components - Vocab**: A tokenizer contains a vocabulary (vocab), which is a collection of all unique character fragments (tokens) the model knows. This vocabulary dictates how words and sub-words are broken down, impacting the model's ability to understand and generate nuanced text.\n",
    "- üè∑Ô∏è **Tokenizer Components - Special Tokens**: Tokenizers can include special tokens that convey specific instructions or structural information to the model, such as 'start of sentence', 'end of sentence', or markers for dialogue turns (e.g., 'assistant message'). These are learned during training and help guide the model's behavior and understanding of context.\n",
    "- üí¨ **Tokenizer Components - Chat Templates**: For conversational models, tokenizers often include chat templates. These templates define how a sequence of messages (e.g., system, user, assistant) is formatted into a single string of tokens that the model can understand, which is essential for building chatbots and AI assistants.\n",
    "- üåç **Diverse Model Examples**: The discussion mentions specific tokenizers for various models like Llama 3.1, Phi 3, Qwen2 (multilingual focus), and StarCoder 2 (code generation). This highlights that tokenization strategies can vary depending on the model's purpose and training data, such as handling different languages or specialized content like code.\n",
    "\n",
    "### **Conceptual Understanding**\n",
    "\n",
    "- **‚ú® Tokenizer Functionality**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It's the bridge between human language and the numerical input that models require. Without tokenization, text data cannot be processed by these neural networks. Understanding encoding/decoding helps in debugging and fine-tuning model interactions.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Essential for any NLP task: machine translation (breaking down sentences in one language to generate in another), sentiment analysis (converting reviews into processable input), text generation (converting model output tokens back to readable text), and search engines (tokenizing queries and documents).\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Natural Language Processing (NLP), machine learning data preprocessing, embeddings (tokens are often mapped to embedding vectors), sequence-to-sequence models, and language modeling.\n",
    "- **üîë Model-Specificity**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Ensures that the input data during inference is processed in exactly the same way as the data the model was trained on. Using an incorrect tokenizer leads to a mismatch in the \"language\" the model expects, resulting in degraded performance or errors.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Critical when deploying pre-trained models from hubs like Hugging Face. If you download a model, you must use its designated tokenizer. This impacts reproducibility and reliability of NLP applications in production.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Model training, transfer learning, model deployment, inference, and maintaining consistent data pipelines in machine learning operations (MLOps).\n",
    "- **üìö Tokenizer Components - Vocab**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - The vocabulary defines the set of basic units (tokens) the model can recognize and generate. The size and composition of the vocab affect the model's granularity, its ability to handle rare words (out-of-vocabulary issue), and overall efficiency.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - In multilingual models, the vocab must cover characters and sub-words from multiple languages. For specialized domains like medicine or law, the vocab might include specific jargon. In code generation, it includes programming language keywords and symbols.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Subword tokenization algorithms (e.g., BPE, WordPiece, SentencePiece), out-of-vocabulary (OOV) handling, computational linguistics, and information theory (compression).\n",
    "- **üè∑Ô∏è Tokenizer Components - Special Tokens**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Special tokens provide crucial metadata or structural cues to the model beyond the content of the text itself. They help models understand task-specific formats, delineate segments of input, or trigger particular behaviors learned during training.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Used in tasks like text classification (e.g., `[CLS]` token in BERT for sentence-level representation), sequence pair tasks (e.g., `[SEP]` to separate two sentences), and guiding generative models (e.g., `[BOS]` for beginning of sequence, `[EOS]` for end of sequence). For chat, they mark speaker roles.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Model pre-training objectives, fine-tuning, sequence labeling, attention mechanisms (which might focus on these tokens), and dialogue systems.\n",
    "- **üí¨ Tokenizer Components - Chat Templates**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - They standardize the format for conversational input, ensuring that multi-turn dialogues are presented to the model in a consistent way that aligns with its training. This is vital for conversational AI to understand the flow and roles in a dialogue.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Directly used in building chatbots, virtual assistants, and any application involving interactive dialogue with a language model. Ensures the model can differentiate between user queries, system instructions, and its own previous responses.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Dialogue management, conversational AI, prompt engineering, instruction fine-tuning, and human-computer interaction.\n",
    "- **üåç Diverse Model Examples**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It shows that tokenization is not a one-size-fits-all process. Different models are optimized for different tasks or languages, and their tokenizers reflect these specializations (e.g., larger vocabularies for multilingual models, specific tokens for code syntax).\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - When selecting a model for a specific task (e.g., code generation with StarCoder 2, multilingual applications with Qwen2), understanding its tokenizer's characteristics can inform the choice and highlight potential data preprocessing needs.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Model selection, multilingual NLP, code intelligence, domain adaptation, and the ongoing research in developing more efficient and versatile tokenization methods.\n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "- How can I apply this concept in my daily data science work or learning?\n",
    "    - When working with any pre-trained transformer model from Hugging Face, I must always load the specific tokenizer associated with that model to correctly preprocess input text and decode output tokens, ensuring compatibility and optimal performance for tasks like text classification or generation.\n",
    "- Can I explain this concept to a beginner in one sentence?\n",
    "    - A tokenizer is like a special dictionary and rulebook that chops up text into tiny pieces (tokens) a computer model can understand, and then reassembles those pieces back into readable text.\n",
    "- Which type of project or domain would this concept be most relevant to?\n",
    "    - This concept is most relevant to any project involving Natural Language Processing (NLP) with transformer models, such as building chatbots, translation services, text summarizers, sentiment analyzers, or code generation tools, across various domains like customer service, healthcare, finance, and software development.\n",
    "\n",
    "# Day 3 - Tokenization Techniques in AI: Using AutoTokenizer with LLAMA 3.1 Model\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This content provides a practical guide to using Hugging Face tokenizers within a Google Colab environment, focusing on the Llama 3.1 model as an initial example. It covers essential setup steps like Hugging Face login, agreeing to model terms of service, and then demonstrates core tokenizer functionalities such as loading a tokenizer using `AutoTokenizer`, encoding text into tokens, decoding tokens back to text, and exploring tokenizer components like special tokens and the vocabulary. The practical importance lies in understanding how to prepare text data for language models and interpret their outputs at a fundamental level.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- üõ†Ô∏è **Hugging Face Login & Setup**: The text explains how to log into Hugging Face from Colab using an API token. This is crucial for accessing certain models and functionalities, including private models or those requiring user agreements, and for future tasks like uploading models.\n",
    "- üìú **Model Terms of Service (ToS)**: For models like Llama 3.1, users must agree to terms of service via the Hugging Face model page. This is a necessary administrative step to gain access to the model and its tokenizer, relevant for ethical and compliant use of powerful AI models.\n",
    "- üîÑ **`AutoTokenizer.from_pretrained()`**: This class method is the standard way to load the correct tokenizer for a given pre-trained model. It simplifies the process by automatically selecting the appropriate tokenizer class, vital for ensuring compatibility between the model and its tokenizer.\n",
    "- üî¢ **`tokenizer.encode()`**: This method converts a string of text into a list of numerical token IDs. This is the primary step in preparing text input for a transformer model, fundamental for any NLP task.\n",
    "- üìù **`tokenizer.decode()`**: This method converts a list of numerical token IDs back into a human-readable text string. It's essential for interpreting the output of language models.\n",
    "- ‚ú® **Special Tokens**: The tokenizer automatically adds special tokens (e.g., `<|begin_of_text|>`) during encoding, which signal structural information or specific contexts to the model (like start of a prompt). These are learned during training and important for guiding model behavior.\n",
    "- üß© **`tokenizer.batch_decode()`**: This method decodes a list of tokens but returns a list of strings, where each string represents an individual token. This is useful for inspecting how text is broken down and understanding the tokenization process at a granular level.\n",
    "- üìñ **Tokenizer Vocabulary (`tokenizer.vocab`)**: This attribute provides access to the tokenizer's full vocabulary, a dictionary mapping token strings (character fragments) to their corresponding numerical IDs. Exploring this helps understand the range of text units the model can process.\n",
    "- ‚ûï **Added Special Vocabulary (`tokenizer.added_tokens_decoder`)**: This provides access to the special tokens that have been explicitly added to the tokenizer, beyond the regular vocabulary, such as `begin_of_text`, `end_of_text`, etc. These are key for model control.\n",
    "- üìè **Token-to-Character Ratio**: The text mentions the rule of thumb that approximately four characters map to one token for English text. This provides a rough estimate for input length considerations and cost in token-based API usage.\n",
    "- ü§î **Tokenization Details**: The example shows that tokenization can be case-sensitive, spaces before words are often part of the token, and words can be broken into sub-word units (e.g., \"Tokenizers\" -> \"Token\", \"izers\"). This is important for understanding model sensitivity and representation of text.\n",
    "\n",
    "### **Conceptual Understanding**\n",
    "\n",
    "- **üõ†Ô∏è Hugging Face Login & Setup**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It's a gateway to accessing a vast ecosystem of models and tools. Authentication is often required for gated models, private repositories, or contributing back to the community.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Necessary for using cutting-edge models that require explicit user agreement (like Llama 3.1), pushing fine-tuned models to the Hugging Face Hub, or collaborating on private projects.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - API key management, secure authentication, cloud services, MLOps (model sharing and versioning), and data privacy/security.\n",
    "- **üìú Model Terms of Service (ToS)**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Ensures responsible and legal use of powerful AI models. Model creators set these terms to prevent misuse and outline permitted applications.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Directly impacts which models a developer or organization can legally use for their projects, especially in commercial applications or sensitive domains. It's a key part of AI ethics and governance.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - AI ethics, legal compliance, responsible AI development, licensing agreements, and intellectual property.\n",
    "- **üîÑ `AutoTokenizer.from_pretrained()`**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It abstracts away the complexity of knowing the specific tokenizer class for each of the thousands of models available. It ensures you load the correct, compatible tokenizer for a chosen model checkpoint.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Simplifies the workflow for data scientists and ML engineers when experimenting with different models, reducing errors and setup time. It's a cornerstone of the Hugging Face library's ease of use.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Software abstraction, factory patterns, model management, and pre-trained model ecosystems.\n",
    "- **üî¢ `tokenizer.encode()`**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - This is the first step in transforming raw text data into a format that neural networks can process. Models operate on numbers, not raw strings.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Used in every NLP application that feeds text to a model: preparing user queries for a chatbot, processing documents for summarization, converting sentences for translation, etc.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Data preprocessing, feature engineering in NLP, input pipelines for machine learning, and numerical representation of text.\n",
    "- **üìù `tokenizer.decode()`**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It translates the model's numerical output back into human-understandable language, allowing us to interpret results, generate text, or present answers to users.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Used to display generated text from a language model, present translated sentences, show summarized content, or reveal the model's \"thoughts\" in intermediate steps.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Post-processing, text generation, natural language generation (NLG), and output interpretation in machine learning.\n",
    "- **‚ú® Special Tokens**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - These tokens act as control signals or structural markers that guide the model's interpretation and generation process, beyond the semantic content of the text itself. They are crucial for tasks requiring specific input formats or contextual cues.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - In chat models, they delineate turns (user, assistant). In other models, they might mark sentence boundaries (`[SEP]`), class labels for classification (`[CLS]`), or padding (`[PAD]`). The `begin_of_text` token primes the model for a new input sequence.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Model architecture (how models learn to use these), training data preparation, prompt engineering, and fine-tuning for specific tasks.\n",
    "- **üß© `tokenizer.batch_decode()`**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It allows for a more detailed inspection of the tokenization process, showing how individual tokens correspond to parts of the input text. This is helpful for debugging and understanding subword tokenization.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Useful during development and debugging to verify that text is being tokenized as expected, especially when dealing with complex strings, different languages, or out-of-vocabulary words. Helps in analyzing why a model might behave unexpectedly for certain inputs.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Debugging, data exploration, subword tokenization algorithms (like BPE, WordPiece), and understanding model input representation.\n",
    "- **üìñ Tokenizer Vocabulary (`tokenizer.vocab`)**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - The vocabulary defines the set of all possible tokens (word pieces, characters, special symbols) that the model can recognize and produce. Its size and composition influence the model's expressiveness and ability to handle diverse text.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - A larger vocab might handle more diverse language but increases model size. Specialized vocabs are used for domains like programming languages (StarCoder) or multiple human languages (Qwen2). Understanding the vocab helps in tasks like checking for out-of-vocabulary words.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Lexicography, subword tokenization, out-of-vocabulary (OOV) handling, model size, and computational linguistics.\n",
    "- **‚ûï Added Special Vocabulary (`tokenizer.added_tokens_decoder`)**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It lists the explicit control tokens (like `<|begin_of_text|>`, `<|end_of_text|>`, chat role markers) that are treated uniquely by the model. Knowing these is key to correctly formatting inputs for specific tasks like chat or structured generation.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Essential for constructing correct prompts for conversational AI, instruction-tuned models, or any model that relies on these specific markers to understand the input structure.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Prompt engineering, instruction fine-tuning, chat templates, and model-specific input formatting.\n",
    "- **üìè Token-to-Character Ratio**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Provides a practical way to estimate the number of tokens an input text will produce, which is important for understanding computational load, API costs (if applicable), and staying within model context limits.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Useful for planning resource allocation for NLP tasks, estimating costs for using commercial NLP APIs, and designing systems that handle text inputs of varying lengths.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Computational efficiency, API pricing models, context window limitations of transformer models, and text preprocessing.\n",
    "- **ü§î Tokenization Details (Case Sensitivity, Spaces, Subwords)**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - These details reveal the nuances of how text is processed. Case sensitivity means \"Word\" and \"word\" can be different tokens. Spaces being part of tokens affects how words are segmented. Subword tokenization allows models to handle rare words and create new words from known parts.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Impacts how you should preprocess text (e.g., to lowercase or not). Explains why a model might be sensitive to subtle input variations. Subword units help in tasks like machine translation (morphology) and code generation (identifiers).\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Text preprocessing, subword tokenization algorithms (BPE, WordPiece, SentencePiece), morphology in linguistics, and model robustness.\n",
    "\n",
    "### **Code Examples**\n",
    "\n",
    "```python\n",
    "# Hugging Face Login (conceptual, actual token obtained from secrets)\n",
    "from huggingface_hub import login\n",
    "from google.colab import userdata\n",
    "\n",
    "# HUGGING_FACE_TOKEN = userdata.get('HF_TOKEN') # Get token from Colab secrets\n",
    "# login(HUGGING_FACE_TOKEN)\n",
    "# print(\"Logged in with write permission:\", login(HUGGING_FACE_TOKEN, add_to_git_credential=False)) # Example from video\n",
    "# Mocked actual login call:\n",
    "# login(token=userdata.get('HF_TOKEN'))\n",
    "\n",
    "# Load tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B\" # Example model ID\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "# Text to tokenize\n",
    "text = \"I'm excited to show Tokenizers in action to my LLM engineers\"\n",
    "\n",
    "# Encode text\n",
    "tokens = tokenizer.encode(text)\n",
    "# print(tokens)\n",
    "\n",
    "# Get number of characters and tokens\n",
    "# print(f\"Number of characters: {len(text)}\")\n",
    "# print(f\"Number of tokens: {len(tokens)}\")\n",
    "\n",
    "# Decode tokens\n",
    "decoded_text = tokenizer.decode(tokens)\n",
    "# print(decoded_text)\n",
    "\n",
    "# Batch decode tokens\n",
    "batch_decoded_tokens = tokenizer.batch_decode(tokens)\n",
    "# print(batch_decoded_tokens)\n",
    "\n",
    "# Inspect vocabulary (conceptual, output is very large)\n",
    "vocab_sample = dict(list(tokenizer.vocab.items())[:5]) # Show a small sample\n",
    "print(vocab_sample)\n",
    "print(f\"Vocabulary size: {len(tokenizer.vocab)}\")\n",
    "\n",
    "# Inspect added special tokens\n",
    "added_special_tokens = tokenizer.added_tokens_decoder\n",
    "print(added_special_tokens)\n",
    "# Example from video, getting specific special tokens:\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(tokenizer.eos_token_id)\n",
    "print(tokenizer.pad_token_id)\n",
    "print(tokenizer.unk_token_id)\n",
    "print(tokenizer.all_special_ids)\n",
    "print(tokenizer.all_special_tokens)\n",
    "print(tokenizer.added_tokens_decoder) # shows map from ID to string for added tokens\n",
    "\n",
    "```\n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "- How can I apply this concept in my daily data science work or learning?\n",
    "    - I can use `AutoTokenizer` to quickly load the correct tokenizer for any Hugging Face model I'm experimenting with, then use `encode` to prepare my text data for model input and `decode` to understand the model's output, ensuring I handle special tokens and model-specific formatting like that shown for Llama 3.1.\n",
    "- Can I explain this concept to a beginner in one sentence?\n",
    "    - Tokenizers are essential tools that convert text into a list of numbers (tokens) that AI models can understand and then turn those numbers back into readable text, with special codes to guide the model.\n",
    "- Which type of project or domain would this concept be most relevant to?\n",
    "    - This is highly relevant for any project involving pre-trained language models for tasks like chatbot development (formatting dialogues with special tokens), text generation (understanding `begin_of_text`), or any NLP task where precise control over input representation and output interpretation is needed, especially when using models with specific licensing or access requirements like Llama 3.1.\n",
    "\n",
    "# Day 3 - Comparing Tokenizers: Llama, PHI-3, and QWEN2 for Open-Source AI Models\n",
    "\n",
    "### \n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This text explores \"instruct\" fine-tuned models designed for chat conversations and how their corresponding tokenizers use chat templates to format dialogue into a single prompt string with special tokens that the model expects. It demonstrates the `apply_chat_template` function, showing how a standardized list of messages (system, user, assistant) is converted into model-specific input, and then contrasts the tokenization and chat template structures of Llama 3.1, Phi 3, Qwen2, and the code-specialized StarCoder 2, emphasizing the critical need to use the correct tokenizer for each model to avoid errors and ensure meaningful interactions.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- üí¨ **Instruct Models & Chat Structure**: Many models are fine-tuned as \"instruct\" variants specifically for chat, expecting prompts structured with special tokens to differentiate system messages, user inputs, and assistant responses. This training allows them to follow conversational flow. Its relevance lies in enabling effective multi-turn conversations with AI.\n",
    "- üìú **`apply_chat_template()` Function**: Tokenizers for instruct models have an `apply_chat_template()` method that converts a list of dictionaries (each with 'role' and 'content') into the specific flat string format, including all necessary special tokens, that the model was trained on. This is crucial for correctly formatting conversational input for chat models.\n",
    "- üîÑ **Standardized Message Format to Model-Specific Prompt**: The common input format for chat (a list of `{\"role\": \"...\", \"content\": \"...\"}` dictionaries) is transformed by `apply_chat_template` into a unique prompt string tailored for each model. This closes the loop on why this list-of-dicts format is used with various AI APIs.\n",
    "- üÜö **Tokenizer & Chat Template Variability**: Different models (e.g., Llama 3.1, Phi 3, Qwen2) have distinct token outputs for the same text and vastly different chat template structures, using unique special tokens and formatting. This highlights that tokenization and prompt formatting are not universal.\n",
    "- ‚ö†Ô∏è **Criticality of Matching Tokenizer to Model**: Using a tokenizer designed for one model with a different model will likely result in \"garbage\" or meaningless input because the token IDs and structural special tokens will not align with what the target model expects. This is a fundamental principle for correct model usage.\n",
    "- üíª **Specialized Tokenizers (StarCoder 2)**: Models designed for specific tasks, like code generation (StarCoder 2), have tokenizers optimized for that domain's syntax and common constructs (e.g., 'def', 'hello_world', indentation). This leads to more efficient and meaningful tokenization of code compared to general-purpose language tokenizers.\n",
    "- üîç **Exploring Tokenizer Differences**: The text encourages experimenting with different tokenizers to see how they handle various inputs, such as finding the longest or rarest word that maps to a single token, or comparing how code is tokenized by general vs. code-specific tokenizers. This fosters deeper understanding.\n",
    "\n",
    "### **Conceptual Understanding**\n",
    "\n",
    "- **üí¨ Instruct Models & Chat Structure**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Instruct models are the backbone of modern chatbots and conversational AI. Understanding that they are trained to expect a specific chat structure (marked by special tokens) is key to interacting with them effectively and eliciting desired responses.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Essential for building any application that involves dialogue with an AI, such as customer service bots, virtual assistants, interactive tutors, or creative writing partners. The structure enables context-aware, multi-turn conversations.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Fine-tuning, prompt engineering, dialogue management, natural language understanding (NLU), and human-computer interaction (HCI).\n",
    "- **üìú `apply_chat_template()` Function**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It provides a standardized way to prepare conversational data for various instruct models, abstracting away the model-specific formatting details. This simplifies development when working with different chat models.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Developers use this to ensure their application correctly formats user and system messages before sending them to a chat model, regardless of whether they're using Llama, Phi, or another instruct model. It promotes code reusability.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - API design, data preprocessing for NLP, software abstraction, and model interoperability (at the input formatting level).\n",
    "- **üîÑ Standardized Message Format to Model-Specific Prompt**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It clarifies how high-level conversational structures (like a list of messages) are translated into the low-level linear sequence of tokens that models actually process. This bridges the gap between user-friendly interaction paradigms and model input requirements.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - This understanding is crucial for debugging chat interactions, customizing system prompts effectively, and for developers integrating various LLM APIs (like OpenAI, Claude, Hugging Face models) that often use similar input message structures.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Data serialization, prompt engineering, model input tokenization, and the internal workings of conversational AI systems.\n",
    "- **üÜö Tokenizer & Chat Template Variability**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - It underscores that there's no \"universal tokenizer\" or \"universal chat format.\" Each model family or even specific variant can have its own way of breaking down text and structuring dialogue, which is tied to its unique training process.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - When switching between models (e.g., from Llama 3.1 to Phi 3 for an application), developers must ensure they also switch to the corresponding tokenizer and correctly apply its chat template. Failure to do so leads to poor performance or errors.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Model-specific architectures, training data diversity, special tokens, and the importance of using model cards or documentation to understand a model's specific requirements.\n",
    "- **‚ö†Ô∏è Criticality of Matching Tokenizer to Model**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - This is a foundational rule in using pre-trained transformer models. The tokenizer defines the \"language\" (vocabulary and grammar of special tokens) the model understands. A mismatch is like speaking French to someone who only understands Japanese.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - In any production system using transformer models, ensuring the correct tokenizer is paired with the model is vital for reliability and accuracy, whether for text generation, classification, or chat.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Model loading, inference pipelines, MLOps (ensuring consistent environments), and debugging NLP models.\n",
    "- **üíª Specialized Tokenizers (StarCoder 2)**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Tokenizers can be optimized for specific types of input, like programming code. Such tokenizers will have vocabularies and rules better suited to code syntax (keywords, operators, variable names, indentation) than general language tokenizers.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - For code generation, code completion, or code understanding tasks, using a specialized tokenizer like StarCoder 2's results in more meaningful and efficient token sequences, leading to better model performance on coding tasks.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Domain-specific language modeling, source code analysis, compilers (lexical analysis phase), and AI for software engineering.\n",
    "- **üîç Exploring Tokenizer Differences**\n",
    "    - **Why is this concept important to know or understand?**\n",
    "        - Hands-on experimentation helps build intuition about how tokenization works, its impact on model input, and the subtle differences between various approaches. It moves understanding from theoretical to practical.\n",
    "    - **How does it connect with real-world tasks, problems, or applications?**\n",
    "        - Such exploration can aid in prompt engineering (e.g., understanding if a rare word is a single token), debugging unexpected model behavior, and appreciating the design choices behind different tokenization strategies.\n",
    "    - **What other concepts, techniques, or areas is this related to?**\n",
    "        - Data science experimentation, debugging, empirical analysis, and building expertise in NLP model handling.\n",
    "\n",
    "### **Code Examples**\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- Llama 3.1 Instruct Example ---\n",
    "model_id_llama_instruct = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "tokenizer_llama_instruct = AutoTokenizer.from_pretrained(model_id_llama_instruct, trust_remote_code=True)\n",
    "\n",
    "messages_llama = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant. Today's date is May 8, 2025. Knowledge cutoff is December 2023.\"}, # Content based on video output\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke about a tokenizer.\"},\n",
    "]\n",
    "\n",
    "# Apply chat template (getting the string representation)\n",
    "prompt_text_llama = tokenizer_llama_instruct.apply_chat_template(\n",
    "    messages_llama,\n",
    "    tokenize=False, # Get text output\n",
    "    add_generation_prompt=True # Important to prepare for assistant's response\n",
    ")\n",
    "# print(\"--- Llama 3.1 Instruct Prompt ---\")\n",
    "# print(prompt_text_llama)\n",
    "\n",
    "# --- Phi 3 Example ---\n",
    "model_id_phi3 = \"microsoft/Phi-3-mini-4k-instruct\" # Example Phi-3 model\n",
    "tokenizer_phi3 = AutoTokenizer.from_pretrained(model_id_phi3, trust_remote_code=True)\n",
    "\n",
    "# Example text for direct encoding comparison\n",
    "text_example = \"I'm excited to show Tokenizers in action to my LLM engineers\"\n",
    "# tokens_llama_plain = tokenizer_llama_instruct.encode(text_example) # Using Llama instruct tokenizer for comparison\n",
    "# tokens_phi3 = tokenizer_phi3.encode(text_example)\n",
    "# print(f\"\\n--- Llama 3.1 Tokens for '{text_example}' ---\")\n",
    "# print(tokens_llama_plain) # From previous example output, Llama added <|begin_of_text|>\n",
    "# print(f\"--- Phi 3 Tokens for '{text_example}' ---\")\n",
    "# print(tokens_phi3) # Phi 3 did not add a start token in the video for direct encode\n",
    "\n",
    "# Apply chat template for Phi 3\n",
    "messages_phi = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful chatbot.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke about a tokenizer.\"},\n",
    "]\n",
    "prompt_text_phi3 = tokenizer_phi3.apply_chat_template(\n",
    "    messages_phi,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "# print(\"\\n--- Phi 3 Instruct Prompt ---\")\n",
    "# print(prompt_text_phi3)\n",
    "\n",
    "# --- Qwen2 Example ---\n",
    "model_id_qwen2 = \"Qwen/Qwen2-7B-Instruct\" # Example Qwen2 instruct model\n",
    "tokenizer_qwen2 = AutoTokenizer.from_pretrained(model_id_qwen2, trust_remote_code=True)\n",
    "\n",
    "messages_qwen = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke about a tokenizer.\"},\n",
    "]\n",
    "prompt_text_qwen2 = tokenizer_qwen2.apply_chat_template(\n",
    "    messages_qwen,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "# print(\"\\n--- Qwen2 Instruct Prompt ---\")\n",
    "# print(prompt_text_qwen2)\n",
    "\n",
    "# --- StarCoder 2 Example ---\n",
    "model_id_starcoder2 = \"bigcode/starcoder2-3b\" # Example StarCoder2 base model (instruct might be different)\n",
    "tokenizer_starcoder2 = AutoTokenizer.from_pretrained(model_id_starcoder2, trust_remote_code=True)\n",
    "\n",
    "code_example = \"\"\"def hello_world(person):\n",
    "    print(f\"Hello, {person}\")\"\"\"\n",
    "# tokens_starcoder = tokenizer_starcoder2.encode(code_example)\n",
    "# print(f\"\\n--- StarCoder2 Tokens for code ---\\n{tokens_starcoder}\")\n",
    "\n",
    "# To show individual tokens and their string representations for StarCoder2:\n",
    "# decoded_per_token_starcoder = []\n",
    "# for token_id in tokens_starcoder:\n",
    "#     decoded_per_token_starcoder.append(tokenizer_starcoder2.decode([token_id]))\n",
    "# print(\"\\n--- StarCoder2 Decoded Tokens ---\")\n",
    "# for i, token_str in enumerate(decoded_per_token_starcoder):\n",
    "#    print(f\"Token ID: {tokens_starcoder[i]}, Decoded: '{token_str}'\")\n",
    "\n",
    "```\n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "- How can I apply this concept in my daily data science work or learning?\n",
    "    - When building a chatbot application with an open-source instruct model, I must use `apply_chat_template` with `tokenize=False` to inspect the exact prompt string being generated (including special tokens like `<|begin_of_text|>`, `<|system|>`, `<|user|>`, `<|assistant|>`) for that specific model, ensuring my conversational history is formatted correctly before tokenization and input to the model.\n",
    "- Can I explain this concept to a beginner in one sentence?\n",
    "    - Chat models need conversations formatted in a very specific way with special tags for who's speaking (system, user, or AI), and `apply_chat_template` is a helper that automatically arranges your chat messages into this required format for the chosen AI model.\n",
    "- Which type of project or domain would this concept be most relevant to?\n",
    "    - This is most relevant for projects involving building or interacting with conversational AI systems, chatbots, or any application requiring multi-turn dialogue with instruct-fine-tuned language models, across domains like customer support, virtual assistance, education, or interactive entertainment. It's also crucial when comparing or migrating between different chat models to understand their unique formatting needs.\n",
    "\n",
    "# Day 3 - Hugging Face Tokenizers: Preparing for Advanced AI Text Generation\n",
    "\n",
    "This concluding segment recaps the learning progression, starting from high-level Hugging Face pipelines for various inference tasks, moving to a deeper understanding of tokenizers and their mechanics (including special tokens), and finally setting the stage for the next topic: working directly with models using Hugging Face's underlying code (wrappers for PyTorch or TensorFlow) to generate text and compare different open-source models. This progression signifies a journey towards more granular control and understanding of transformer models."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
