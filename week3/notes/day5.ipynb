{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d2fd15e",
   "metadata": {},
   "source": [
    "# Day 5 - Combining Frontier & Open-Source Models for Audio-to-Text Summarization\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This lesson focuses on building a practical application that automatically generates meeting minutes from audio recordings. The project combines the strengths of frontier models (via APIs for audio-to-text transcription) and open-source models (hosted locally or via Hugging Face for text summarization and action item extraction) to create a useful business tool.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üöÄ **Project Goal:** Build an AI system to generate structured meeting minutes (discussion points, takeaways, action items) from audio recordings. This is relevant for automating a common business task, saving time and improving record-keeping.\n",
    "- ü§ù **Hybrid Approach:** Utilize both frontier models (for accurate speech-to-text conversion via API) and open-source models (using Hugging Face Transformers for text processing). This demonstrates a practical pattern of leveraging different model types based on their strengths and accessibility for specific tasks.\n",
    "- üîä **Input Data:** Use publicly available audio recordings of council meetings from Hugging Face datasets as the source material. This provides realistic data for development and testing.\n",
    "- ‚öôÔ∏è **Core Skills:** Solidify understanding of using pipelines, tokenizers, and models within the Hugging Face ecosystem for inference with open-source LLMs. This is crucial for customizing and deploying AI solutions beyond API-only approaches.\n",
    "- üìà **Business Relevance:** The project simulates building a real-world feature found in many productivity applications, giving practical experience in developing end-to-end AI solutions.\n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "    - You can apply this by identifying tasks that can be broken down into sub-problems solvable by different types of models (e.g., using a powerful API for initial data processing like transcription, then a fine-tuned open-source model for specific analysis or summarization). This hybrid approach is often cost-effective and flexible.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    "    - We're building a tool that listens to meeting recordings, automatically types out what was said using one AI, and then uses another AI to summarize the key points and list who needs to do what.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "    - This concept is highly relevant for productivity tools, business process automation, corporate knowledge management, legal tech (meeting/deposition summarization), and accessibility tools (providing summaries for hearing-impaired individuals).\n",
    "\n",
    "# Day 5 - Using Hugging Face & OpenAI for AI-Powered Meeting Minutes Generation\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This section details the step-by-step implementation of the meeting minutes generator within a Google Colab environment. It showcases integrating Google Drive for file access, using the OpenAI Whisper API for audio transcription, and employing a quantized Llama 3.1 model via Hugging Face Transformers for summarizing the transcript into structured minutes with action items.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üíæ **Data Source & Access:** Utilizes the \"Meeting Bank\" dataset (specifically a segment from a Denver City Council meeting) stored on Google Drive, demonstrating how to mount and access Drive files directly within Colab. This is relevant for projects requiring access to personal or team data stored in the cloud.\n",
    "- üîä **Audio Transcription:** Leverages OpenAI's `whisper-1` model via its API to convert the meeting audio file (`.mp3`) into text. This highlights the practical use of specialized frontier models for high-accuracy tasks like speech-to-text.\n",
    "- üìù **Text Generation with Open Source LLM:** Employs the `meta-llama/Meta-Llama-3.1-8B-Instruct` model from Hugging Face to process the transcript. This involves crafting specific system and user prompts to guide the model in generating minutes in Markdown format, including summary, discussion points, takeaways, and action items.\n",
    "- ‚öôÔ∏è **Efficient Inference:** Applies 4-bit quantization (`BitsAndBytesConfig`) to the Llama 3.1 model before loading it onto the GPU (T4). This significantly reduces memory requirements, making it feasible to run large models on standard Colab GPUs, a crucial technique for resource-constrained environments.\n",
    "- üîÑ **Streaming Output:** Uses the `TextStreamer` from Hugging Face `transformers` to display the generated text token by token in real-time. This improves user experience for long generations.\n",
    "- üìÑ **Markdown Formatting:** The generated output is explicitly requested and formatted in Markdown, which is then rendered nicely within the Colab notebook using `display(Markdown(...))`. This is useful for creating well-structured, readable reports.\n",
    "- üí° **Next Steps:** Proposes an exercise to wrap the implemented logic into a Gradio user interface, allowing users to input a filename from Google Drive and generate minutes. This encourages building practical, user-facing applications.\n",
    "\n",
    "## **Code Examples**\n",
    "\n",
    "**Python**\n",
    "\n",
    "```python\n",
    "# 1. Mount Google Drive in Colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "audio_file_path = \"/content/drive/My Drive/LLMs/Denver_Extract.mp3\" # Example path\n",
    "\n",
    "# 2. Setup OpenAI client\n",
    "from openai import OpenAI\n",
    "client = OpenAI(api_key=openai_api_key) # Assuming key is retrieved\n",
    "\n",
    "# 3. Transcribe Audio using OpenAI Whisper\n",
    "audio_file= open(audio_file_path, \"rb\")\n",
    "transcription = client.audio.transcriptions.create(\n",
    "  model=\"whisper-1\",\n",
    "  file=audio_file,\n",
    "  response_format=\"text\"\n",
    ")\n",
    "print(transcription)\n",
    "\n",
    "# 4. Define Prompts for Llama 3.1\n",
    "SYSTEM_PROMPT = \"\"\"You are an assistant that produces meeting minutes from transcripts... in markdown.\"\"\"\n",
    "USER_PROMPT = f\"\"\"Below is the transcript...\\nPlease write minutes in markdown...\\n\\n{transcription}\"\"\"\n",
    "messages = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": USER_PROMPT}]\n",
    "\n",
    "# 5. Setup Quantization Config\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# 6. Load Tokenizer and Model (Quantized)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" # Or the specific model used\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token # Set pad token if needed\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16, # Or appropriate dtype\n",
    "    device_map=\"auto\",         # Automatically use GPU if available\n",
    "    quantization_config=quantization_config\n",
    ")\n",
    "\n",
    "# 7. Prepare Inputs and Streamer\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "# 8. Generate Text (Streaming)\n",
    "outputs = model.generate(\n",
    "    inputs,\n",
    "    max_new_tokens=2000,\n",
    "    streamer=streamer\n",
    ")\n",
    "\n",
    "# 9. Decode the full response (optional, if not just relying on streamer)\n",
    "response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 10. Display as Markdown\n",
    "from IPython.display import display, Markdown\n",
    "# Assuming 'response' variable holds the markdown string extracted from response_text\n",
    "# Example: Extracting the part after the prompt if needed\n",
    "# response = response_text.split(USER_PROMPT)[-1].strip() # Basic split logic, might need refinement\n",
    "# display(Markdown(response))\n",
    "# Note: The actual extraction logic might differ based on exact model output format.\n",
    "# In the video, the streamed output was directly usable markdown.\n",
    "\n",
    "```\n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "    - You can integrate external data sources like Google Drive into your Colab workflows, combine API-based models (like Whisper) for specialized tasks with open-source models (like Llama 3) for custom processing, and use quantization to run larger models on available hardware.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    "    - We're connecting Google Drive to our code notebook, using one AI service (OpenAI) to turn spoken audio from a file into text, and then feeding that text to another, locally-run AI (Llama 3) to automatically write meeting notes in a neat format.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "    - This is highly relevant for building productivity tools, automating reporting in business intelligence, enhancing accessibility by summarizing long audio/video content, and creating customized NLP applications where combining different AI models offers the best performance or cost-effectiveness.\n",
    "\n",
    "# Day 5 - Build a Synthetic Test Data Generator: Open-Source AI Model for Business\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This segment introduces the main end-of-week challenge: building a synthetic test data generator using an open-source model, emphasizing its broad applicability and value. It also recaps the key skills mastered in week 3, covering the combined use of frontier and open-source models and proficiency with Hugging Face tools, before previewing the focus of week 4 on LLM selection strategies and code generation.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üéØ **End-of-Week Challenge:** Create a tool that generates synthetic test data based on user descriptions (e.g., product descriptions, job postings) using an open-source model, preferably with a Gradio UI. This is highly relevant as synthetic data generation is crucial for augmenting datasets, testing systems, and bootstrapping projects when real data is scarce.\n",
    "- üõ†Ô∏è **Value Proposition:** The synthetic data generator is presented as a universally applicable tool for any business vertical, useful for future projects within the course and beyond. Investing time in building it provides a tangible asset.\n",
    "- ‚úÖ **Week 3 Skills Recap:** Reinforces the ability to confidently code with frontier models, build complex AI assistants (multimodal, tool-using, multi-agent), and integrate frontier and open-source models within a single solution using Hugging Face `pipeline`, `tokenizer`, and `model` APIs for inference.\n",
    "- ü§î **Week 4 Preview: Model Selection:** Addresses the critical question of how to choose the right LLM (closed vs. open source, specific models) for a given task. Techniques like using leaderboards and arenas for comparison will be explored.\n",
    "- üíª **Week 4 Preview: Code Generation:** Introduces code generation as the practical focus for the upcoming week, utilizing both frontier and open-source models to tackle programming tasks.\n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept (synthetic data generation) in my daily data science work or learning?**\n",
    "    - You can use synthetic data generation to create larger, more diverse datasets for training machine learning models (especially when real data is limited or sensitive), generate edge cases for robust testing of applications, or create realistic mock data for demos and prototyping.\n",
    "- **Can I explain this concept (synthetic data generation challenge) to a beginner in one sentence?**\n",
    "    - The challenge is to build a tool where you tell an AI what kind of data you need (like fake customer reviews or product names), and it automatically creates lots of realistic examples for you to use.\n",
    "- **Which type of project or domain would this concept (synthetic data generation) be most relevant to?**\n",
    "    - It's relevant across many domains, including software testing (generating test inputs/outputs), machine learning (data augmentation, tackling class imbalance), privacy-preserving analytics (generating data without real user info), simulation, and any scenario needing varied data for development or evaluation where real data is insufficient or restricted."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
