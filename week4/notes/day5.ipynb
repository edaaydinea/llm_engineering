{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b472f5b9",
   "metadata": {},
   "source": [
    "# Day 5 - Evaluating LLM Performance: Model - Centric vs Business-Centric Metrics\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This lesson emphasizes the critical importance of evaluating AI solutions, particularly in the context of Large Language Models (LLMs), by introducing two distinct categories of performance metrics: model-centric (technical) metrics and business-centric (outcome) metrics. Understanding and utilizing both types is essential for optimizing models and demonstrating real-world impact, with a detailed explanation of cross-entropy loss and perplexity as key technical measures.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üéØ **Importance of Evaluation**: Deciding if an AI solution is performing well is a crucial upfront consideration, as it dictates how success is measured and achieved.\n",
    "    - **Relevance**: Proper evaluation guides model development, helps in proving the value of an AI solution, and aligns technical work with business objectives.\n",
    "- ‚öñÔ∏è **Two Types of Metrics**: Performance evaluation relies on two kinds of metrics: model-centric (technical) and business-centric (outcome).\n",
    "    - **Relevance**: Distinguishing between these helps data scientists optimize models effectively while also communicating their value in terms business stakeholders understand.\n",
    "- ‚öôÔ∏è **Model-Centric (Technical) Metrics**: These metrics, like loss and perplexity, directly measure a model's performance on its task and are used by data scientists for optimization.\n",
    "    - **Relevance**: They provide immediate feedback on how well the model is learning and performing at a technical level, crucial for training and fine-tuning.\n",
    "- üìâ **Cross-Entropy Loss**: A common loss function for LLMs that measures how poorly the model predicted the next token in a sequence. It's calculated as the negative logarithm of the probability the model assigned to the actual correct next token.\n",
    "    - **Relevance**: A fundamental metric for training LLMs. Lower cross-entropy loss indicates the model is getting better at predicting the next token. A perfect prediction (probability 1) results in zero loss.\n",
    "    - Loss=‚àílog(P(actual¬†next¬†token))\n",
    "- ü§Ø **Perplexity**: Related to cross-entropy loss (often ecross-entropy¬†loss), perplexity indicates the model's uncertainty in predicting the next token. Lower perplexity is better.\n",
    "    - **Relevance**: Provides an intuitive measure of how \"surprised\" or \"confused\" a model is by the next token. A perplexity of 1 means the model is perfectly certain and correct.\n",
    "    - Perplexity=ecross-entropy¬†loss\n",
    "- üìà **Business-Centric (Outcome) Metrics**: These are Key Performance Indicators (KPIs) tied to the actual business goals the AI solution is meant to achieve, such as ROI, time savings, or benchmark comparisons.\n",
    "    - **Relevance**: These metrics demonstrate the tangible impact and value of the AI solution to stakeholders. For example, in code translation, it could be the speed improvement of C++ code over Python.\n",
    "- ü§ù **Using Both Metrics in Concert**: It's vital to use both model-centric metrics for optimization and business-centric metrics to prove real-world value.\n",
    "    - **Relevance**: Technical metrics ensure the model is performing well, while business metrics validate that this performance translates into desired outcomes.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "- **Why is distinguishing between model-centric and business-centric metrics important?**\n",
    "    - Model-centric metrics tell you if your model is learning and performing its specific task well (e.g., predicting the next word). Business-centric metrics tell you if the model, as part of a larger solution, is actually solving the intended real-world problem and delivering value (e.g., increasing sales, reducing time). You need both to build effective and impactful AI.\n",
    "- **How do cross-entropy loss and perplexity help in model training?**\n",
    "    - During training, an LLM tries to predict the next token in a sequence. Cross-entropy loss quantifies how \"wrong\" its prediction was by looking at the probability it assigned to the *correct* token. The goal of training is to minimize this loss, meaning the model gets better at assigning higher probabilities to correct tokens. Perplexity offers a more intuitive scale for this uncertainty.\n",
    "- **Why can't we rely solely on business-centric metrics for model development?**\n",
    "    - Business outcomes are often influenced by many factors beyond the model itself (e.g., data quality, user interface, market conditions). While they are the ultimate measure of success, they are often slow to measure and not directly optimizable by tweaking model parameters. Model-centric metrics provide faster, more direct feedback for the development loop.\n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "    - When starting any AI project, define both technical metrics (like accuracy, F1-score, or perplexity for LLMs) to guide model development and business metrics (like user engagement, cost reduction, or task completion rate) to measure the project's overall success. Regularly track both throughout the project lifecycle.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    "    - To know if an AI is good, we use technical scores to check how well it does its specific AI task, and business scores to see if it actually helps achieve real-world goals, and we need both.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "    - This dual-metric approach is relevant to virtually all applied AI projects, from developing LLM-based applications (like chatbots or code generators where perplexity and task-specific business outcomes are key) to traditional machine learning projects (like fraud detection where precision/recall and financial loss reduction are measured).\n",
    "\n",
    "# Day 5 - Mastering LLM Code Generation: Advanced Challenges for Python Developers\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This session recaps the performance of various Large Language Models (LLMs) like Claude 3.5 Sonnet, GPT-4, and open-source models (like \"Code Qwen\") in a code translation task, highlighting Claude's success and the impressive capabilities of smaller open-source models despite their size. The instructor then introduces a series of challenging assignments designed to solidify learning, including enhancing the existing code translation solution, developing tools for adding code comments and generating unit tests, and creating a code generator for simulated stock trading.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üèÜ **Code Translation Recap**: Claude 3.5 Sonnet was the top performer in translating Python code to C++, followed by GPT-4. Open-source models like \"Code Qwen\" (likely a typo for Qwen or refers to CodeGemma based on previous context) showed commendable effort but faced challenges with correctness, underscoring the performance gap with larger frontier models.\n",
    "    - **Relevance**: Demonstrates the current state-of-the-art in LLM code generation and the trade-offs between proprietary and open-source models in terms of capability, cost, and accessibility.\n",
    "- üí∏ **Cost and Accessibility**: A key point was the cost-effectiveness and freeness (open-source nature) of models like \"Code Qwen\" (7B parameters) compared to the API costs or significantly larger size (trillion+ parameters) of frontier models.\n",
    "    - **Relevance**: Important consideration for practical applications, where budget and access to resources can dictate model choice.\n",
    "- üöÄ **New Challenges Issued**: Students are tasked with several significant projects:\n",
    "    - Expanding the existing code translator to include more models (Gemini, Code Llama, StarCoder) and attempting to improve CodeGemma's performance.\n",
    "    - Developing a tool to automatically add comments/docstrings to code.\n",
    "    - Creating a tool to generate unit tests for Python modules.\n",
    "    - Building a code generator for simulated stock trading decisions (with a strong disclaimer against real-world use).\n",
    "    - **Relevance**: These assignments encourage practical application of LLMs for diverse software development tasks, pushing students to explore creative solutions and advanced prompting techniques.\n",
    "- üõ†Ô∏è **Focus on Practical Skills**: The assignments aim to build proficiency in using LLMs for code generation, debugging, and creating useful development tools.\n",
    "    - **Relevance**: Directly applicable skills for software engineering and AI development roles.\n",
    "- üéì **Course Milestone**: The lesson marks the 50% completion point of the \"LLM Engineer\" journey.\n",
    "    - **Relevance**: Motivates students by acknowledging their progress and setting the stage for upcoming advanced topics.\n",
    "- üîÆ **Future Topic Tease**: The next week will focus on Retrieval Augmented Generation (RAG).\n",
    "    - **Relevance**: Prepares students for another critical and popular technique in the LLM space.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "- **Evaluating LLM-Generated Code:**\n",
    "    - **Why is this important?** Beyond just generating code, it's crucial to evaluate its correctness, efficiency (e.g., speed of C++ vs. Python), and adherence to requirements. The business-centric metric here was clear: the performance improvement of the translated code.\n",
    "    - **How does it connect to real-world tasks?** In any software development context, code must work correctly and efficiently. LLMs as coding assistants must be held to these standards.\n",
    "    - **What other concepts is this related to?** This ties into software testing, benchmarking, and the previously discussed model-centric vs. business-centric evaluation metrics.\n",
    "- **Prompt Engineering for Specific Outcomes:**\n",
    "    - **Why is this important?** The attempt to force \"Code Qwen\" with system prompts, and the challenge to make CodeGemma work better, highlight that getting desired outputs from LLMs, especially smaller ones, often requires careful and sometimes \"aggressive\" prompting.\n",
    "    - **How does it connect to real-world tasks?** Effective prompt engineering is key to maximizing the utility of LLMs across various applications, not just code generation.\n",
    "    - **What other concepts is this related to?** Instruction tuning, model behavior control, and mitigating unwanted model behaviors (like rewriting a random number generator unnecessarily).\n",
    "- **Open Source vs. Frontier Models for Code Generation:**\n",
    "    - **Why is this important?** The lesson reiterates the trade-offs. Frontier models (Claude, GPT) generally offer higher performance and reliability for complex tasks but come with API costs and are closed-source. Open-source models are rapidly improving, offer cost advantages (free to run locally, aside from hardware/electricity), and provide transparency, but may require more effort to achieve comparable results.\n",
    "    - **How does it connect to real-world tasks?** The choice of model depends on project requirements, budget, privacy concerns, and the need for customization.\n",
    "    - **What other concepts is this related to?** Model quantization, local deployment, fine-tuning, and the general AI ethics discussion around access and control.\n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "    - When using LLMs for code generation or any other task, always start with a clear business-centric metric for success (e.g., \"does the generated code run X% faster?\" or \"does the generated text achieve Y goal?\"). Then, experiment with different models (open-source and frontier, if available) and prompting strategies to achieve that goal, keeping an eye on practical constraints like cost and effort. The assignments given are direct applications of this.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    "    - We're learning to make AI write and improve computer code by giving it tough coding homework, like teaching it to add comments, create tests, or even (safely!) suggest stock trades, while also figuring out which AI (big expensive ones or free smaller ones) is best for the job.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "    - The concepts of evaluating LLM-generated code and the proposed challenges are most relevant to software development, AI-assisted programming, developer tooling, and potentially specialized domains like quantitative finance (for the trading bot, in a simulated context). More broadly, it's relevant to any field looking to automate or augment complex task generation using LLMs."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
