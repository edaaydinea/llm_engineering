{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c347844",
   "metadata": {},
   "source": [
    "# Day 2 - Comparing LLMs: Top 6 Leaderboards for Evaluating Language Models\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This lesson outlines a plan to significantly broaden the understanding of Large Language Model (LLM) evaluation beyond the primary Hugging Face Open LLM Leaderboard. The session will cover a \"tour of six essential leaderboards,\" including specialized ones on Hugging Face (for code generation, performance metrics, medical applications, and specific languages), Vellum's Leaderboard (which compares both open and closed-source models and includes practical data like costs and context window lengths), the SEAL Leaderboard (for expert skills), and the human-judged Chatbot Arena by LMSYS. Finally, the lesson will briefly touch upon diverse commercial use cases of LLMs across various industries to provide practical context for model selection.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üåê **Expanding Evaluation Horizons**: The lesson emphasizes the importance of looking beyond a single source of truth by exploring a variety of leaderboards and evaluation platforms to get a more complete picture of LLM capabilities.\n",
    "- üíª **BigCode Leaderboard (Hugging Face)**: This is a specialized leaderboard found on Hugging Face that focuses specifically on ranking LLMs based on their performance in code generation tasks, using benchmarks like HumanEval and MultiPL-E.\n",
    "    \n",
    "    **1**\n",
    "    \n",
    "- ‚ö° **LLM Perf Leaderboard (Hugging Face)**: Another Hugging Face resource (e.g., `ArtificialAnalysis/LLM-Performance-Leaderboard`), this leaderboard assesses models not only on their output quality but also on crucial performance metrics such as inference speed (tokens/second), latency, and computational cost, which are vital for real-world deployment.\n",
    "    \n",
    "    **2**\n",
    "    \n",
    "- üåç **Domain & Language-Specific Leaderboards (Hugging Face)**: The Hugging Face platform hosts numerous other leaderboards tailored to specific needs, such as the \"Open Medical-LLM Leaderboard\" for healthcare applications or leaderboards for particular languages (e.g., the \"Open Portuguese LLM Leaderboard\").\n",
    "    \n",
    "    **3**\n",
    "    \n",
    "- ‚öñÔ∏è **Vellum's Leaderboard (Vellum.ai)**: This external leaderboard is highlighted as a valuable resource because it compares both open-source and closed-source models side-by-side. It's also noted for collating practical information like API costs and context window lengths, which is crucial for making pragmatic choices.\n",
    "    \n",
    "    **4**\n",
    "    \n",
    "- üõ°Ô∏è **SEAL Leaderboard (Scale AI)**: This platform focuses on assessing LLMs using expert-driven private evaluations on various challenging tasks and expert skills, aiming to provide robust and reliable benchmarks for frontier models.\n",
    "    \n",
    "    **5**\n",
    "    \n",
    "- ü•ä **Chatbot Arena (LMSYS)**: A unique evaluation approach where LLMs are compared in direct, anonymous head-to-head \"battles.\" Human users chat with two models simultaneously and vote for the one that provides a better response. This system generates an Elo rating, offering a measure of conversational quality based on human preference.\n",
    "    \n",
    "    **6**\n",
    "    \n",
    "- üìà **Commercial LLM Applications**: The lesson will also provide a quick overview of how LLMs are being deployed to solve tangible commercial problems across diverse sectors including law, talent management, software development (code), healthcare, and education, illustrating their practical impact.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "**The Necessity of Diverse Evaluation Platforms and Real-World Context in LLM Selection**\n",
    "\n",
    "- **Why is this concept important to know or understand?**\n",
    "No single benchmark or leaderboard can capture the entirety of an LLM's capabilities or its suitability for every conceivable task. Different platforms emphasize different aspects: some focus on open-source models, others compare across open and proprietary offerings; some use automated, task-specific benchmarks, while others (like Chatbot Arena) incorporate human judgment for more qualitative aspects like conversational flow. Specialized leaderboards (e.g., for coding, medical, specific languages) offer deeper insights for particular use cases. Understanding this diverse ecosystem of evaluation tools, along with seeing how LLMs perform in real commercial applications, allows for a much more nuanced, well-rounded, and ultimately more effective model selection process. It moves beyond just scores to consider practical utility, cost, and user experience.\n",
    "- **How does it connect with real-world tasks, problems, or applications?**\n",
    "    - **Task-Specific Suitability**: If building a coding assistant, the BigCode Leaderboard offers more targeted insights than a general language understanding benchmark. For a customer service chatbot, the human preference Elo ratings from Chatbot Arena can be more indicative of real-world success.\n",
    "    - **Practical Deployment Factors**: Leaderboards like LLM Perf or Vellum's that include inference speed, cost, and context window limitations are crucial for making choices that are viable in production environments.\n",
    "    - **Holistic Assessment**: Combining information from multiple sources‚Äîfor instance, a model's strong benchmark scores on the Open LLM Leaderboard, its favorable Elo rating on Chatbot Arena, and acceptable performance/cost metrics‚Äîprovides a more robust basis for selection.\n",
    "        \n",
    "        **7**\n",
    "        \n",
    "    - **Validation of Value**: Seeing LLMs successfully applied in various commercial sectors (law, healthcare, etc.) provides tangible evidence of their potential and can help justify their adoption for similar problems.\n",
    "    This multifaceted evaluation strategy ensures that the chosen LLM is not only technically proficient on paper but also aligns with the specific needs, constraints, and objectives of the real-world project.\n",
    "- **What other concepts, techniques, or areas is this related to?**\n",
    "This approach embodies the principle of **data triangulation**‚Äîusing multiple sources and methods to gain a more reliable understanding. It is a core part of **comprehensive AI model validation and verification**. The use of human judgment in platforms like Chatbot Arena brings in elements of **qualitative research** and **user experience (UX) testing**. The review of commercial use cases connects to **market analysis**, **technology adoption trends**, and understanding the **return on investment (ROI)** for AI solutions.\n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "When tasked with selecting an LLM, actively seek out and compare information from multiple relevant leaderboards. For instance, check the Open LLM Leaderboard for general open-source performance, the Chatbot Arena if it's a conversational application, and something like the LLM Perf or Vellum's leaderboard for cost and speed considerations. Supplement this by researching case studies or examples of LLMs used in similar domains or for comparable tasks to gauge practical success.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    "To pick the very best AI \"brain\" for a specific job, we need to look at lots of different \"scorecards\" and \"expert opinions\"‚Äîsome \"scorecards\" test general knowledge, others test specific skills like coding or chatting, some tell us how fast or expensive the AI is, and we also look at how similar AIs are already successfully working in real businesses.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "This comprehensive strategy of consulting multiple leaderboards (e.g., BigCode for software development, a medical leaderboard for healthcare AI, Vellum's for comparing proprietary vs. open-source options) and analyzing real-world commercial applications is vital for virtually any project involving LLM selection, especially when the decision has significant implications for performance, cost, user experience, or business outcomes. It is particularly critical for roles such as AI product managers, solutions architects, and lead ML engineers who must make strategic and well-justified technology choices.\n",
    "\n",
    "# Day 2 - Specialized LLM Leaderboards: Finding the Best Model for Your Use Case\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This lesson provides a detailed walkthrough of several key Hugging Face leaderboards designed for evaluating open-source Large Language Models (LLMs), moving beyond the main Open LLM Leaderboard. The session focuses on the BigCode Models Leaderboard for assessing code generation abilities, the LLM Perf Leaderboard with its practical \"Find your best model\" chart for balancing speed, accuracy, and memory usage, and also highlights the availability of numerous domain-specific (like the Open Medical LLM Leaderboard) and language-specific (e.g., Portuguese) leaderboards found within Hugging Face Spaces. These tools are presented as essential for making informed decisions when selecting open-source LLMs for particular tasks and deployment scenarios.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üíª **BigCode Models Leaderboard**: Hosted on Hugging Face Spaces, this leaderboard is tailored for evaluating and ranking open-source LLMs based on their proficiency in code generation. It employs benchmarks such as HumanEval (for Python) and similar tests for other programming languages like Java, JavaScript, and C++. The leaderboard also features an overall \"win rate\" metric. Notable models mentioned by the speaker (at the time of recording) include specialized versions like CodeQwen and Code Llama, as well as DeepSeek Coder, StarCoder2, and CodeGemma. It's observed that fine-tuned models often significantly surpass base models in these coding tasks.\n",
    "- ‚ö° **LLM Perf Leaderboard**: This Hugging Face leaderboard is crucial for understanding the operational performance of open-source LLMs. Its \"Find your best model\" tab offers a powerful visualization:\n",
    "    - **X-axis (Speed)**: Represents the time taken to generate a specific number of tokens (e.g., 64 tokens), with models further to the left being faster.\n",
    "    - **Y-axis (Accuracy)**: Often uses an aggregate score from a general benchmark like the Open LLM Leaderboard, where higher is better.\n",
    "    - **Blob Size (Memory Footprint)**: The size of the plotted circle for each model indicates its memory requirement; smaller blobs are preferable, signifying lower hardware needs.\n",
    "    - **Color Coding**: Different colors distinguish model families (e.g., yellow for Phi models).\n",
    "    This chart enables users to make informed trade-offs between speed, accuracy, and memory demands, and can be filtered by specific hardware configurations (like T4 GPUs).\n",
    "- ü©∫ **Open Medical LLM Leaderboard**: An example of a highly specialized, domain-specific leaderboard available on Hugging Face Spaces. This resource evaluates LLMs using benchmarks relevant to the medical field, such as Clinical Knowledge, College Biology, Medical Genetics, and PubMedQA. It's an indispensable tool for projects requiring LLMs in healthcare applications.\n",
    "- üåê **Language-Specific & Other Niche Leaderboards**: Beyond broad or domain-specific evaluations, Hugging Face Spaces hosts a wide array of other leaderboards. These include leaderboards focused on LLM performance in particular languages (e.g., a Portuguese LLM leaderboard was mentioned) and other specialized areas, easily discoverable through a search for \"leaderboard\" within the Spaces platform.\n",
    "- ‚ÑπÔ∏è **Utilizing \"About\" Pages**: The speaker emphasized that the \"About\" page for each leaderboard typically contains vital information regarding the datasets used for evaluation, the methodologies for calculating scores, and guidance on how to interpret the presented metrics.\n",
    "- üéØ **Exclusively for Open-Source Models**: It's reiterated that the leaderboards discussed in this segment are tools for comparing and selecting *open-source* LLMs, with leaderboards covering closed-source models to be discussed later.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "**The Importance of Specialized and Performance-Focused Leaderboards for Open-Source LLM Selection**\n",
    "\n",
    "- **Why is this concept important to know or understand?**\n",
    "While general LLM leaderboards offer a broad overview of capabilities, specialized leaderboards (like BigCode for programming or the Open Medical LLM Leaderboard for healthcare) and performance-centric leaderboards (such as the LLM Perf Leaderboard) provide much more targeted and actionable insights. For open-source LLMs, where users are often responsible for deployment and managing operational costs, factors like inference speed, memory consumption, and energy efficiency can be just as critical as raw benchmark accuracy. These specialized tools allow developers and engineers to:\n",
    "    - Identify models that excel in the specific skills required for their application (e.g., coding, medical knowledge).\n",
    "    - Assess the practical deployability of models based on performance trade-offs (speed vs. accuracy vs. resource needs).\n",
    "    - Make informed decisions that align not only with functional requirements but also with hardware limitations and budget constraints.\n",
    "- **How does it connect with real-world tasks, problems, or applications?**\n",
    "    - **Tailored Model Choice**: A developer building a code completion tool would prioritize models topping the BigCode Leaderboard. A healthcare startup would consult the Open Medical LLM Leaderboard.\n",
    "    - **Resource Optimization & Cost Management**: The LLM Perf Leaderboard is essential for any project where inference costs or hardware constraints are a concern. It helps select the most efficient open-source model that meets performance requirements for a given hardware setup (e.g., specific GPUs like NVIDIA T4).\n",
    "    - **Niche Application Development**: Language-specific leaderboards are invaluable for creating applications targeted at non-English speaking audiences, ensuring the chosen model is proficient in the required language.\n",
    "    - **Informed Prototyping**: These leaderboards help narrow down the vast number of open-source models to a few suitable candidates for prototyping, saving significant time and resources.\n",
    "    By using these targeted evaluation resources, teams can make more precise and efficient choices, increasing the likelihood of project success.\n",
    "- **What other concepts, techniques, or areas is this related to?**\n",
    "The use of these diverse leaderboards aligns with the principles of **domain-specific adaptation** and **task-oriented evaluation** in machine learning. The LLM Perf Leaderboard directly addresses critical aspects of **MLOps**, focusing on **inference optimization**, **computational efficiency**, and **hardware compatibility**. The proliferation of specialized leaderboards indicates a trend towards more **application-specific AI solutions** and provides the tools necessary for effective **model lifecycle management** within the open-source AI landscape. They enable a more nuanced approach than relying on a single, aggregated score from a general leaderboard.\n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "When selecting an open-source LLM, go beyond general leaderboards. If your project involves coding, consult the BigCode Models Leaderboard. For deployment, critically analyze the LLM Perf Leaderboard's \"Find your best model\" chart, considering your specific hardware and desired speed/accuracy/memory trade-offs. If working in a specialized field like medicine or a specific language, actively search Hugging Face Spaces for relevant domain-specific or language-specific leaderboards to make the most informed choice.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    "Besides the main \"exam scores\" for open-source AIs, there are special \"report cards\" just for how well they code (BigCode Leaderboard), how fast and efficient they are on different computers (LLM Perf Leaderboard), or how good they are at specific subjects like medicine or different languages, helping us pick the absolute best open-source AI for any particular job.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "The BigCode Models Leaderboard is indispensable for software development projects leveraging LLMs for code generation, completion, or analysis. The LLM Perf Leaderboard is critical for any project deploying open-source LLMs where operational aspects like inference speed, computational cost, or specific hardware limitations (e.g., deploying on edge devices or specific cloud GPU instances) are major considerations. Domain-specific leaderboards, such as the Open Medical LLM Leaderboard or language-specific ones, are essential for specialized fields requiring validated LLM performance on tasks relevant to that particular expert domain or linguistic context.\n",
    "\n",
    "# Day 2 - LLAMA vs GPT-4: Benchmarking Large Language Models for Code Generation\n",
    "\n",
    "## Summary\n",
    "\n",
    "This lesson explores essential leaderboards outside the Hugging Face ecosystem that facilitate the comparison of both open-source and closed-source Large Language Models (LLMs). The focus is on [Vellum.ai](http://vellum.ai/)'s Leaderboard, recognized for its comprehensive comparisons including practical metrics like API costs, speed, latency, and context window sizes, alongside benchmark scores. Additionally, Scale AI's SEAL Leaderboard is introduced, which offers a suite of specialized leaderboards evaluating models on specific expert skills such as adversarial robustness, coding, and nuanced instruction following, thus providing a broader toolkit for informed LLM selection.\n",
    "\n",
    "## Highlights\n",
    "\n",
    "- üåê [**Vellum.ai](http://vellum.ai/) Leaderboard - Bridging Open and Closed Source**:\n",
    "    - This is a key external resource ([vellum.ai/llm-leaderboard](http://vellum.ai/llm-leaderboard)) for comparing leading open-source models (like Llama 3.1 405B) directly against top-tier closed-source models (e.g., GPT-4o, Claude 3.5 Sonnet).\n",
    "    - It presents initial comparisons on established benchmarks such as MMLU (the basic version), HumanEval (Python), and MATH, showing the competitive stance of large open-source models.\n",
    "    - Offers critical **performance and cost metrics**:\n",
    "        - **Speed (Tokens/Second)**: Highlights faster models, often smaller ones like Llama 3.1 8B or Gemini 1.5 Flash.\n",
    "        - **Latency (Seconds to First Token)**: Indicates responsiveness, where smaller models also tend to perform better.\n",
    "        - **Cost (USD per 1 Million Tokens)**: Breaks down pricing for input and output tokens, showcasing cost-effective options like Llama 3.1 8B and GPT-4o mini.\n",
    "    - Features an **interactive model comparison tool** allowing side-by-side evaluation of two models across various capabilities (reasoning, coding, math, etc.).\n",
    "    - Provides a detailed table with model rankings on benchmarks including MMLU, HumanEval, BBH (Big-Bench Hard ‚Äì where Claude 3.5 Sonnet was noted for a high score of 93.1%), GSM8K, and MATH Level 5.\n",
    "    - Crucially, [Vellum.ai](http://vellum.ai/) centralizes information on **context window sizes** (e.g., Gemini 1.5 Flash - 1M tokens; Claude family - 200k; GPT family - 128k; Llama 3 models - 8k) and **API input/output token costs** for numerous models, making it an invaluable reference.\n",
    "- üõ°Ô∏è **Scale AI's SEAL Leaderboard - Evaluating Specialized Expert Skills**:\n",
    "    - Offered by [Scale.com](http://scale.com/) (a company known for data solutions), this platform provides a collection of leaderboards focusing on specific LLM skills and attributes, also comparing both open and closed-source models.\n",
    "    - **Examples of SEAL Leaderboards include**:\n",
    "        - **Adversarial Robustness**: Assesses model resilience against prompts designed to elicit harmful, biased, or off-topic responses ‚Äì critical for ensuring safety in public-facing applications.\n",
    "        - **Coding**: Provides detailed benchmarks for coding abilities (Claude 3.5 Sonnet was leading, with Mistral Large 2 as a notable open-source model at the time of recording).\n",
    "        - **Instruction Following**: Measures how accurately models can adhere to complex and nuanced instructions (Llama 3.1 405B demonstrated strong performance here, reportedly outperforming GPT-4o and second only to Claude 3.5 Sonnet).\n",
    "        - **Math Problems**: Ranks models on their mathematical problem-solving capabilities (Claude 3.5 Sonnet was leading, with Llama 3.1 405B in third place).\n",
    "        - **Spanish Language Performance**: A language-specific leaderboard (GPT-4o was in the lead, with Mistral as a strong open-source option).\n",
    "    - Scale AI is noted to be continuously expanding this suite with more business-specific leaderboards.\n",
    "- üîë **Essential Additions to Bookmarks**: Both [Vellum.ai](http://vellum.ai/)'s and Scale AI's SEAL Leaderboard are strongly recommended as essential resources for anyone involved in selecting or evaluating LLMs, offering complementary perspectives to open-source focused platforms.\n",
    "\n",
    "## Conceptual Understanding\n",
    "\n",
    "**The Value of External, Mixed-Source Leaderboards with Practical & Specialized Metrics**\n",
    "\n",
    "- **Why is this concept important to know or understand?**\n",
    "While leaderboards focused on open-source models (like those on Hugging Face) are invaluable, platforms like [Vellum.ai](http://vellum.ai/)'s and Scale AI's SEAL Leaderboard broaden the evaluative landscape significantly. Their importance stems from:\n",
    "    1. **Holistic Market View**: They allow direct comparison between open-source and leading closed-source/proprietary models, which is often necessary for real-world decision-making where projects might consider either or both.\n",
    "    2. **Emphasis on Practical Deployment Factors**: [Vellum.ai](http://vellum.ai/)'s inclusion of crucial operational data like API costs, context window limitations, processing speed, and response latency moves evaluation beyond raw benchmark scores to factors that directly impact production viability and user experience.\n",
    "    3. **Assessment of Specialized and Safety-Critical Skills**: Scale AI's SEAL Leaderboard provides targeted evaluations for specific, often nuanced, capabilities like adversarial robustness, complex instruction following, or expert-level coding. These are critical for applications where reliability, safety, or highly specialized performance is paramount.\n",
    "    Together, these external resources enable a more comprehensive, pragmatic, and risk-informed approach to LLM selection.\n",
    "- **How does it connect with real-world tasks, problems, or applications?**\n",
    "    - **Strategic Decision-Making**: When deciding whether to build with an open-source model or pay for a proprietary API, these leaderboards provide direct comparative data on both capability and cost. For instance, one can weigh the performance of Llama 3.1 405B against GPT-4o or Claude 3.5 Sonnet for a specific task and factor in the associated costs or development effort.\n",
    "    - **Budgeting and Infrastructure Planning**: Information on cost per token, context window limits (as provided by [Vellum.ai](http://vellum.ai/)) is fundamental for project budgeting, estimating operational expenses, and planning the necessary infrastructure.\n",
    "    - **Ensuring Application Safety and Reliability**: Leaderboards focusing on adversarial robustness (like Scale AI's) are essential for developing public-facing applications (e.g., customer support chatbots) that need to be resilient against misuse.\n",
    "    - **Matching Models to Niche Requirements**: If a project demands exceptional instruction-following capabilities or strong performance in a specific language like Spanish, the specialized leaderboards from Scale AI can pinpoint the most suitable models more effectively than general-purpose benchmarks.\n",
    "- **What other concepts, techniques, or areas is this related to?**\n",
    "The use of these leaderboards aligns with principles of **competitive analysis** and **technology assessment** in strategic decision-making. The focus on operational metrics such as cost, speed, and latency is a core component of **MLOps (Machine Learning Operations)** and **system performance engineering**. Evaluating models on specialized attributes like adversarial robustness or precise instruction following is directly related to the growing fields of **AI safety**, **AI alignment**, and developing **trustworthy AI systems**. The provision of centralized data on API pricing and model specifications also aids in **financial modeling** and **Return on Investment (ROI) analysis** for AI projects.\n",
    "\n",
    "## Reflective Questions\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "When evaluating LLMs for any significant project, make it a practice to consult resources like [Vellum.ai](http://vellum.ai/)'s Leaderboard to directly compare open-source options against leading proprietary models, paying keen attention to API costs, context window sizes, speed, and latency. If your project has highly specific requirements (e.g., safety, complex instruction following, multilingual needs), explore specialized leaderboards such as those offered by Scale AI to find models that excel in those particular niches.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    " To pick the absolute best AI \"brain\" for a task, we don't just look at open-source model scores; we use special websites like [Vellum.ai](http://vellum.ai/) and Scale AI's SEAL Leaderboard that also compare them to big commercial AIs (like GPT-4o or Claude), and check super important details like how much they cost, how much information they can \"remember\" at once, how fast they are, and how good they are at very specific jobs like coding safely or understanding complex instructions.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "These comprehensive leaderboards ([Vellum.ai](http://vellum.ai/), Scale AI's SEAL) are critical for any project that requires a thorough evaluation balancing cutting-edge capabilities with practical deployment considerations. This includes enterprise-level AI integrations, development of commercial AI-powered products, academic research comparing different model paradigms, and any situation where choosing between open-source and proprietary models involves weighing performance, cost, context handling, speed, and specialized skills like adversarial robustness or advanced instruction following.\n",
    "\n",
    "# Day 2 - Human-Rated Language Models: Understanding the LM Sys Chatbot Arena\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This lesson introduces the LMSYS Chatbot Arena, a distinctive and popular platform for evaluating the conversational prowess of Large Language Models (LLMs) through direct human judgment. Unlike traditional leaderboards that rely on automated benchmarks, the Chatbot Arena employs a crowdsourced approach where users engage in blind A/B tests, interacting with two anonymous models and then voting for the one they prefer. These votes are aggregated to produce an Elo rating, which ranks the models based on their perceived conversational quality and user experience.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- ü§∫ **LMSYS Chatbot Arena Overview**: The Chatbot Arena (often found at `chat.lmsys.org` or linked via LMSYS) is described as an engaging, crowdsourced open platform dedicated to assessing the chat and instruction-following capabilities of various LLMs. It's positioned as a more \"fun\" and interactive way to compare models.\n",
    "- üßë‚Äç‚öñÔ∏è **Human-Powered Evaluation Method**: The core of the Arena is its reliance on human preferences. Users participate by chatting with two anonymous LLM \"challengers\" simultaneously on the same prompt. After the interaction, the user votes for which model (Model A or Model B) provided a better response, or if it was a tie, or if both were poor.\n",
    "- ‚öñÔ∏è **Elo Rating System for Ranking**: Based on the outcomes of these numerous head-to-head \"battles\" (with over a million human votes collected, as mentioned), models are assigned an Elo rating. This system, familiar from chess and other competitive games, provides a dynamic and relative ranking of their conversational abilities.\n",
    "- üóìÔ∏è **Useful Source for Knowledge Cutoff Dates**: A secondary benefit noted is that the Chatbot Arena's leaderboard often lists the knowledge cutoff dates for the models it features, serving as a convenient reference for this information.\n",
    "- üèÜ **Example Model Rankings (Context: August 2024 Recording)**: The speaker provided a snapshot of the leaderboard as of their recording time (implicitly August 2024):\n",
    "    - A recently released version of ChatGPT-4o (August 2024) held the top position with an Elo of 1316.\n",
    "    - Gemini 1.5 Pro and Grok 2 were also shown to be performing strongly.\n",
    "    - Claude 3.5 Sonnet, which had previously been a front-runner, was ranked lower (around 1270 Elo).\n",
    "    - The large open-source model, Llama 3.1 405B, was featured with an Elo of 1266.\n",
    "    - Cohere's Command R+ was also listed.\n",
    "- üó≥Ô∏è **Live Voting Example & Impact**: The lesson included a demonstration where the speaker asked two anonymous models for a joke suitable for data scientists. Model A was later revealed as Claude 3 Haiku, and Model B as Grok 2. The speaker preferred Model B's joke and explanation, casting a vote that would infinitesimally influence their relative Elo scores.\n",
    "- ü§ù **Call for Community Participation**: Viewers are encouraged to actively participate in voting on the Chatbot Arena. This not only contributes valuable data to the community-driven evaluation effort but also offers users a direct, hands-on experience with the conversational styles and capabilities of different leading-edge LLMs.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "**The Value of Human Preference in Evaluating Conversational LLM Performance (LMSYS Chatbot Arena)**\n",
    "\n",
    "- **Why is this concept important to know or understand?**\n",
    "While automated benchmarks are excellent for measuring specific, often quantitative, aspects of LLM performance (like factual recall, reasoning on structured problems, or coding ability), they frequently fall short in capturing the more subjective, nuanced qualities that define a good conversational experience. Factors such as the naturalness of language, engaging tone, creativity, coherence over extended dialogue, perceived helpfulness, and overall human-AI interaction quality are difficult to quantify automatically. The Chatbot Arena addresses this gap by directly incorporating human judgment, providing a measure of which models users genuinely *prefer* interacting with. This offers a complementary and often more holistic perspective on \"chat\" ability.\n",
    "- **How does it connect with real-world tasks, problems, or applications?**\n",
    "For any application where an LLM's primary role is to engage in conversation with a human, the user's subjective experience is a critical determinant of success. This includes:\n",
    "    - **Customer Service Chatbots**: Where empathy, clarity, and effective problem resolution are key.\n",
    "    - **Virtual Assistants**: Which need to be helpful, understand context, and maintain engaging interactions.\n",
    "    - **AI Companions or Tutors**: Where personality, patience, and adaptability are important.\n",
    "    - **Creative Writing Aids**: Where inspiring and coherent collaboration is desired.\n",
    "    The Elo ratings from the Chatbot Arena can be a more reliable predictor of a model's success in these human-centric roles than scores on purely technical benchmarks.\n",
    "- **What other concepts, techniques, or areas is this related to?**\n",
    "The Chatbot Arena's methodology draws on principles of **crowdsourcing** and **human computation** for data collection and evaluation. The use of the **Elo rating system** is borrowed from competitive game theory and provides a robust way to rank entities based on pairwise comparisons. The focus on user experience and preference aligns strongly with the field of **Human-Computer Interaction (HCI)** and **qualitative research methods**. It serves as a practical example of incorporating **human-in-the-loop evaluation** into the AI development and assessment lifecycle.\n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "When selecting an LLM for a task that heavily involves human interaction and conversation (like building a chatbot or an AI assistant), consider the LMSYS Chatbot Arena's Elo ratings as a significant data point alongside traditional benchmark scores. Actively participate in voting on the Arena to gain first-hand experience with the conversational nuances of different models, which can be more insightful than just reading about their capabilities.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    "The LMSYS Chatbot Arena is like a big online \"chatbot tournament\" where real people chat with two anonymous AIs at a time and vote for which one they liked better, and these votes are used to give each AI a score (an Elo rating) that shows how good it is at having a conversation from a human's point of view.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "The Chatbot Arena is most relevant for any project or domain where the primary function of the LLM is to engage in natural, effective, and satisfying conversations with humans. This includes developing customer service agents, virtual personal assistants, AI companions, interactive educational tools, creative writing partners, and any system where the overall quality of the human-AI dialogue and user experience is a key performance indicator.\n",
    "\n",
    "# Day 2 - Commercial Applications of Large Language Models: From Law to Education\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This lesson provides a rapid overview of five distinct commercial applications where Large Language Models (LLMs) are making a significant impact across various industries. The examples showcased include Harvey for legal applications, Nebula.io for talent and recruitment, Bloop AI for legacy code porting, Salesforce Einstein Copilot Health Actions for healthcare, and Khan Academy's Khanmigo for education. The intent is to illustrate the practical utility of LLMs and to prompt thinking about how one would use the previously discussed leaderboards and evaluation metrics to select appropriate models for such real-world challenges.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- ‚öñÔ∏è **LLMs in Law (Harvey)**: Harvey exemplifies the use of LLMs in the legal sector by providing tools for lawyers to perform tasks such as answering complex legal questions (e.g., defining \"claim of disloyalty\") and likely assisting in the analysis of legal documents to identify key terms and information.\n",
    "- üßë‚Äçüíº **LLMs in Talent & Recruitment (Nebula.io)**: The speaker's own company, Nebula.io, applies LLMs to the human resources field. It aids managers in the hiring process and in engaging with candidates, while also helping individuals explore career paths by understanding the content and context of their professional experience.\n",
    "- üíª **LLMs in Legacy Code Modernization (Bloop AI)**: Bloop AI is highlighted as an innovative use of LLMs to port legacy code (such as COBOL) into modern programming languages like Java. This addresses significant challenges in maintaining old software systems and can leverage coding models to potentially add comments and test cases during the conversion.\n",
    "- ‚ù§Ô∏è **LLMs in Healthcare (Salesforce Einstein Copilot Health Actions)**: This Salesforce product demonstrates LLM application in healthcare by offering tools for practitioners. An example given is a dashboard that can summarize the outcomes of a medical appointment for a care coordinator, thereby saving time and improving information flow.\n",
    "- üìö **LLMs in Education (Khan Academy's Khanmigo)**: Khanmigo by Khan Academy represents the use of LLMs as an educational tool, designed to act as a companion for teachers, learners, and parents, showcasing the potential for AI to support and personalize the educational experience.\n",
    "- ü§î **Connecting Use Cases to Model Selection**: Throughout these examples, the core idea emphasized is the importance of considering how one would select the most suitable LLM for each specific problem. This involves referencing appropriate leaderboards and benchmarks‚Äîfor instance, using the Open Medical LLM Leaderboard for a healthcare application like Salesforce's product, or focusing on coding-specific metrics for a tool like Bloop AI.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "**Leveraging Commercial Use Cases for Informed LLM Selection and Innovation**\n",
    "\n",
    "- **Why is this concept important to know or understand?**\n",
    "Studying existing commercial applications of LLMs is invaluable for aspiring and practicing LLM engineers for several reasons:\n",
    "    1. **Demonstrates Practical Viability**: It showcases how abstract LLM capabilities translate into tangible business value and solutions to real-world problems, moving beyond theoretical performance.\n",
    "    2. **Reveals Diverse LLM Requirements**: Different industries and tasks necessitate different strengths in an LLM. A legal LLM might prioritize factual accuracy and nuanced understanding, a code-porting LLM needs strong multilingual code generation skills, and an educational AI requires safe and engaging conversational abilities. Recognizing this diversity is key to effective model selection.\n",
    "    3. **Provides Context for Evaluation**: Knowing how LLMs are used in practice helps in choosing relevant benchmarks and leaderboards. For example, if building a medical diagnosis assistant, benchmarks on medical knowledge (like those on the Open Medical LLM Leaderboard) become more pertinent than generic language understanding scores.\n",
    "    4. **Fosters Innovative Thinking**: Understanding current applications can inspire new ideas for leveraging LLMs in other domains or for solving previously unaddressed challenges.\n",
    "    By connecting theoretical knowledge of LLMs and evaluation metrics to these practical examples, one can develop a more grounded and effective approach to applying AI.\n",
    "- **How does it connect with real-world tasks, problems, or applications?**\n",
    "When an engineer is tasked with developing an LLM-powered solution, they can draw inspiration and guidance from these established commercial use cases:\n",
    "    - For a task involving contract analysis, one might look at how Harvey approaches legal text and select models known for strong textual comprehension and reasoning.\n",
    "    - If tasked with automating code documentation or migration, the approach of Bloop AI would suggest prioritizing models that excel on coding benchmarks like HumanEval or the BigCode Leaderboard.\n",
    "    - When designing an AI tutor, the principles behind Khanmigo would guide the selection towards models with good conversational skills (e.g., high Elo on Chatbot Arena) and strong alignment for safe educational content.\n",
    "    This allows for a more targeted selection of candidate LLMs and a clearer definition of success metrics for the project.\n",
    "- **What other concepts, techniques, or areas is this related to?**\n",
    "This exploration of commercial applications is closely linked to **use case discovery**, **requirements engineering**, and **solution architecture** within the AI development lifecycle. It highlights the importance of **domain expertise** in successfully applying LLMs (e.g., legal knowledge for Harvey, educational principles for Khanmigo). Furthermore, it is a practical aspect of **AI strategy** and **AI product management**, where understanding market needs and existing solutions informs the development of new and competitive AI-driven products and services. It also underscores the principle that the \"best\" LLM is always **task-dependent**.\n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "When you encounter a new LLM or learn about a specific evaluation metric, try to map it to potential real-world applications like the ones discussed (law, HR, code migration, healthcare, education). Conversely, when analyzing a business problem, think about which LLM capabilities and, therefore, which types of models and benchmarks would be most relevant to crafting a solution, using these commercial examples as a guide for what's possible and practical.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    "By looking at how different companies successfully use AI \"brains\" (LLMs) in real businesses‚Äîlike Harvey helping lawyers, Bloop AI fixing old computer code, or Khanmigo teaching students‚Äîwe can learn what kinds of AI are good for different jobs and get ideas for how to use AI to solve other problems.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "Understanding these diverse commercial applications of LLMs is highly beneficial for anyone involved in the ideation, design, development, or deployment of AI solutions across any industry. It is particularly crucial for AI product managers, solutions architects, business analysts trying to identify AI opportunities, and entrepreneurs seeking to build LLM-powered ventures, as it provides concrete examples of value creation and informs strategic model selection.\n",
    "\n",
    "# Day 2 - Comparing Frontier and Open-Source LLMs for Code Conversion Projects\n",
    "\n",
    "## **Summary**\n",
    "\n",
    "This segment introduces an engaging practical challenge for the week: to develop a code conversion tool capable of translating Python code into C++ for performance optimization. This project will involve selecting and then utilizing both a cutting-edge frontier model and a suitable open-source model, followed by a comparison of their results. The lesson also serves as a recap of the substantial skills learners have acquired by the end of Day 2 of Week 4, particularly emphasizing the newfound ability to navigate various leaderboards and evaluation resources to confidently select the most appropriate Large Language Models (LLMs) for diverse tasks and projects.\n",
    "\n",
    "## **Highlights**\n",
    "\n",
    "- üíª **This Week's Challenge: Python to C++ Code Conversion for Performance**:\n",
    "    - The central project is to build a tool that converts Python code to C++ with the goal of improving execution performance. This task is inspired by real-world applications like Bloop AI's legacy code porting.\n",
    "    - The approach will involve using **both a frontier LLM and an open-source LLM**.\n",
    "    - A key initial step will be **selecting the most suitable LLMs** for this specific code generation and translation task, drawing upon the knowledge of benchmarks and leaderboards covered.\n",
    "    - The project aims to assess how effectively LLMs can assist in code optimization and translation.\n",
    "- üõ†Ô∏è **Recap of Acquired LLM Engineering Skills (End of Day 2, Week 4)**:\n",
    "    - **Frontier Model Proficiency**: Ability to develop applications using advanced frontier models, including integrating them with external tools to build AI assistants.\n",
    "    - **Open-Source Solution Development**: Competence in building solutions using the Hugging Face ecosystem, leveraging both the high-level `Pipeline` API for various inference tasks and the more fundamental `Tokenizers` and `Models` APIs. This lower-level API knowledge is noted as crucial for future topics on model training.\n",
    "    - **Strategic LLM Selection**: A core achievement is the ability to **confidently choose the right LLM(s)** for specific projects. This involves shortlisting (typically 2-3) candidates for prototyping based on a thorough understanding and critical interpretation of results from leaderboards (like the Hugging Face Open LLM Leaderboard, Vellum.ai, Scale AI's SEAL), arenas (like LMSYS Chatbot Arena), and other evaluation resources.\n",
    "- üéØ **Anticipated Learning Outcomes from the Challenge**:\n",
    "    - A deeper, practical understanding of how to assess the code generation and translation capabilities of various LLMs.\n",
    "    - Hands-on experience in utilizing a frontier model specifically for code generation.\n",
    "    - The ability to build a complete, end-to-end solution that incorporates LLM-generated code, marking another significant step towards becoming a highly proficient LLM engineer.\n",
    "\n",
    "## **Conceptual Understanding**\n",
    "\n",
    "**Applying LLM Evaluation and Selection Expertise to a Practical Code Generation and Optimization Task**\n",
    "\n",
    "- **Why is this concept important to know or understand?**\n",
    "The challenge of creating a Python-to-C++ code converter is a highly practical exercise that directly applies the week's lessons on evaluating and selecting LLMs. Successfully tackling this task requires more than just prompting an LLM; it demands a strategic approach to choosing the right models based on their demonstrated strengths in code understanding and generation. Key considerations would include:\n",
    "    - **Performance on Coding Benchmarks**: Prioritizing models that rank highly on benchmarks like HumanEval, MultiPL-E, or those featured on the BigCode Models Leaderboard.\n",
    "    - **Instruction Following Capabilities**: Selecting models that can accurately interpret and execute the specific requirements of code translation (relevant to IFEval).\n",
    "    - **Context Window Size**: Ensuring the chosen models can handle potentially lengthy code snippets.\n",
    "    - **Comparing Frontier vs. Open-Source**: Evaluating the trade-offs between the potentially superior performance or specialized coding abilities of a frontier model versus the customizability, control, or cost-effectiveness of an open-source alternative for this particular coding challenge.\n",
    "    This project crystallizes theoretical knowledge about LLM evaluation into a tangible engineering problem, forcing a practical application of selection criteria.\n",
    "- **How does it connect with real-world tasks, problems, or applications?**\n",
    "Code translation, modernization, and optimization are common and often complex challenges in the software industry. Organizations frequently need to migrate code from older languages to newer ones for better performance, maintainability, or to leverage modern ecosystems, as exemplified by the Bloop AI use case for COBOL. This challenge simulates such real-world scenarios, where an LLM must not only understand the syntax and semantics of Python but also generate equivalent, efficient, and correct C++ code. Successfully using LLMs for this purpose demonstrates a high-value skill in leveraging AI for software engineering productivity and improvement.\n",
    "- **What other concepts, techniques, or areas is this related to?**\n",
    "This practical challenge intersects with several important areas:\n",
    "    - **Program Synthesis**: The automatic generation of code based on specifications.\n",
    "    - **Compiler Design**: While not building a full compiler, the task involves aspects of language translation.\n",
    "    - **Software Re-engineering & Modernization**: Applying LLMs to update or improve existing codebases.\n",
    "    - **Performance Engineering**: Using C++ as a target implies a focus on optimizing code execution speed.\n",
    "    The initial model selection phase is a direct application of the **LLM evaluation techniques** covered, including the interpretation of **leaderboard data** and **benchmark results**. The comparative analysis of the frontier versus open-source model output reinforces the importance of **empirical testing** and **prototyping** in AI development.\n",
    "\n",
    "## **Reflective Questions**\n",
    "\n",
    "- **How can I apply this concept in my daily data science work or learning?**\n",
    "For any new project requiring LLM intervention, like this Python to C++ code conversion task, the immediate next step after defining the problem should be to consult relevant leaderboards (e.g., BigCode Leaderboard, performance benchmarks if speed of conversion is key) and specific benchmark results (e.g., HumanEval, MultiPL-E) to identify a shortlist of 2-3 promising frontier and open-source models. Then, outline a clear methodology for comparing their outputs and effectiveness specifically for the code translation and optimization requirements of this challenge.\n",
    "- **Can I explain this concept to a beginner in one sentence?**\n",
    "We're going to tackle a cool project: building an AI tool that can turn Python computer code into faster C++ code, and the first big step is to use our newly learned skills to pick the smartest AI \"brains\" (both the big commercial ones and the open-source ones) for this specific coding job by looking at their \"exam scores\" on various coding tests and then seeing how well they actually do the conversion.\n",
    "- **Which type of project or domain would this concept be most relevant to?**\n",
    "This code conversion challenge, and the skills emphasized in its setup, are highly pertinent to the software engineering and development domains. This includes areas like legacy system modernization, cross-language development, automated code optimization, and tools for improving developer productivity. More broadly, the rigorous model selection process highlighted is a universal best practice applicable to any project aiming to effectively leverage LLMs by choosing the most suitable model based on empirical evidence and specific task demands."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
