{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display, update_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the openai model from the github models\n",
    "load_dotenv(override=True)\n",
    "token = os.getenv(\"GITHUB_TOKEN\")\n",
    "endpoint = \"https://models.inference.ai.azure.com\"\n",
    "\n",
    "openai_client = OpenAI(\n",
    "    base_url=endpoint,\n",
    "    api_key=token,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found him to be too mean!\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o-mini\n",
    "# Temperature setting controls creativity\n",
    "\n",
    "completion = openai_client.chat.completions.create(\n",
    "    model='gpt-4o-mini',\n",
    "    messages=prompts,\n",
    "    temperature=0.7\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure! Here's one for the data-savvy crowd:\n",
      "\n",
      "Why did the data scientist break up with the graph?\n",
      "\n",
      "**It just didn’t have enough points!**\n"
     ]
    }
   ],
   "source": [
    "# GPT-4o\n",
    "\n",
    "completion = openai_client.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.4\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's one:\n",
      "\n",
      "\"Why did the data scientist break up with the statistician?  \n",
      "Because they found a new model with a better fit!\"\n"
     ]
    }
   ],
   "source": [
    "# GPT-o1\n",
    "\n",
    "completion = openai_client.chat.completions.create(\n",
    "    model='o1',\n",
    "    messages=prompts,\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many data scientists does it take to change a light bulb? \n",
      "\n",
      "Just one—but not before running 10,000 simulations to make sure the change is statistically significant!\n"
     ]
    }
   ],
   "source": [
    "# GPT o3-mini\n",
    "\n",
    "completion = openai_client.chat.completions.create(\n",
    "    model='o3-mini',\n",
    "    messages=prompts,\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Deciding whether a business problem is suitable for a Large Language Model (LLM) solution involves a systematic evaluation of the problem's characteristics, constraints, and requirements. Here's a step-by-step guideline to help you determine suitability:\n",
       "\n",
       "---\n",
       "\n",
       "### **1. Define the Business Problem**\n",
       "- Is the problem centered on language, text, or conversational data?\n",
       "  - Examples: text generation, summarization, classification, translation, answering questions, etc.\n",
       "- Is the problem well-defined with clear objectives and measurable success criteria?\n",
       "\n",
       "---\n",
       "\n",
       "### **2. Assess the Nature of the Task**\n",
       "- **Suitable tasks for LLMs:**\n",
       "  - Text-based tasks (e.g., summarization, content generation, sentiment analysis).\n",
       "  - Conversational AI (e.g., customer support chatbots, virtual assistants).\n",
       "  - Information retrieval and Q&A (e.g., FAQ answering, document search).\n",
       "  - Language translation or transcription.\n",
       "  - Code generation or debugging.\n",
       "- **Unsuitable tasks for LLMs:**\n",
       "  - Highly numerical or computation-heavy tasks.\n",
       "  - Real-time decision-making with strict latency limits.\n",
       "  - Tasks requiring highly domain-specific or niche knowledge without sufficient data to fine-tune.\n",
       "\n",
       "---\n",
       "\n",
       "### **3. Evaluate Data Availability**\n",
       "- Do you have access to sufficient, high-quality text data for the problem domain?\n",
       "- Is the data labeled or structured for supervised fine-tuning (if needed)?\n",
       "- Is the data sensitive, confidential, or regulated by compliance requirements (e.g., GDPR, HIPAA)?\n",
       "\n",
       "---\n",
       "\n",
       "### **4. Consider LLM Capabilities**\n",
       "- Does the problem require:\n",
       "  - Understanding and generating natural language?\n",
       "  - Context retention over multiple interactions (e.g., chatbots)?\n",
       "  - Knowledge of general or domain-specific topics?\n",
       "- Is the problem within the scope of the LLM's pre-trained knowledge, or would fine-tuning be required?\n",
       "\n",
       "---\n",
       "\n",
       "### **5. Evaluate Business Constraints**\n",
       "- **Cost:** Does your budget support the computational and licensing costs of using an LLM?\n",
       "- **Latency:** Can the business tolerate delays in model inference, or does it require real-time responses?\n",
       "- **Scalability:** Can the LLM solution scale with your business needs?\n",
       "- **Ethics & Compliance:** Does the LLM align with ethical guidelines and regulatory requirements?\n",
       "\n",
       "---\n",
       "\n",
       "### **6. Assess Alternatives**\n",
       "- Are there simpler solutions (e.g., rule-based systems, traditional machine learning models) that could solve the problem effectively?\n",
       "- Is an LLM overkill for the task complexity?\n",
       "\n",
       "---\n",
       "\n",
       "### **7. Test Feasibility**\n",
       "- Run a small-scale Proof of Concept (PoC) using an LLM to validate its effectiveness for the task.\n",
       "- Measure performance against Key Performance Indicators (KPIs) such as accuracy, relevance, or user satisfaction.\n",
       "\n",
       "---\n",
       "\n",
       "### **8. Monitor and Maintain**\n",
       "- Are you prepared for ongoing monitoring, retraining, and maintenance of the LLM solution?\n",
       "- Do you have processes in place to handle model drift, biases, or unexpected outputs?\n",
       "\n",
       "---\n",
       "\n",
       "### **Decision Checklist**\n",
       "| Question                                    | Yes | No  |\n",
       "|---------------------------------------------|-----|-----|\n",
       "| Is the problem primarily language-based?    | ✅  | ❌  |\n",
       "| Does the problem align with LLM capabilities?| ✅  | ❌  |\n",
       "| Do you have sufficient high-quality data?   | ✅  | ❌  |\n",
       "| Are business constraints (cost, latency, etc.) manageable? | ✅  | ❌  |\n",
       "| Are simpler alternatives insufficient?      | ✅  | ❌  |\n",
       "\n",
       "- If most answers are \"Yes,\" the problem is likely suitable for an LLM solution.\n",
       "- If many answers are \"No,\" consider alternative approaches.\n",
       "\n",
       "---\n",
       "\n",
       "By systematically evaluating these factors, you can make an informed decision on whether an LLM solution is the right fit for your business problem."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = openai_client.chat.completions.create(\n",
    "    model='gpt-4o',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    if chunk.choices:  # Check if choices is not empty\n",
    "        reply += chunk.choices[0].delta.content or ''\n",
    "        reply = reply.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(reply), display_id=display_handle.display_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_model = \"gpt-4o-mini\"\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = openai_client.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Oh great, another greeting. What’s next, a boring small talk?'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_gpt()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
