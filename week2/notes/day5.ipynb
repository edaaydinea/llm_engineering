{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7f8e00",
   "metadata": {},
   "source": [
    "# Day 5 - Multimodal AI Assistants: Integrating Image and Sound Generation\n",
    "\n",
    "### Summary\n",
    "\n",
    "This session focuses on building advanced AI assistants using agents and tools, specifically creating a multimodal AI assistant that can generate images and sounds. It introduces the concept of autonomous software entities capable of performing complex tasks through interaction within an agent framework. The session walks through coding functions to generate images using Dall-E 3 and integrating these functions into an AI assistant to enhance its capabilities.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- ü§ñ Introduction to autonomous software agents and agent frameworks.\n",
    "- üéØ Agents are goal-oriented and task-specific software entities.\n",
    "- üß† Agent frameworks enable complex problem-solving with limited human involvement.\n",
    "- üõ†Ô∏è Tools like database and internet connections enhance agent functionality.\n",
    "- üñºÔ∏è Building an image generation function using Dall-E 3.\n",
    "- üîä Adding sound generation capabilities to the AI assistant.\n",
    "- üßë‚Äçüíª Creating a multimodal AI assistant that can speak and draw.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "```python\n",
    "  # Example: Function to generate images using Dall-E 3\n",
    "  # (Conceptual example, actual code would involve API calls)\n",
    "  def generate_image(prompt):\n",
    "      # API call to Dall-E 3 with the prompt\n",
    "      image = call_dalle3_api(prompt)\n",
    "      return image\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "  # Example: Integrating image generation into an AI assistant\n",
    "  # (Conceptual example, actual code would involve agent framework)\n",
    "  def airline_assistant(user_request):\n",
    "      if \"draw\" in user_request:\n",
    "          image = generate_image(user_request)\n",
    "          return display_image(image)\n",
    "      # ... other assistant functionalities ...\n",
    "```\n",
    "\n",
    "# Day 5 - Multimodal AI: Integrating DALL-E 3 Image Generation in JupyterLab\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This lab session focuses on extending an AI assistant with multimodal capabilities, specifically image and audio generation. It introduces Dall-E 3 for image creation and OpenAI's text-to-speech for audio output, integrating these functionalities into the existing AI assistant framework. The session emphasizes the creative potential of these tools while also noting the associated costs.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- üñºÔ∏è Integration of Dall-E 3 for generating creative images based on text prompts.\n",
    "- üí∏ Cost consideration for image generation using Dall-E 3 ($0.04 per image).\n",
    "- üîä Implementation of text-to-speech functionality using OpenAI's audio API.\n",
    "- üó£Ô∏è Exploration of different voice options (e.g., Onyx, Alloy) for audio output.\n",
    "- üêç Use of Python libraries like PIL (Python Imaging Library) and Pi Dub for image and audio processing.\n",
    "- üíº Expansion of the AI assistant's capabilities to include multimodal interactions.\n",
    "- üöÄ Preparation for building a full agent framework in the next video.\n",
    "\n",
    "### **Code Examples**\n",
    "\n",
    "```python\n",
    "  # Function to generate an image using Dall-E 3\n",
    "  def artist(city):\n",
    "      response = openai.images.generate(\n",
    "          model=\"dall-e-3\",\n",
    "          prompt=f\"An image representing a vacation in {city}, showing tourist spots and everything unique about {city} in a vibrant pop art style.\",\n",
    "          size=\"256x256\",\n",
    "          quality=\"standard\",\n",
    "          n=1,\n",
    "      )\n",
    "      image_url = response.data[0].b64_json\n",
    "      image_bytes = base64.b64decode(image_url)\n",
    "      image_io = io.BytesIO(image_bytes)\n",
    "      image = Image.open(image_io)\n",
    "      return image\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "  # Function to generate audio from text using OpenAI's speech API\n",
    "  def talker(text):\n",
    "      response = openai.audio.speech.create(\n",
    "          model=\"tts-1\",\n",
    "          voice=\"onyx\",\n",
    "          input=text\n",
    "      )\n",
    "      audio_bytes = response.content\n",
    "      audio_io = io.BytesIO(audio_bytes)\n",
    "      audio_segment = AudioSegment.from_file(audio_io)\n",
    "      play(audio_segment)\n",
    "\n",
    "```\n",
    "\n",
    "# Day 5 - Build a Multimodal AI Agent: Integrating Audio & Image Tools\n",
    "\n",
    "### Summary\n",
    "\n",
    "The session focuses on building a full agent framework that integrates various AI techniques to create a more sophisticated chatbot. This framework combines breaking down complex problems, utilizing tools for extra capabilities, and incorporating an agent environment for collaboration. The resulting application demonstrates a multimodal interaction where the chatbot can respond with both text and audio, and even generate images based on the context of the conversation, such as displaying an image of London when the user asks about ticket prices to London.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- üß© The agent framework combines techniques like breaking down tasks and using tools to enhance the LLM's capabilities.\n",
    "- üó£Ô∏è The chatbot now integrates a text-to-speech model, allowing it to speak its responses to the user.\n",
    "- üñºÔ∏è When a user inquires about ticket prices to a specific city, the framework triggers an image generation model to display a relevant image of that city.\n",
    "- üí¨ The underlying chat function and tool usage remain similar to previous implementations, with a new addition to trigger image generation upon a tool call related to city information.\n",
    "- ‚öôÔ∏è Building a more complex UI with Gradio required moving away from the default chat interface to create a custom layout for input, buttons, and displaying the generated image.\n",
    "- ‚úàÔ∏è The demonstration showcases a multimodal airline AI assistant that can provide ticket prices, speak the response, and display relevant imagery, representing a basic form of an agentic framework.\n",
    "- ‚ú® The resulting multimodal app highlights the power of combining different AI models with a relatively small amount of code to create compelling user experiences.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "```python\n",
    "# Example of a chat function (conceptual)\n",
    "def chat(message, history):\n",
    "    # Process message and history for LLM\n",
    "    response = query_llm(formatted_history_and_message)\n",
    "    return response\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "# Example of tool usage with image generation (conceptual)\n",
    "def handle_tool_call(tool_name, arguments):\n",
    "    if tool_name == \"get_ticket_price\":\n",
    "        price = get_price(arguments[\"city\"])\n",
    "        image_url = generate_city_image(arguments[\"city\"])\n",
    "        return f\"Ticket price: {price}\", image_url\n",
    "    # ... other tools\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "# Example of integrating text-to-speech (conceptual)\n",
    "def talker(text):\n",
    "    generate_audio(text)\n",
    "    play_audio()\n",
    "```\n",
    "\n",
    "# Day 5 - How to Build a Multimodal AI Assistant: Integrating Tools and Agents\n",
    "\n",
    "### Summary\n",
    "\n",
    "This session reviews the newly created multimodal airline AI assistant and presents a set of challenges to further enhance its capabilities. These challenges include adding a booking tool, integrating a translation agent using a different LLM (like Claude), and incorporating an audio-to-text agent to complete the interaction loop. Successfully completing these tasks will signify a strong understanding of multimodality and agent orchestration, marking significant progress in mastering LLM engineering. The upcoming week will then shift focus to exploring the open-source LLM ecosystem, particularly Hugging Face, pipelines, tokenizers, and running inference on open-source models.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- ü§© The newly developed airline AI assistant is lauded for its ability to generate diverse and compelling images and for the ease with which such sophisticated frameworks can be built.\n",
    "- üßë‚Äçüíª The first challenge is to add a tool that simulates making a booking, with output indicating the booking confirmation, potentially by printing to the console or writing to a file.\n",
    "- üó£Ô∏è The second challenge involves integrating another agent that can translate the AI's responses into a different language, displayed in a separate panel using a different LLM like Claude, requiring modifications to the Gradio interface.\n",
    "- üéß The final multimodal challenge is to add an agent that can transcribe audio input into text, which will then serve as the input for the AI assistant, completing the full conversational loop.\n",
    "- üöÄ Completing these challenges will signify a strong grasp of multimodality and the ability to combine different agents to achieve complex tasks, marking 25% completion of the LLM engineering mastery journey.\n",
    "- ü§ó The next week will focus on the open-source community, specifically working with Hugging Face, understanding pipelines and tokenizers, and running inference on open-source transformer models using Google Colab with GPUs.\n",
    "- üõ£Ô∏è By the end of the following week, learners are expected to be highly proficient in performing inference on open-source LLMs.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "```python\n",
    "# Conceptual example of adding a booking tool\n",
    "def make_booking(flight_details):\n",
    "    # Simulate booking process\n",
    "    booking_confirmation = f\"Booking confirmed for: {flight_details}\"\n",
    "    print(booking_confirmation)\n",
    "    return booking_confirmation\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "# Conceptual example of a translation agent\n",
    "def translate_text(text, target_language, model=\"claude\"):\n",
    "    # Call a translation model (e.g., Claude)\n",
    "    translated_text = call_translation_api(text, target_language, model)\n",
    "    return translated_text\n",
    "\n",
    "```\n",
    "\n",
    "```python\n",
    "# Conceptual example of an audio-to-text agent\n",
    "def transcribe_audio(audio_input):\n",
    "    # Use a speech-to-text model to transcribe audio\n",
    "    text_output = call_speech_to_text_api(audio_input)\n",
    "    return text_output\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
