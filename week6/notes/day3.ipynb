{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69a9e131",
   "metadata": {},
   "source": [
    "# Day 3 - Feature Engineering and Bag of Words: Building ML Baselines for NLP\n",
    "\n",
    "### Summary\n",
    "This lecture introduces the concept and importance of establishing **baseline models** using traditional machine learning techniques before diving into more complex solutions like Large Language Models (LLMs). The session aims to demonstrate how foundational methods such as feature engineering, Bag of Words, Word2Vec, linear regression, random forests, and support vector regression can be applied to a real-world business problem—predicting product prices from descriptions—thereby providing a benchmark to measure the efficacy of advanced models and to assess if simpler approaches might be sufficient. Understanding these traditional techniques is crucial for data scientists to make informed decisions about model selection and to appreciate the incremental value of more sophisticated approaches.\n",
    "\n",
    "---\n",
    "### Highlights\n",
    "- **Revisiting Traditional ML:** The lecture takes a step back to explore foundational machine learning models, emphasizing their continued relevance even in the age of advanced AI. This is important for a well-rounded understanding, allowing data scientists to choose the right tool for the job rather than always defaulting to the newest trend.\n",
    "- **The Critical Role of Baselines:** Establishing a simple baseline model is fundamental in data science. It provides a **yardstick** against which more complex models (like LLMs or deep neural networks) are measured, ensuring that any increased complexity leads to tangible performance gains.\n",
    "- **LLMs: Not a Universal Solution:** The lecture highlights that LLMs may not always be the optimal choice, especially for tasks like predicting a numerical value (e.g., product price) from text. Traditional NLP combined with regression models might offer a more direct, interpretable, and efficient solution.\n",
    "- **Feature Engineering Explained:** This is an \"old school\" but vital process where data scientists use their domain knowledge to manually create relevant input variables (features) from raw data. For price prediction, an example feature could be the product's Amazon best-seller rank, directly impacting model performance.\n",
    "- **Bag of Words (BoW):** A foundational NLP technique where text is represented as a collection of word counts, ignoring grammar and word order but capturing word frequencies (excluding common \"stop words\"). This vector of counts can then be used with models like linear regression to predict outcomes like price.\n",
    "- **Word2Vec for Semantic Embeddings:** A more advanced technique than BoW, Word2Vec is a neural network-based method that generates dense vector representations (embeddings) of words. These embeddings capture semantic relationships (e.g., \"king\" - \"man\" + \"woman\" ≈ \"queen\"), providing richer features for ML models.\n",
    "- **Linear Regression as a Starting Point:** This statistical method is used to model the relationship between a dependent variable (e.g., price) and independent variables (e.g., engineered features, BoW vectors, Word2Vec embeddings) by fitting a linear equation. It’s a common first model for regression tasks due to its simplicity and interpretability.\n",
    "- **Random Forests for Enhanced Prediction:** An ensemble learning method that constructs multiple decision trees using random subsets of data and features. By averaging the predictions of these trees, random forests can achieve higher accuracy and are more robust to overfitting than individual trees, making them suitable for complex regression tasks.\n",
    "- **Support Vector Regression (SVR):** A type of Support Vector Machine (SVM) adapted for regression. SVR aims to find a function that best fits the data by allowing a certain tolerance (margin) for errors, making it effective for problems with non-linear relationships and high-dimensional feature spaces.\n",
    "- **Practical Application Focus:** All discussed techniques (feature engineering, BoW, Word2Vec, linear regression, random forests, SVR) will be practically applied to predict product prices from descriptions. This hands-on approach helps solidify understanding and demonstrates how these models perform on a real-world commercial problem.\n",
    "\n",
    "---\n",
    "### Conceptual Understanding\n",
    "- **Feature Engineering**\n",
    "    1.  **Why is this concept important?** Feature engineering is crucial because the quality and relevance of input features directly dictate a model's performance. Well-crafted features can simplify complex problems, improve model accuracy, and reduce computational cost.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** In predicting product prices, features like 'brand reputation score', 'number of positive reviews', or 'seasonality index' (derived from date) can be engineered to significantly improve prediction accuracy beyond using just raw description text. Other applications include credit risk assessment (e.g., creating a 'debt-to-income ratio' feature) or medical diagnosis (e.g., combining various test results into a composite risk score).\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Data preprocessing (scaling, normalization, handling missing values), dimensionality reduction (e.g., PCA), feature selection techniques (filter, wrapper, embedded methods), and domain expertise acquisition are vital complements to feature engineering.\n",
    "\n",
    "- **Bag of Words (BoW)**\n",
    "    1.  **Why is this concept important?** BoW is a fundamental and intuitive method for converting unstructured text data into a numerical format that machine learning algorithms can process. It provides a simple way to quantify text content.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** It's widely used as a baseline in text classification (e.g., spam email detection, sentiment analysis of customer feedback), document clustering, and information retrieval. For product price prediction, BoW on descriptions can identify words indicative of higher or lower prices (e.g., \"premium,\" \"refurbished\").\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** TF-IDF (Term Frequency-Inverse Document Frequency) for weighting word importance, n-grams for capturing short phrases, stop-word removal, stemming/lemmatization for text normalization, and topic modeling (like LDA) which can use BoW as input.\n",
    "\n",
    "- **Word2Vec (and similar word embeddings)**\n",
    "    1.  **Why is this concept important?** Word2Vec captures the semantic meaning and context of words by representing them as dense vectors in a multi-dimensional space, where similar words are closer together. This is a significant improvement over sparse BoW representations.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** Used in semantic search (finding documents with similar meaning, not just keywords), recommendation systems (recommending items based on textual descriptions), machine translation, and more nuanced sentiment analysis. For price prediction, Word2Vec can help understand if words like \"sleek\" or \"robust\" in a description correlate with price.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Other word embedding techniques like GloVe and FastText, sentence and document embeddings (e.g., Doc2Vec, Sentence-BERT), neural network architectures (especially recurrent neural networks and transformers), and attention mechanisms.\n",
    "\n",
    "- **Random Forests**\n",
    "    1.  **Why is this concept important?** Random Forests are powerful, versatile ensemble learning models that offer high accuracy, robustness against overfitting, and the ability to handle diverse data types without extensive preprocessing. They also provide useful feature importance estimates.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** Widely used in various classification and regression tasks such as fraud detection, medical diagnosis (predicting disease risk), stock market prediction, customer churn prediction, and image classification. In the context of product price prediction, they can model complex, non-linear relationships between features (from text or engineered) and price.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Decision Trees (the base learners), ensemble methods in general (bagging, boosting), Gradient Boosting Machines (GBM), XGBoost, LightGBM, and techniques for hyperparameter tuning.\n",
    "\n",
    "- **Support Vector Regression (SVR)**\n",
    "    1.  **Why is this concept important?** SVR is an extension of Support Vector Machines for regression problems, effective in high-dimensional spaces and when the number of features might exceed the number of samples. Its use of kernel functions allows it to model non-linear relationships.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** SVR is applied in financial forecasting (predicting stock prices), time series analysis, house price prediction, and in engineering for predicting material properties or system performance. For product price prediction, SVR can effectively map complex feature sets derived from product descriptions to a continuous price value.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Support Vector Machines (SVMs) for classification, kernel trick (linear, polynomial, RBF, sigmoid kernels), concepts of margin and support vectors, regularization (C parameter), and optimization techniques used in SVM/SVR training.\n",
    "\n",
    "---\n",
    "### Reflective Questions\n",
    "1.  **Application:** Which specific dataset or project could benefit from using a Bag-of-Words model combined with linear regression as an initial baseline? Provide a one-sentence explanation.\n",
    "    * *Answer:* A project aiming to classify news articles into topics (e.g., sports, politics, technology) could benefit from a BoW with linear regression (or logistic regression for classification) baseline, as the frequency of specific keywords often provides a strong initial signal for a document's category.\n",
    "2.  **Teaching:** How would you explain why establishing a baseline model is crucial to a junior colleague, using one concrete example? Keep the answer under two sentences.\n",
    "    * *Answer:* Think of it like this: if you're trying to improve a car's speed, you first need to know its current top speed (the baseline); otherwise, you won't know if your fancy new engine modifications actually made it faster or just added complexity for no real gain.\n",
    "3.  **Extension:** After implementing feature engineering and linear regression for price prediction, what related technique or area should you explore next if the linear model underperforms, and why?\n",
    "    * *Answer:* If linear regression underperforms, exploring polynomial regression or tree-based models like Random Forests would be a logical next step, as these can capture non-linear relationships between the engineered features and price that a simple linear model cannot.\n",
    "\n",
    "# Day 3 - Baseline Models in ML: Implementing Simple Prediction Functions\n",
    "\n",
    "### Summary\n",
    "This JupyterLab session outlines the setup and initial steps for establishing baseline models in a machine learning project focused on product price prediction. It covers importing essential libraries like pandas, scikit-learn, and Gensim, and introduces a custom `Tester` class designed to standardize the evaluation of different models by running them on test data, calculating error metrics like squared log error, and visualizing performance. The utility of this `Tester` class is demonstrated by evaluating two \"comedy\" baseline models: one that guesses prices randomly and another that always guesses the average price from the training set, thereby setting a performance floor for subsequent, more sophisticated models.\n",
    "\n",
    "---\n",
    "### Highlights\n",
    "- **Core Libraries for Traditional ML:** The session begins by importing crucial Python libraries for data science: `pandas` for structured data manipulation, `numpy` for numerical computations, `scikit-learn` (sklearn) for a wide array of machine learning algorithms including `LinearRegression`, `SVR` (Support Vector Regression), and `RandomForestRegressor`, and `Gensim` for NLP tasks like Word2Vec. This is relevant as these libraries form the bedrock of most traditional machine learning workflows.\n",
    "- **Introducing the `Tester` Class:** A key component introduced is a custom `Tester` class. This class is designed to provide a standardized and repeatable way to evaluate different price prediction models, taking a prediction function as input, running it on test data, and generating reports and visualizations. This is important for maintaining consistency and efficiency when comparing multiple models.\n",
    "- **Standardized Model Evaluation Workflow:** The `Tester` class executes a prediction function for numerous test items (250 in this case), calculates the absolute error and the \"squared log error\" (defined as $(\\log(truth+1) - \\log(guess+1))^2$), and provides both a detailed item-by-item report and a summary visualization. This structured approach is vital for rigorous model assessment in any data science project.\n",
    "- **Visual Performance Feedback:** The evaluation includes a scatter plot comparing the model's predicted prices against the actual prices. A diagonal line represents perfect accuracy, and data points are color-coded (green, yellow, red) based on the prediction error, allowing for quick visual assessment of model performance across the price range.\n",
    "- **Error-Based Color Coding:** Predictions are color-coded in the output: green if the guess is within $40 or 20% of the true price, yellow for moderate errors, and red for significant deviations. This offers an intuitive way to quickly gauge prediction quality on individual items.\n",
    "- **\"Comedy Model 1\" - Random Guessing:** The first, extremely basic baseline model implemented simply returns a random integer between 1 and 999 as the predicted price, irrespective of the item's details. This serves as a sanity check; any useful model should significantly outperform this.\n",
    "- **\"Comedy Model 2\" - Average Price Guessing:** The second simple baseline calculates the average price of all items in the training dataset and uses this single average value as the prediction for every test item. This model utilizes minimal information from the data and provides another low bar for comparison.\n",
    "- **Data Encapsulation via `Item` Class:** The continued use of an `Item` class (from prior sessions) is evident, which encapsulates product attributes and methods to generate prompts or extract prices. This emphasizes good software engineering practice in data science for managing complex data.\n",
    "- **Relevance of Data Handling Skills:** The instructor stresses that understanding data structures (like the `Item` class) and data \"massaging\" (preparation and feature engineering) are crucial real-world skills for data scientists.\n",
    "- **Squared Log Error Metric:** The session introduces the squared log error, calculated for each prediction as $(\\log(y_{true} + 1) - \\log(y_{pred} + 1))^2$. This metric is useful for regression tasks where relative error is important and target values can be zero.\n",
    "\n",
    "---\n",
    "### Conceptual Understanding\n",
    "- **The Role of a Test Harness (like the `Tester` class)**\n",
    "    1.  **Why is this concept important?** A test harness standardizes model evaluation, ensuring different models are compared fairly using the same data, metrics, and procedures. It automates repetitive testing tasks, which saves significant time, reduces errors, and facilitates systematic performance tracking during model development.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** In any machine learning project, whether it's predicting stock prices, diagnosing diseases, or recommending products, a robust testing framework is essential for iterating on models, verifying improvements, ensuring reliability, and selecting the best model for deployment.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Unit testing, integration testing in software development, cross-validation strategies for more robust model generalization estimates, A/B testing for comparing models in live environments, and MLOps principles for managing the ML lifecycle.\n",
    "\n",
    "- **Squared Logarithmic Error (and its variants like RMSLE)**\n",
    "    1.  **Why is this concept important?** The squared logarithmic error (SLE) is $(\\log(y_{true} + 1) - \\log(y_{pred} + 1))^2$. Its mean (MSLE) or the root of its mean (RMSLE) is often used as a regression metric. It's useful because it measures the relative error (i.e., the ratio between predicted and actual values) and penalizes underestimations more heavily than overestimations. The `+1` term allows the metric to handle actual or predicted values of 0.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** It's commonly used in competitions and real-world regression tasks where the target variable spans a large range of positive values (e.g., product prices, sales counts, income levels) and where the percentage error is more indicative of performance than absolute error.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Other regression evaluation metrics like Mean Absolute Error (MAE), Mean Squared Error (MSE), R-squared ($R^2$); understanding the impact of log transformations on data distributions; and scenarios where percentage errors are preferred over absolute errors.\n",
    "\n",
    "---\n",
    "### Code Examples\n",
    "The transcript describes the following Python code elements and their usage:\n",
    "\n",
    "1.  **Key Imports for Machine Learning and NLP:**\n",
    "    ```python\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.svm import SVR\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    import gensim # For Word2Vec\n",
    "    import random # For the random guess model\n",
    "    # matplotlib.pyplot for plotting (implicitly used by Tester class)\n",
    "    ```\n",
    "\n",
    "2.  **Constants for Colored Output:**\n",
    "    ```python\n",
    "    GREEN = '\\033[92m'\n",
    "    RESET = '\\033[0m'\n",
    "    # Other colors like YELLOW and RED would be similarly defined\n",
    "    ```\n",
    "\n",
    "3.  **Structure of the `Tester` Class (Conceptual):**\n",
    "    ```python\n",
    "    class Tester:\n",
    "        def __init__(self):\n",
    "            # Initialize colors, etc.\n",
    "            pass\n",
    "\n",
    "        def run_data_point(self, prediction_function, data_point):\n",
    "            # Calls prediction_function(data_point) to get guess\n",
    "            guess = prediction_function(data_point)\n",
    "            truth = data_point.price # Assuming 'price' attribute exists\n",
    "            error = abs(guess - truth)\n",
    "            # Calculate squared_log_error = (np.log(truth + 1) - np.log(guess + 1))**2\n",
    "            # Print formatted output with colors based on error\n",
    "            # Store results for plotting\n",
    "            pass\n",
    "\n",
    "        def test(self, prediction_function, test_data_list, num_samples=250):\n",
    "            # Initialize overall metrics (e.g., list of errors)\n",
    "            print(f\"Testing model: {prediction_function.__name__}\")\n",
    "            for item in test_data_list[:num_samples]:\n",
    "                self.run_data_point(prediction_function, item)\n",
    "            # Generate and show summary plot (guess vs. truth)\n",
    "            # Print overall summary statistics\n",
    "            pass\n",
    "    ```\n",
    "\n",
    "4.  **Random Guess Model Function:**\n",
    "    ```python\n",
    "    def random_guess_model(item): # 'item' is an instance of the Item class\n",
    "        \"\"\"Ignores the item and returns a random price.\"\"\"\n",
    "        random.seed(42) # For reproducibility, as mentioned for the test run\n",
    "        return random.randint(1, 999) # Range mentioned in transcript\n",
    "    ```\n",
    "\n",
    "5.  **Average Guess Model Function (Conceptual):**\n",
    "    ```python\n",
    "    # First, calculate the average price from the training data\n",
    "    # train_items would be a list of Item objects\n",
    "    # avg_price = sum(item.price for item in train_items) / len(train_items)\n",
    "\n",
    "    def average_guess_model(item): # 'item' is an instance of the Item class\n",
    "        \"\"\"Ignores the item and returns the pre-calculated average price.\"\"\"\n",
    "        # In a real implementation, avg_price would be accessible here,\n",
    "        # possibly as a global variable or passed via a class.\n",
    "        return avg_price # Where avg_price is the mean price from the training set\n",
    "    ```\n",
    "\n",
    "6.  **Using the `Tester` Class:**\n",
    "    ```python\n",
    "    # Assuming 'tester' is an instance of the Tester class\n",
    "    # and 'test_items' is the list of test data points\n",
    "    \n",
    "    # tester = Tester() # Instantiate the tester\n",
    "    # tester.test(random_guess_model, test_items)\n",
    "    # tester.test(average_guess_model, test_items)\n",
    "    ```\n",
    "\n",
    "---\n",
    "### Reflective Questions\n",
    "1.  **Application:** Which specific dataset or project from your experience could have benefited from a standardized `Tester` class like the one described for evaluating models? Provide a one-sentence explanation.\n",
    "    * *Answer:* A past project on predicting house prices using various feature sets and regression algorithms could have significantly benefited from such a `Tester` class to ensure all models were evaluated consistently with metrics like RMSLE and to easily visualize error patterns.\n",
    "2.  **Teaching:** How would you explain the primary advantage of using Root Mean Squared Log Error (RMSLE) or the described squared log error over Mean Absolute Error (MAE) for a price prediction task with a wide price range (e.g., $10 to $10,000) to a junior colleague? Keep the answer under two sentences.\n",
    "    * *Answer:* RMSLE focuses on the relative (percentage) difference, so being $50 off on a $100 item is penalized much more than being $50 off on a $5000 item; MAE would treat both $50 errors equally, which might not reflect business impact accurately for wide-ranging prices.\n",
    "3.  **Extension:** If the initial \"comedy models\" (random and average guess) demonstrate very poor performance for price prediction, what is the immediate next, slightly more sophisticated, yet still simple, baseline model you would implement leveraging the product's textual description, and why?\n",
    "    * *Answer:* The next logical baseline would be a Linear Regression model using TF-IDF weighted Bag-of-Words features extracted from the product titles or descriptions, because this directly incorporates textual information from the product and often establishes a strong initial performance benchmark for text-based regression tasks.\n",
    "\n",
    "# Day 3: Feature Engineering Techniques for Amazon Product Price Prediction Models\n",
    "\n",
    "### Summary\n",
    "This segment introduces feature engineering as a critical, albeit sometimes \"grotty,\" aspect of traditional machine learning, contrasting it with the automated feature discovery capabilities of modern deep neural networks. The speaker demonstrates practical steps using an Amazon product dataset, starting with parsing product 'details' from JSON strings into Python dictionaries and then identifying potentially useful features like 'Item Weight' by analyzing their prevalence with `collections.Counter`. A significant portion is dedicated to the challenges of \"dirty data,\" exemplified by normalizing inconsistent 'Item Weight' units into pounds and then applying mean imputation to handle missing weight values, ensuring the data is suitable for regression models.\n",
    "\n",
    "### Highlights\n",
    "-   **Rationale for Feature Engineering:** Feature engineering is presented as a fundamental technique in traditional machine learning used to manually extract and create informative signals from raw data. This was the standard approach to improving model predictions before the advent of deep learning models that can learn features automatically, though it remains relevant.\n",
    "-   **Parsing JSON Data:** Product details stored as JSON strings within the dataset are converted into Python dictionaries using `json.loads()`. This transformation is crucial for accessing and utilizing the nested information as individual features for the model. This is a common task when dealing with semi-structured data from APIs or databases.\n",
    "-   **Identifying Candidate Features with `collections.Counter`:** To understand feature availability and consistency across different products, `collections.Counter` is used on the keys of the parsed dictionaries. This helps identify the most common features (e.g., 'Item Weight', 'Brand', 'Best Sellers Rank'), guiding the selection of those likely to be reliable inputs for a model.\n",
    "-   **Criteria for Feature Selection:** Good candidate features are characterized as being (1) well-populated across the dataset, (2) consistently present, and (3) intuitively or demonstrably related to the target variable (e.g., product price). This initial screening is vital for focusing engineering efforts.\n",
    "-   **Addressing \"Dirty Data\" - Weight Normalization:** The 'Item Weight' feature illustrates a common data quality issue, with weights recorded in various units (pounds, ounces, kilograms, etc.). A custom function is implemented to parse these varied entries and normalize them into a consistent unit (pounds), highlighting the meticulous data cleaning often required.\n",
    "-   **Calculating a Global Average for Imputation:** The average weight (e.g., 13.6 pounds) is calculated from all valid weight entries in the training dataset. This average serves as a fallback value for imputation, a key step before actually filling in missing data.\n",
    "-   **Mean Imputation for Missing Weights:** For products where weight is missing, zero, or in an unrecognized format after normalization, the pre-calculated average weight is substituted. This technique ensures that the machine learning model receives a complete dataset for the weight feature, a common requirement for many algorithms.\n",
    "-   **Iterative and Detailed Nature of Feature Engineering:** The process of cleaning data, normalizing units, and deciding on imputation strategies is described as \"grotty work\" and \"hokey,\" underscoring the detailed, sometimes imperfect, and iterative effort involved in crafting useful features.\n",
    "\n",
    "### Code Examples\n",
    "The following conceptual Python snippets illustrate the operations described in the text:\n",
    "\n",
    "```python\n",
    "# 1. Parsing JSON strings from a dataset field\n",
    "import json\n",
    "# Assuming 'item' is a data record and item['details'] is a JSON string\n",
    "# details_string = item['details'] # Example: '{\"Item Weight\": \"5 pounds\", \"Manufacturer\": \"SomeBrand\"}'\n",
    "# product_features_dict = json.loads(details_string)\n",
    "\n",
    "# 2. Counting feature occurrences using collections.Counter\n",
    "from collections import Counter\n",
    "# Assuming 'all_feature_keys_from_dataset' is a list of all keys found in all product dictionaries\n",
    "# feature_counts = Counter(all_feature_keys_from_dataset)\n",
    "# most_common_features = feature_counts.most_common(40) # To get the top 40 features\n",
    "\n",
    "# 3. Conceptual structure for a weight normalization function (as described)\n",
    "def normalize_item_weight(raw_weight_value):\n",
    "    \"\"\"\n",
    "    Converts a raw weight string (e.g., \"2.5 lbs\", \"500 grams\") to pounds.\n",
    "    This function would contain complex logic to parse units and values.\n",
    "    \"\"\"\n",
    "    weight_str = str(raw_weight_value).lower()\n",
    "    numeric_value = None\n",
    "    # Example extraction (actual implementation would be more robust)\n",
    "    import re\n",
    "    match = re.search(r\"([\\d.]+)\", weight_str)\n",
    "    if match:\n",
    "        numeric_value = float(match.group(1))\n",
    "\n",
    "    if numeric_value is None:\n",
    "        return None\n",
    "\n",
    "    if \"pounds\" in weight_str or \"lb\" in weight_str:\n",
    "        return numeric_value\n",
    "    elif \"ounces\" in weight_str or \"oz\" in weight_str:\n",
    "        return numeric_value / 16.0\n",
    "    elif \"kilograms\" in weight_str or \"kg\" in weight_str:\n",
    "        return numeric_value * 2.20462\n",
    "    elif \"grams\" in weight_str or \"g\" in weight_str: # Assuming 'g' without 'k' is grams\n",
    "        return (numeric_value / 1000.0) * 2.20462\n",
    "    # ... add more unit conversions as needed\n",
    "    else:\n",
    "        return None # Unit not recognized\n",
    "\n",
    "# 4. Conceptual structure for get_weight_with_default function (as described)\n",
    "# AVERAGE_WEIGHT_POUNDS = 13.6 # Calculated from the training dataset\n",
    "def get_weight_with_default(item_features_dict, average_weight):\n",
    "    \"\"\"\n",
    "    Retrieves normalized weight; if missing, invalid, or zero, returns a default average weight.\n",
    "    \"\"\"\n",
    "    raw_weight = item_features_dict.get(\"Item Weight\") # Key might vary\n",
    "    \n",
    "    if raw_weight is None:\n",
    "        return average_weight\n",
    "        \n",
    "    normalized_weight = normalize_item_weight(raw_weight)\n",
    "    \n",
    "    if normalized_weight is None or normalized_weight <= 0:\n",
    "        return average_weight\n",
    "    else:\n",
    "        return normalized_weight\n",
    "\n",
    "```\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Importance of Data Cleaning and Standardization (e.g., Weight Normalization)**\n",
    "    1.  **Why is this concept important?** Raw datasets frequently contain \"dirty\" data—errors, inconsistencies in format or units, or irrelevant information. Cleaning and standardizing data, such as converting all 'Item Weight' entries to a single unit like pounds, is crucial because it ensures feature values are comparable, accurate, and correctly interpreted by the machine learning model, preventing skewed analyses or poor model performance.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** This is fundamental in virtually all data-driven tasks. For example, in analyzing international sales data, all monetary values must be converted to a common currency. In healthcare, patient measurements (height, weight, temperature) from different systems might need unit normalization before being used in diagnostic models.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Exploratory Data Analysis (EDA) to identify inconsistencies, regular expressions (regex) for parsing and cleaning text-based unit information, outlier detection and handling, data type validation and casting, and creating data dictionaries or schemas to enforce consistency.\n",
    "\n",
    "-   **Mean Imputation for Missing Values**\n",
    "    1.  **Why is this concept important?** Many machine learning algorithms cannot process datasets with missing values. Mean imputation offers a straightforward method to fill these gaps by replacing missing entries in a numerical feature with the mean of that feature calculated from the available data. This allows for the inclusion of otherwise incomplete data records in the analysis.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** It's often applied in survey data where respondents might skip questions, in sensor data where readings might occasionally fail, or in any dataset where data collection is imperfect. For example, if 10% of products are missing weight information, mean imputation allows these products to still be included in a price prediction model.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Other imputation methods like median imputation (more robust to outliers for skewed data), mode imputation (for categorical features), regression imputation (predicting missing values based on other features), K-Nearest Neighbors (KNN) imputation, and more advanced techniques like multiple imputation. It's also important to understand the potential biases introduced by imputation, such as reduction of variance and distortion of covariance. Creating a binary indicator feature for whether a value was imputed can sometimes be beneficial.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** How could the described weight normalization and imputation strategy be applied to a dataset of international shipping logistics?\n",
    "    -   *Answer:* In international shipping, package weights and dimensions might be reported in various metric and imperial units (kilograms, pounds, meters, feet). Normalizing all weights to kilograms and all dimensions to meters, then using mean imputation (perhaps stratified by shipping lane or package type) for missing values, would ensure data consistency for tasks like cost prediction or load optimization.\n",
    "\n",
    "2.  **Teaching:** How would you explain to a non-technical stakeholder why parsing the 'details' JSON string is a necessary first step before using product weight for price prediction?\n",
    "    -   *Answer:* \"Think of the 'details' as a sealed envelope containing a list of product facts, including its weight. Before we can use the weight to help predict price, we first need to open that envelope (parse the JSON) and properly read and isolate the specific weight information, separating it from other facts, so our system can understand and use it correctly.\"\n",
    "\n",
    "3.  **Extension:** The speaker uses mean imputation for missing weights. What is a potential drawback of this method, and what alternative could be considered if 'Item Weight' is expected to vary significantly based on 'Product Category' (another available feature)?\n",
    "    -   *Answer:* A drawback of global mean imputation is that it ignores potential relationships between variables and can be inaccurate if the data has distinct subgroups; for example, the average weight of electronics is very different from furniture. If 'Product Category' is available, a more nuanced alternative would be conditional mean imputation, where missing weights are replaced by the mean weight of items within the same specific 'Product Category', leading to more contextually relevant imputed values.\n",
    "\n",
    "# Day 3 - Optimizing LLM Performance: Advanced Feature Engineering Strategies\n",
    "\n",
    "### Summary\n",
    "This text details the \"hacky\" yet crucial process of feature engineering in traditional data science, using examples like calculating average bestseller ranks, imputing missing values, and deriving features from text length and product brands for an e-commerce dataset. It contrasts this manual, domain-expertise-driven approach with modern deep learning, where models can often automatically discern relevant features, thereby reducing the dependency on specific domain knowledge for data scientists but emphasizing expertise in model building. The ultimate goal is to create a robust set of features, like those consolidated in a `get_features` function, to feed into a machine learning model.\n",
    "\n",
    "### Highlights\n",
    "-   **Best Seller Rank Aggregation:** When products have multiple bestseller ranks across different lists (e.g., on Amazon), a practical approach is to average these ranks. This method, while \"rough and ready,\" provides a single representative rank; for instance, ranks of 1 and 10,000 would average to around 5,000. This is relevant for standardizing ranking information from diverse categorical listings in e-commerce platforms.\n",
    "-   **Imputation of Missing Ranks:** For items lacking a bestseller rank, the average of all available average ranks from the training dataset is used as a default. This technique ensures completeness in the feature set, a common challenge in real-world data where missing values can hinder model training.\n",
    "-   **Text Length as a Feature:** The length of a product's text description is incorporated as a feature, prompted by an earlier visual analysis suggesting a correlation between text amount and price. In data science, simple metadata like text length can serve as a proxy for content detail or product type, potentially influencing target variables.\n",
    "-   **Brand Categorization Feature:** Features are engineered based on product brands, such as identifying \"top electronics brands\" (e.g., HP, Dell, Samsung). This involves creating binary indicators (1 or 0) if a product belongs to a predefined list, a technique useful for capturing brand-driven effects in market analysis or sales prediction.\n",
    "-   **Embracing Feature Proliferation:** The speaker encourages creating numerous features (e.g., minimum rank, maximum rank, various brand categories), as regression models can later help sift through them to identify those with actual predictive signal. This iterative approach is common in exploring complex datasets to maximize information extraction.\n",
    "-   **Domain Expertise in Traditional Feature Engineering:** Crafting effective features often requires substantial domain knowledge (e.g., understanding car brands or electronics). This expertise allows data scientists to intuit which aspects of the data are most likely to be informative for the model.\n",
    "-   **Shift with Modern Deep Learning:** Unlike traditional methods, modern deep learning models (especially large neural networks) can often automatically learn relevant features from raw data, reducing the critical need for deep, pre-existing domain expertise by the data scientist. The expertise shifts towards model architecture and training.\n",
    "-   **Iterative and Experimental Process:** Feature engineering is characterized as an iterative process involving guesswork, trial, and error. This experimentation is fundamental to both traditional and modern data science for optimizing model inputs.\n",
    "-   **Consolidated Feature Set:** The process culminates in a function (e.g., `get_features`) that compiles all engineered features (weight, rank, text length, brand category) into a structured format, like a dictionary, for each item. This structured data is then ready for input into a machine learning model.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Domain Expertise in Traditional Feature Engineering**\n",
    "    1.  **Why is this concept important?** In traditional machine learning, the model's ability to learn is heavily dependent on the quality and relevance of the input features. Domain expertise guides the creation of these features, transforming raw data into representations that are meaningful for the problem at hand, thereby directly impacting model performance and interpretability.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** In fields like credit risk assessment, a domain expert might know that certain transaction patterns or types of expenditures are highly indicative of default risk, leading to the creation of features that a non-expert might overlook. Similarly, in medical diagnostics, knowledge of physiology and disease progression is vital for engineering features from patient data.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Exploratory Data Analysis (EDA) is crucial for understanding the data and inspiring feature ideas. Collaboration with Subject Matter Experts (SMEs) is also key. Statistical testing and feature selection techniques (e.g., chi-squared tests for categorical features, ANOVA for numerical features against a categorical target) help validate the relevance of engineered features.\n",
    "\n",
    "-   **Automated Feature Learning in Deep Learning**\n",
    "    1.  **Why is this concept important?** Deep learning models, particularly those with many layers (like CNNs or Transformers), can automatically learn hierarchical representations and features from raw data. This reduces the manual effort and potential bias of human feature engineering, and can often uncover more complex and effective features than humans can devise, especially for high-dimensional data like images or text.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** In image recognition, CNNs automatically learn to detect edges, textures, and eventually complex objects without being explicitly told what to look for. In Natural Language Processing (NLP), models like BERT or GPT learn meaningful word and sentence embeddings that capture semantic relationships, directly from large text corpora.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Representation Learning, Transfer Learning (using pre-trained models that have already learned powerful features), various neural network architectures (Convolutional Neural Networks, Recurrent Neural Networks, Transformers, Autoencoders), and dimensionality reduction techniques. Understanding backpropagation and gradient descent is also fundamental to how these models learn features.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** Which specific dataset or project could benefit from the described feature engineering techniques for brands and ranks?\n",
    "    -   *Answer:* These techniques would be highly beneficial for a project aiming to predict product sales or customer ratings on an e-commerce platform. Engineering features like average sales rank can quantify a product's popularity, while categorizing brands (e.g., \"luxury,\" \"mass-market,\" \"emerging\") can help the model capture brand equity effects on performance.\n",
    "\n",
    "2.  **Teaching:** How would you explain the benefit of adding text length as a feature to a junior colleague, using one concrete example?\n",
    "    -   *Answer:* \"Imagine we're trying to predict the price of online course listings. A course with a very long description (high text length) might be a comprehensive, multi-module program, thus likely more expensive, whereas a short description might point to a brief introductory workshop; this simple text length feature can give our model an initial hint about the product's depth or value.\"\n",
    "\n",
    "3.  **Extension:** The speaker mentions regression models will decide which features give signal. What related technique or area should you explore next to manage a large number of engineered features effectively, and why?\n",
    "    -   *Answer:* After generating many potential features, one should explore feature selection and dimensionality reduction techniques. Methods like L1 (Lasso) regularization, principal component analysis (PCA), or recursive feature elimination help identify the most impactful features, reduce model complexity, prevent overfitting, and can improve training speed and interpretability.\n",
    "\n",
    "# Day 3 - Linear Regression for LLM Fine-Tuning: Baseline Model Comparison\n",
    "\n",
    "### Summary\n",
    "This session transitions from feature engineering to applying a traditional linear regression model for product price prediction using the four previously crafted features: weight, rank, text length, and 'is_top_electronics_brand'. After fitting the model, an analysis of feature coefficients reveals their respective impacts, with 'is_top_electronics_brand' showing a significant positive influence. Ultimately, the linear regression model achieves only a marginal improvement ($139 average error) over a simple average price baseline ($145 error), underscoring the limitations of basic models with limited features and inviting further feature engineering or more advanced modeling techniques.\n",
    "\n",
    "### Highlights\n",
    "-   **Model Training with Engineered Features:** A `sklearn.linear_model.LinearRegression` model is trained using a Pandas DataFrame containing the four engineered features (weight, rank, text length, is_top_electronics_brand) to predict product prices. This step directly applies the created features to a standard machine learning algorithm.\n",
    "-   **Interpreting Feature Coefficients:** The trained model assigns coefficients to each feature, indicating their learned linear relationship with the price. For example, 'is_top_electronics_brand' received a large positive coefficient, suggesting it significantly increases the predicted price (by about $200), while 'text_length' had a very small coefficient, indicating minimal linear impact.\n",
    "-   **Modest Performance Improvement:** The linear regression model yielded an average prediction error of $139, only a slight improvement over the $145-$146 error from a naive average price baseline. This demonstrates that the initial set of engineered features, while helpful, did not enable the simple linear model to capture the full complexity of price determination.\n",
    "-   **Impact of 'is_top_electronics_brand' Feature:** The binary feature indicating if a product is a top electronics brand had a noticeable effect, providing an uplift in predicted price for those items. This highlights how well-defined categorical features can contribute clear, interpretable signals to a linear model.\n",
    "-   **Quantitative Model Evaluation:** The model's effectiveness was quantitatively assessed using metrics such as Mean Squared Error (MSE) and R-squared, alongside visual inspection of prediction accuracy (achieving \"green\" or accurate predictions about 16% of the time). This is standard practice for evaluating and comparing regression model performance.\n",
    "\n",
    "### Code Examples\n",
    "The following conceptual Python snippets illustrate the core machine learning steps described:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Assume get_features(item) is the function created previously\n",
    "# Assume train_items is a list of items for training\n",
    "# Assume test_items is a list of items for testing\n",
    "\n",
    "# 1. Prepare DataFrames\n",
    "# def items_to_dataframe(items_list):\n",
    "#     features_list = [get_features(item) for item in items_list]\n",
    "#     df = pd.DataFrame(features_list)\n",
    "#     # Ensure all feature columns are present, fill NaNs if any (e.g., with 0 or mean)\n",
    "#     # df['price'] = [item['price'] for item in items_list] # Assuming price is available\n",
    "#     return df\n",
    "\n",
    "# train_df = items_to_dataframe(train_items)\n",
    "# test_df = items_to_dataframe(test_items[:250]) # Using a subset of test data\n",
    "\n",
    "# feature_columns = ['weight', 'rank', 'text_length', 'is_top_electronics_brand']\n",
    "# X_train = train_df[feature_columns]\n",
    "# y_train = train_df['price'] # Assuming 'price' column exists from item data\n",
    "\n",
    "# X_test = test_df[feature_columns]\n",
    "# y_test = test_df['price'] # Assuming 'price' column exists\n",
    "\n",
    "# 2. Train Linear Regression Model\n",
    "# model = LinearRegression()\n",
    "# model.fit(X_train, y_train)\n",
    "\n",
    "# 3. Inspect Coefficients\n",
    "# coefficients = pd.Series(model.coef_, index=feature_columns)\n",
    "# print(\"Feature Coefficients:\")\n",
    "# print(coefficients)\n",
    "# print(f\"Intercept: {model.intercept_}\")\n",
    "\n",
    "# 4. Make Predictions\n",
    "# test_predictions = model.predict(X_test)\n",
    "\n",
    "# 5. Evaluate (conceptual, actual metrics like MSE, R2 would be calculated using sklearn.metrics)\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# mse = mean_squared_error(y_test, test_predictions)\n",
    "# r_squared = r2_score(y_test, test_predictions)\n",
    "# print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "# print(f\"R-squared (R2): {r_squared}\")\n",
    "\n",
    "# 6. Wrapper for visualizer\n",
    "# def linear_regression_pricer(item):\n",
    "#     item_features_dict = get_features(item)\n",
    "#     item_df = pd.DataFrame([item_features_dict])[feature_columns] # Ensure correct column order\n",
    "#     return model.predict(item_df)[0]\n",
    "\n",
    "```\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Linear Regression Coefficients**\n",
    "    1.  **Why is this concept important?** Coefficients in a linear regression model represent the estimated average change in the target variable (e.g., price) for a one-unit increase in a specific feature, assuming all other features remain constant. They offer insights into the direction (positive or negative) and strength of the linear relationships the model has identified between each feature and the target.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** In economics, coefficients can estimate the impact of interest rates on housing demand. In marketing, they can show how much ad spend on different channels contributes to sales. In this context, they show how features like 'weight' or 'is_top_electronics_brand' are associated with price changes.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Understanding p-values for coefficient significance, confidence intervals for coefficients, the impact of feature scaling on coefficient magnitudes (though not on their interpretation for unscaled features in terms of direct price impact per unit), and the problem of multicollinearity, where high correlation between features can make individual coefficients unstable and hard to interpret.\n",
    "\n",
    "-   **R-squared ($R^2$)**\n",
    "    1.  **Why is this concept important?** R-squared, the coefficient of determination, indicates the proportion of the variance in the dependent variable (e.g., product price) that can be explained by the independent variables (the engineered features) included in the model. It provides a measure of the model's goodness-of-fit, ranging from 0 (no variance explained) to 1 (all variance explained).\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** It's widely used to assess how well a regression model accounts for the observed outcomes. For instance, if a price prediction model has an $R^2$ of 0.20, it means that 20% of the variation in product prices is explained by the features in the model, while 80% is due to other factors or random error.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Adjusted R-squared (which penalizes the addition of irrelevant predictors, making it more suitable for comparing models with different numbers of features), Mean Squared Error (MSE) or Root Mean Squared Error (RMSE) (which provide the magnitude of average prediction error in the target variable's units), and F-statistic (which tests the overall significance of the regression model).\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** How could you use the model's feature coefficients (e.g., 'text_length' having a very small coefficient) to guide your next steps in feature engineering for this price prediction task?\n",
    "    -   *Answer:* A very small coefficient for 'text_length' suggests it currently has little linear influence on the predicted price. To improve its utility, one could explore non-linear transformations (e.g., log of text length, binning into categories like 'short', 'medium', 'long') or create interaction features (e.g., 'text_length' multiplied by 'is_top_electronics_brand') to capture more complex relationships.\n",
    "\n",
    "2.  **Teaching:** How would you briefly explain R-squared to a stakeholder who asks if the $139 error (Mean Absolute Error) for the price prediction model is \"good\"?\n",
    "    -   *Answer:* \"The $139 error tells us our model's average prediction mistake in dollar terms for each product. R-squared, on the other hand, tells us what percentage of the overall price differences we see in the market our model can actually explain using its current features; a low R-squared, even with a moderate error, suggests our model might be missing key factors driving prices.\"\n",
    "\n",
    "# Day 3 - Bag of Words NLP: Implementing Count Vectorizer for Text Analysis in ML\n",
    "\n",
    "### Summary\n",
    "This section details the shift from manual feature engineering to Natural Language Processing (NLP) techniques for predicting product prices, starting with a simple Bag-of-Words (BoW) model using `CountVectorizer`. This BoW approach, despite its simplicity, surprisingly outperformed the previous feature-engineered linear regression model, achieving a lower average prediction error. The discussion then moves to a more sophisticated method, Word2Vec from `gensim`, to create dense word embeddings, yet when paired with linear regression, it did not yield further improvement, suggesting the linear model might not fully leverage the richer embeddings.\n",
    "\n",
    "### Highlights\n",
    "-   **Transition to NLP for Feature Generation:** The focus shifts from creating features manually to utilizing the text content of product descriptions directly as input for the prediction model. This involves processing lists of text \"documents\" and their corresponding \"prices.\"\n",
    "-   **Strategic Data Input to Prevent Leakage:** A crucial step is using \"test prompts\" (product descriptions without the price) from the training data to build the NLP models. This prevents data leakage, where the model might learn to simply extract the price from the input text if the training prompt (which includes the price) were used.\n",
    "-   **Bag-of-Words (BoW) Model Implementation:** `CountVectorizer` from scikit-learn is used to implement a BoW model. It converts text documents into numerical vectors based on word counts, limited to the top 1000 words and excluding common English stop words. This method inherently ignores word order.\n",
    "-   **Improved Performance with BoW and Linear Regression:** A linear regression model trained with BoW features achieved an average prediction error of $113. This was a significant improvement over the $139 error from the linear regression model using manually engineered features.\n",
    "-   **Introduction to Word2Vec for Dense Embeddings:** Word2Vec (via `gensim`) is introduced as a more advanced technique to create dense vector representations (embeddings) of words, in this case, 400-dimensional vectors. Unlike BoW, Word2Vec aims to capture semantic relationships between words.\n",
    "-   **Document Vector Creation from Word2Vec:** To apply Word2Vec embeddings (which are per-word) to a document-level task like price prediction with linear regression, an aggregation step (typically averaging the vectors of words in a document) is implicitly required to create a single vector per document.\n",
    "-   **Word2Vec with Linear Regression Shows No Further Gain:** Counterintuitively, the linear regression model using Word2Vec-derived document vectors performed slightly worse or similarly to the BoW approach. This suggests that the linear model might not be complex enough to take full advantage of the richer, denser information provided by Word2Vec embeddings.\n",
    "-   **Iterative Approach to Baseline Modeling:** The progression from manual features to BoW, then to Word2Vec, all paired with linear regression, illustrates an iterative strategy to build increasingly sophisticated baseline models. The goal is to establish strong traditional ML benchmarks before moving to more complex models like Large Language Models (LLMs).\n",
    "\n",
    "### Code Examples\n",
    "The following conceptual Python snippets illustrate the NLP techniques described:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np # For averaging Word2Vec vectors\n",
    "\n",
    "# Assume 'documents_raw_text' is a list of product text descriptions (test prompts from training data)\n",
    "# Assume 'prices_target' is a list of corresponding product prices\n",
    "\n",
    "# --- 1. Bag-of-Words (BoW) with CountVectorizer ---\n",
    "# vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "# X_bow_features = vectorizer.fit_transform(documents_raw_text)\n",
    "\n",
    "# bow_linear_model = LinearRegression()\n",
    "# bow_linear_model.fit(X_bow_features, prices_target)\n",
    "\n",
    "# For prediction on a new item's text:\n",
    "# new_item_text_list = [\"example new product description\"]\n",
    "# new_item_bow_features = vectorizer.transform(new_item_text_list)\n",
    "# predicted_price_bow = bow_linear_model.predict(new_item_bow_features)\n",
    "\n",
    "\n",
    "# --- 2. Word2Vec with Gensim ---\n",
    "# Preprocessing: Word2Vec typically expects a list of lists of tokens.\n",
    "# tokenized_documents = [doc.lower().split() for doc in documents_raw_text]\n",
    "\n",
    "# Train Word2Vec model (or load a pre-trained one)\n",
    "# Parameters mentioned: vector_size=400, workers=8. Other common params: window, min_count.\n",
    "# w2v_model = Word2Vec(sentences=tokenized_documents, vector_size=400, window=5, min_count=1, workers=8)\n",
    "# Note: Actual training involves w2v_model.build_vocab(tokenized_documents) and w2v_model.train(...)\n",
    "\n",
    "# Function to create document vectors by averaging word vectors\n",
    "# def get_document_vector(doc_tokens, w2v_model_instance):\n",
    "#     valid_word_vectors = [w2v_model_instance.wv[word] for word in doc_tokens if word in w2v_model_instance.wv]\n",
    "#     if not valid_word_vectors:\n",
    "#         return np.zeros(w2v_model_instance.vector_size) # Return zero vector if no words in vocab\n",
    "#     return np.mean(valid_word_vectors, axis=0)\n",
    "\n",
    "# Create document vectors for the dataset\n",
    "# X_w2v_features = np.array([get_document_vector(tokens, w2v_model) for tokens in tokenized_documents])\n",
    "\n",
    "# w2v_linear_model = LinearRegression()\n",
    "# w2v_linear_model.fit(X_w2v_features, prices_target)\n",
    "\n",
    "# For prediction on a new item's text:\n",
    "# new_item_tokens = new_item_text_list[0].lower().split()\n",
    "# new_item_w2v_vector = get_document_vector(new_item_tokens, w2v_model).reshape(1, -1) # Reshape for single sample\n",
    "# predicted_price_w2v = w2v_linear_model.predict(new_item_w2v_vector)\n",
    "\n",
    "```\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Bag-of-Words (BoW) Representation**\n",
    "    1.  **Why is this concept important?** BoW is a foundational technique in NLP that converts text into a numerical format digestible by machine learning algorithms. It creates a vocabulary from the corpus and represents each document as a vector of word frequencies, providing a simple yet often effective way to quantify text data.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** It's commonly used as a baseline for document classification (e.g., spam filtering, topic categorization) and sentiment analysis. Its main limitation is the disregard for word order and semantic context (e.g., \"not good\" might be treated similarly to \"good\" if individual word counts are the only focus without considering n-grams).\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** TF-IDF (Term Frequency-Inverse Document Frequency) for more nuanced word weighting, N-grams (sequences of N words) to capture some local word order, hashing vectorizers for memory-efficient vectorization of large vocabularies, and stemming/lemmatization for word normalization.\n",
    "\n",
    "-   **Word2Vec and Dense Embeddings**\n",
    "    1.  **Why is this concept important?** Word2Vec generates dense vector representations (embeddings) of words where the geometric relationships between vectors capture semantic similarities. For example, `vector('king') - vector('man') + vector('woman')` might be close to `vector('queen')`. This allows models to understand synonymy and analogy, which BoW cannot.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** These embeddings significantly improve performance in tasks requiring deeper language understanding, such as machine translation, question answering, text generation, and recommendation systems. They serve as powerful input features for various neural network architectures.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Other word embedding models like GloVe and FastText; contextual embeddings from models like ELMo, BERT, and other Transformers which provide different vectors for the same word depending on its context; methods for creating sentence or document embeddings from word embeddings (e.g., averaging, TF-IDF weighted averaging, Doc2Vec).\n",
    "\n",
    "-   **Preventing Data Leakage in NLP Tasks**\n",
    "    1.  **Why is this concept important?** Data leakage occurs when information that will not be available at prediction time is inadvertently used during model training, leading to overly optimistic performance metrics and poor real-world utility. In NLP for price prediction, if the training text contained the actual price, the model might learn to simply extract this string rather than learning underlying patterns from the descriptive text.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** This is a critical consideration across all machine learning domains. Examples include using future data in time-series forecasting, or having an ID field that accidentally correlates with the target in classification tasks. Vigilance is required during data preprocessing and feature engineering.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Rigorous separation of training, validation, and test datasets; careful feature engineering to ensure inputs only contain information that would genuinely be available at the point of prediction; understanding the temporal aspects of data if applicable; thorough review of data sources and preprocessing pipelines.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** If the simple Bag-of-Words model with linear regression provides a decent baseline for product price prediction from text, what specific product characteristics might necessitate moving to more complex embeddings like Word2Vec or Transformers?\n",
    "    -   *Answer:* One might need more complex embeddings if product prices are highly dependent on subtle nuances in descriptions, brand perception conveyed through specific phrasing, comparisons to other products mentioned implicitly, or if product categories are very diverse and require fine-grained semantic differentiation not captured by simple keyword presence.\n",
    "\n",
    "2.  **Teaching:** How would you explain to a junior data scientist why \"the order of words doesn't matter\" in a Bag-of-Words model, using an example from e-commerce product descriptions?\n",
    "    -   *Answer:* \"Imagine two product titles: 'Durable Red Bike for Kids' and 'Kids Bike Red Durable for'. In a Bag-of-Words model, both are treated almost identically because it's like dropping the words into a bag and just counting: one 'durable', one 'red', one 'bike', etc. The model loses the original sentence structure and focuses only on word frequencies.\"\n",
    "\n",
    "3.  **Extension:** The speaker hypothesized that linear regression might not be powerful enough to take advantage of Word2Vec features. What kind of model architecture would you explore next to better leverage these dense embeddings, and why?\n",
    "    -   *Answer:* A neural network, such as a simple feed-forward network (Multi-Layer Perceptron), would be a good next step. These models can learn complex, non-linear relationships from the dense Word2Vec embeddings, potentially capturing patterns that a linear model cannot. For even more sophistication, one might consider architectures like CNNs (for local patterns in word sequences if document vectors are formed by concatenating or stacking word vectors) or RNNs/LSTMs (if sequential information is preserved or document vectors are processed sequentially).\n",
    "\n",
    "# Day 3 - Support Vector Regression vs Random Forest: Machine Learning Face-Off\n",
    "\n",
    "### Summary\n",
    "This concluding session on traditional machine learning explores Support Vector Regression (SVR) and Random Forest regression for product price prediction, using text-derived vector features. While `LinearSVR` offers a slight improvement over previous models, achieving a $113 error, the `RandomForestRegressor` emerges as the top performer, significantly reducing the average error to $97. This highlights the power of ensemble methods and sets a strong benchmark before transitioning to Large Language Models (LLMs), with a final encouragement to experiment by combining engineered features with text vectors.\n",
    "\n",
    "### Highlights\n",
    "-   **Support Vector Regression (SVR) Performance:** A `LinearSVR` model, a computationally faster variant of SVR, was applied to the text-derived features, yielding an average prediction error of $113.0. This marked a marginal improvement over the Bag-of-Words linear regression model, demonstrating SVR's capability in handling high-dimensional text data.\n",
    "-   **Random Forest as a Powerful Ensemble:** `RandomForestRegressor` is introduced as an ensemble learning method that aggregates predictions from multiple decision trees trained on different subsets of data and features. This technique is noted for its strong performance and relatively few hyperparameters, making it user-friendly.\n",
    "-   **Random Forest Achieves Best Traditional Model Score:** The Random Forest model significantly outperformed all prior traditional models, achieving an average error of $97. This was the first model to break the $100 error threshold, establishing it as the leading baseline from traditional machine learning techniques in this experiment.\n",
    "-   **Combining Engineered and Learned Features:** A key suggestion for further improvement is to combine manually engineered features (e.g., product weight, brand category) with the text-derived vector representations (from BoW or Word2Vec) and use this richer feature set as input for models like Random Forest. This hybrid approach can often capture more signal.\n",
    "-   **Practicality of Model Training Time:** The choice of `LinearSVR` over SVR with more complex, time-consuming kernels (one reportedly \"ran all night\") underscores the importance of balancing model complexity and potential accuracy with practical constraints like training duration and computational resources.\n",
    "\n",
    "### Code Examples\n",
    "The following conceptual Python snippets illustrate the models discussed:\n",
    "\n",
    "```python\n",
    "from sklearn.svm import LinearSVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Assume X_train_vectors, y_train_prices are prepared (e.g., from BoW or Word2Vec on training data)\n",
    "# Assume X_test_vectors, y_test_prices are prepared (e.g., from BoW or Word2Vec on test data)\n",
    "\n",
    "# --- 1. Support Vector Regression (LinearSVR) ---\n",
    "# svr_model = LinearSVR(random_state=42, C=1.0, max_iter=1000) # Example parameters\n",
    "# svr_model.fit(X_train_vectors, y_train_prices)\n",
    "# svr_predictions = svr_model.predict(X_test_vectors)\n",
    "# # Evaluation (e.g., calculating Mean Absolute Error) would follow\n",
    "\n",
    "# --- 2. Random Forest Regressor ---\n",
    "# rf_model = RandomForestRegressor(n_estimators=100, random_state=42) # n_estimators is a key hyperparameter\n",
    "# rf_model.fit(X_train_vectors, y_train_prices)\n",
    "# rf_predictions = rf_model.predict(X_test_vectors)\n",
    "# # Evaluation (e.g., calculating Mean Absolute Error) would follow\n",
    "\n",
    "# --- 3. Conceptual: Combining Features (Example for Random Forest) ---\n",
    "# Assume X_train_engineered_features is a DataFrame of engineered features\n",
    "# Assume X_train_text_vectors is a NumPy array or sparse matrix from text vectorization\n",
    "\n",
    "# import numpy as np\n",
    "# from scipy.sparse import hstack # If text vectors are sparse\n",
    "\n",
    "# If X_train_text_vectors is dense (e.g., from Word2Vec averaging):\n",
    "# X_train_combined = np.concatenate((X_train_engineered_features.values, X_train_text_vectors), axis=1)\n",
    "# If X_train_text_vectors is sparse (e.g., from CountVectorizer):\n",
    "# X_train_combined = hstack((X_train_engineered_features.values, X_train_text_vectors))\n",
    "# Similar concatenation for test data\n",
    "\n",
    "# rf_model_combined = RandomForestRegressor(random_state=42)\n",
    "# rf_model_combined.fit(X_train_combined, y_train_prices)\n",
    "# rf_predictions_combined = rf_model_combined.predict(X_test_combined)\n",
    "```\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Support Vector Regression (SVR)**\n",
    "    1.  **Why is this concept important?** SVR adapts Support Vector Machine (SVM) principles for regression tasks. It aims to find a function (hyperplane in higher dimensions) that best fits the data by defining a margin (epsilon-insensitive tube) around the predicted values. Data points falling outside this tube contribute to the loss function, and the model tries to minimize this loss while also controlling complexity.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** SVR is used for tasks like financial forecasting, house price prediction, and other problems where the underlying relationships might be non-linear (when using non-linear kernels like RBF or polynomial). `LinearSVR` is a faster alternative when a linear relationship is assumed or as a computationally cheaper baseline.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Kernel functions (linear, RBF, polynomial, sigmoid) that allow SVR to model non-linearities, hyperparameters like `C` (regularization parameter balancing margin violations against model simplicity) and `epsilon` (width of the insensitive tube), and the concept of support vectors (data points influencing the model's fit).\n",
    "\n",
    "-   **Random Forest (Ensemble Learning)**\n",
    "    1.  **Why is this concept important?** Random Forests are a type of ensemble learning method that constructs multiple decision trees during training and outputs the average of their predictions (for regression) or the mode of their classes (for classification). By combining diverse trees trained on random subsets of data and features, they reduce variance, improve generalization, and are less prone to overfitting than individual decision trees.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** They are widely used and effective for a broad range of problems, including bioinformatics, fraud detection, predicting stock prices, and image analysis. They can also provide useful estimates of feature importance.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Decision Trees (the base learners), Bagging (Bootstrap Aggregating, the core idea behind Random Forests), Feature Randomness (selecting a random subset of features for splitting at each node), Out-of-Bag (OOB) error estimation, and other ensemble methods like Gradient Boosting Machines (e.g., XGBoost, LightGBM, CatBoost).\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** The speaker mentions Random Forests having few hyperparameters as an advantage. In a project with tight deadlines, why would this characteristic be particularly beneficial when choosing a model?\n",
    "    -   *Answer:* Having fewer hyperparameters significantly reduces the time and computational effort needed for model tuning (e.g., via grid search or random search). This allows for faster iteration and quicker deployment of a reasonably strong model, which is critical when project timelines are constrained.\n",
    "\n",
    "2.  **Extension:** The Random Forest model was the \"running winner.\" How might creating an ensemble of the Random Forest model's predictions *with* the predictions from a distinctly different model, like the LinearSVR (even if it performed slightly worse individually), potentially lead to a more robust final result?\n",
    "    -   *Answer:* Combining predictions from diverse models (a technique called \"stacking\" or \"blending\") can improve overall performance if the individual models make errors on different types of instances or capture different aspects of the data-price relationship. A meta-model could learn to weigh their outputs optimally, potentially creating a final prediction that is more accurate and robust than any single model alone.\n",
    "\n",
    "# Day 3 - Comparing Traditional ML Models: From Random to Random Forest\n",
    "\n",
    "### Summary\n",
    "This transcript recaps the performance journey through various traditional machine learning models for product price prediction, starting from a random baseline ($341 error) and culminating in a Random Forest model achieving a $97 average error. The speaker contextualizes this best error as respectable given the inherent difficulty of the task, and then pivots to the next phase: evaluating Large Language Models (LLMs) like GPT-4 in a challenging zero-shot learning paradigm, where they will predict prices without task-specific training data.\n",
    "\n",
    "### Highlights\n",
    "-   **Performance Ladder of Traditional Models:** A clear hierarchy of model effectiveness was demonstrated: Random Guess ($341 error), Constant Average ($146), Linear Regression with manual features ($139), Linear Regression with Bag-of-Words ($114), Support Vector Regression ($113), and finally, Random Forest Regression as the top performer ($97). This progression illustrates the impact of model choice and feature representation.\n",
    "-   **Word2Vec's Surprising Underperformance (with Linear Model):** The more sophisticated Word2Vec embeddings (400 dimensions) resulted in slightly worse performance ($115 error) compared to simpler Bag-of-Words (1000 dimensions) when both were paired with a linear regression model. This suggests that the linear model might not have fully leveraged the semantic richness of Word2Vec, possibly benefiting more from the breadth of keywords in BoW for this task.\n",
    "-   **Contextualizing the Best Achieved Error:** The $97 average error from the Random Forest model is framed as a significant achievement (\"not bad at all\"), considering the complexity and subjective nature of accurately pricing diverse products (electronics, automotive, appliances) based solely on textual descriptions, a task also challenging for humans.\n",
    "-   **Transition to LLMs in a Zero-Shot Setting:** The subsequent experiments will involve using \"frontier\" LLMs (GPT-4 variants) to predict prices. A key aspect of this approach is that the LLMs will operate in a zero-shot learning mode, meaning they will not be trained on the specific product dataset but will rely on their vast pre-existing knowledge to infer prices from descriptions.\n",
    "-   **Contrasting Learning Paradigms:** The traditional models benefited from task-specific training data, learning patterns directly relevant to the dataset's products and prices. In contrast, the LLMs will be tested on their ability to generalize from their broad \"worldly knowledge\" without this targeted training, presenting a different set of advantages and challenges.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Zero-Shot Learning with LLMs**\n",
    "    1.  **Why is this concept important?** Zero-shot learning enables LLMs to perform tasks for which they have not been explicitly trained with specific examples. This capability stems from the extensive knowledge and pattern recognition abilities acquired during their pre-training on vast and diverse datasets. It allows for flexible application to new tasks without the immediate need for custom datasets or fine-tuning.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** This is highly valuable for rapidly addressing novel problems, tasks with limited labeled data, or when quick prototyping is needed. For instance, an LLM could classify text, answer questions about a new topic, or, as in this case, attempt to estimate a product's price based on a description, all without prior examples of that specific task.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Few-shot learning (where the LLM is given a small number of examples in the prompt to guide its response), one-shot learning, prompting engineering (designing effective prompts to elicit desired behavior), fine-tuning (further training a pre-trained model on a task-specific dataset to improve performance), and evaluating the generalization capabilities of large models.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Hypothesis:** Why might a 1000-dimension Bag-of-Words feature set have enabled a linear regression model to outperform one using 400-dimension Word2Vec features for this specific price prediction task?\n",
    "    -   *Answer:* The linear regression model might have benefited more from the high-dimensional, sparse Bag-of-Words vectors because they explicitly capture the presence or frequency of a wide array of specific keywords, which can be strong linear predictors for some products. While Word2Vec's denser 400 dimensions capture semantic meaning, a simple linear model might not be able to fully exploit these complex, non-linear relationships, whereas it can more easily assign weights to individual word occurrences from BoW.\n",
    "\n",
    "2.  **Expectation:** What could be a primary advantage and a primary disadvantage of using an LLM in a zero-shot setting for price prediction compared to the trained Random Forest model?\n",
    "    -   *Answer:* A primary advantage of the zero-shot LLM could be its ability to leverage broader world knowledge, potentially understanding nuances in descriptions or recognizing product types/brands not well-represented in the specific training set used by the Random Forest. A primary disadvantage could be its lack of specific tuning to the dataset's particular price distributions and feature-price correlations, potentially leading to less precise or more generalized price estimations compared to the Random Forest, which learned directly from in-domain examples.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
