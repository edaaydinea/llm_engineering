{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb6e42e",
   "metadata": {},
   "source": [
    "# Day 2 - LLM Deployment Pipeline: From Business Problem to Production Solution\n",
    "\n",
    "### Summary\n",
    "This lecture segment introduces the initial three steps of a crucial five-step strategy for developing and deploying Large Language Model (LLM) solutions to address real-world commercial problems. It emphasizes the foundational importance of thoroughly understanding business requirements and data, meticulously preparing data and evaluating baseline models, and systematically selecting appropriate LLMs. This structured approach enables data scientists to effectively translate business challenges into operational LLM applications by ensuring comprehensive planning and informed decision-making from the outset.\n",
    "\n",
    "### Highlights\n",
    "-   **Five-Step LLM Strategy**: The lecture outlines a strategic framework (Understand, Prepare, Select, Customize, Productionize) for guiding the application of LLMs to business problems. This methodical approach helps data scientists manage the journey from problem definition to a deployed solution, ensuring all critical phases are addressed.\n",
    "-   **Step 1: Understanding Business & Data Requirements**: This initial phase stresses the need to deeply analyze business objectives, define clear success metrics (both technical and business-focused), assess data characteristics (quantity, quality, format), and identify non-functional requirements (e.g., budget, latency, scalability, time-to-market). This understanding is vital for aligning the LLM solution with strategic goals and practical constraints, directly influencing model selection and overall project viability.\n",
    "-   **Step 2: Preparation - Baselines and Data Curation**: Key activities in this step include researching existing solutions (ranging from non-AI to traditional data science methods) to establish performance baselines and comparing relevant LLMs using criteria like cost, context length, and benchmark performance. A critical component is data curation, which involves cleaning, preprocessing (e.g., parsing), and strategically splitting the dataset into training, validation, and test sets, forming the bedrock for robust model development.\n",
    "-   **Step 3: Model Selection**: This step involves choosing the most suitable LLM(s) based on the insights gained during the understanding and preparation phases, followed by initial experimentation. It acts as a bridge between planning and the more intensive customization stages, ensuring that the selected models are well-suited to the specific problem and data landscape.\n",
    "-   **Significance of Non-Functional Requirements (NFRs)**: The lecture underscores how NFRs—such as budgetary limits, acceptable response times (latency), and speed of deployment (time-to-market)—critically shape technological decisions. For example, tight deadlines might favor using a frontier model via an API and rapid prototyping tools like Gradio, demonstrating the need to consider operational aspects early in the project lifecycle.\n",
    "-   **Data Splitting Protocol (Train/Validation/Test)**: The importance of dividing the available data into distinct training, validation, and test sets is highlighted. This practice is essential for rigorous model development, enabling hyperparameter tuning on the validation set and providing an unbiased evaluation of the final model's performance on unseen test data, thereby preventing overfitting and ensuring the model generalizes well to new data.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Importance of Non-Functional Requirements (NFRs)**\n",
    "    1.  **Why is this concept important?** NFRs define the operational quality and constraints of an LLM system, such as its performance (latency, throughput), scalability, cost, and security. Overlooking NFRs can result in a model that performs well in a lab setting but fails in a real-world business context because it's too slow, too expensive, or cannot handle the required user load.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** For an interactive LLM-powered customer support tool, low latency is essential for a good user experience. A system analyzing financial transactions for fraud detection must be highly scalable and reliable. The project's budget will determine the feasibility of using large proprietary models versus smaller open-source alternatives.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Students should explore **system design principles**, **cloud computing architectures** (for understanding scalable and cost-effective deployment options), **MLOps (Machine Learning Operations)** practices (for deploying, monitoring, and maintaining models in production), and **cost-benefit analysis** to make informed decisions balancing model capabilities with operational constraints.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** Which specific dataset or project could benefit from this structured five-step LLM strategy? Provide a one-sentence explanation.\n",
    "    -   *Answer:* A project to create an LLM-based system for personalized educational content generation would greatly benefit from this strategy, as it ensures clear definition of learning objectives and success metrics (Understand), careful preparation of diverse educational data (Prepare), and selection of models capable of coherent and contextually relevant content generation (Select).\n",
    "2.  **Teaching:** How would you explain the importance of establishing a baseline (Step 2: Preparation) to a junior colleague, using one concrete example? Keep the answer under two sentences.\n",
    "    -   *Answer:* Think of it like this: if you're building an LLM to summarize news articles, knowing that a simple algorithm currently extracts the first three sentences with 50% usefulness gives you a benchmark; this baseline allows you to concretely prove that your new LLM is, for instance, 30% more useful and therefore a valuable improvement.\n",
    "\n",
    "# Day 2 - Prompting, RAG, and Fine-Tuning: When to Use Each Approach\n",
    "\n",
    "### Summary\n",
    "This lecture segment focuses on optimizing Large Language Models (LLMs), the fourth step in a strategy for solving commercial problems, by providing a comparative analysis of three core techniques: prompting, Retrieval Augmented Generation (RAG), and fine-tuning. It details the distinct advantages, disadvantages, and typical use-cases for each, highlighting that prompting excels in speed and low-cost iteration, RAG delivers accuracy by leveraging external, up-to-date knowledge sources, and fine-tuning enables deep model specialization and nuanced understanding, albeit with higher demands on data and resources. This comprehensive overview equips data science students and professionals to make informed decisions on which optimization method, or combination thereof, is most suitable for their specific project requirements, available data, and performance goals.\n",
    "\n",
    "### Highlights\n",
    "-   **Three Core LLM Optimization Techniques**: The lecture identifies prompting (including strategies like multi-shot, chaining, and tool usage), Retrieval Augmented Generation (RAG), and fine-tuning as the primary methods for enhancing LLM performance beyond their pre-trained state. This framework is crucial for data scientists to select appropriate pathways for model improvement.\n",
    "-   **Inference-Time vs. Training-Time Optimization**: A key distinction is made: prompting and RAG are inference-time techniques, meaning they modify or augment the model's behavior when a prediction is requested, without altering the model's underlying weights. Fine-tuning, conversely, is a training-time technique that involves further training the model on new data to adjust its internal parameters.\n",
    "-   **Prompting: Agility and Low Cost**: Prompt engineering is lauded for its speed, ease of implementation, and low cost, allowing for rapid iterations and quick, direct improvements in LLM outputs. It's often the first approach for enhancing model performance, particularly with powerful frontier models.\n",
    "-   **RAG: Enhancing Accuracy with External Knowledge**: RAG improves LLM accuracy by dynamically providing relevant, factual information from external knowledge bases during inference. This makes it highly scalable for vast amounts of data and efficient, as it avoids overly large prompts and allows the knowledge to be updated without retraining the core model.\n",
    "-   **Fine-tuning: Achieving Deep Expertise and Nuance**: Fine-tuning enables an LLM to develop deep, specialized knowledge, learn specific styles or tones, and exhibit more nuanced understanding relevant to a particular domain. While complex and data-intensive, it can lead to superior performance on targeted tasks and potentially faster inference for the specialized skill, as the knowledge becomes an intrinsic part of the model.\n",
    "-   **Limitations of Prompting**: The primary drawbacks of prompting include the constraints of the LLM's context window size, leading to diminishing returns with excessively long prompts, and increased latency and cost, especially with prompt chaining.\n",
    "-   **Challenges of RAG Implementation**: RAG systems, while powerful, involve a higher initial setup effort (e.g., vector databases, data pipelines) and require ongoing maintenance to keep the external knowledge base current and accurate. They may also provide less nuanced understanding compared to a fine-tuned model.\n",
    "-   **Fine-tuning Complexities: Data, Cost, and Catastrophic Forgetting**: The main challenges with fine-tuning are its difficulty, the substantial amount of high-quality training data required, associated training costs (compute and time), and the significant risk of \"catastrophic forgetting.\"\n",
    "-   **Catastrophic Forgetting Explained**: This phenomenon occurs when a pre-trained model, during fine-tuning on a specific task, loses some of its original general knowledge and capabilities, potentially degrading its performance on tasks outside the fine-tuning domain.\n",
    "-   **Guidance on Technique Selection**: The lecture advises using prompting as a starting point for quick wins; RAG when factual accuracy from a large, dynamic knowledge base is crucial without incurring training costs; and fine-tuning for specialized tasks requiring top performance, nuanced understanding, and where sufficient data is available. These techniques can also be used in combination.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Catastrophic Forgetting in Fine-tuning**\n",
    "    1.  **Why is this concept important?** Catastrophic forgetting is a critical issue in continual learning and fine-tuning because as a model specializes on new data, it can overwrite or lose previously learned knowledge, diminishing its general capabilities. For data scientists, understanding this risk is vital for preserving a model's broad usefulness while adapting it to specific tasks.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** If a general-purpose LLM initially proficient in multiple languages and coding is fine-tuned exclusively on legal document analysis, it might excel in law but perform poorly when later asked to generate Python code or translate French, which it could do before. This is problematic if the model is intended for diverse applications post-fine-tuning.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** To mitigate catastrophic forgetting, data scientists should explore **continual learning strategies**, **regularization techniques** (like Elastic Weight Consolidation - EWC), **replay mechanisms** (selectively retraining on old data), and **parameter-efficient fine-tuning (PEFT)** methods such as LoRA or Adapters, which update only a small subset of model parameters.\n",
    "\n",
    "-   **Inference-Time vs. Training-Time Optimization**\n",
    "    1.  **Why is this concept important?** Distinguishing between these optimization types helps in resource allocation and setting expectations. Training-time optimizations (like fine-tuning) lead to more ingrained changes in the model but are resource-intensive and slow. Inference-time optimizations (like prompting or RAG) are more agile and less costly to implement but might not achieve the same depth of adaptation.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** For a system needing to answer questions based on rapidly changing information (e.g., daily news summaries), RAG (inference-time) is ideal because its knowledge base can be updated quickly. For developing a chatbot that deeply embodies a specific brand's unique voice and complex policies, fine-tuning (training-time) on brand communication data would be more effective.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Alongside training-time methods like **supervised fine-tuning (SFT)** and **Reinforcement Learning from Human Feedback (RLHF)**, one should study model architecture. For inference-time, deep dives into **advanced prompt engineering** (e.g., Chain-of-Thought, Tree-of-Thought), **vector database technologies**, **semantic search algorithms**, and frameworks for **tool integration** (e.g., function calling) are beneficial.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** Imagine you are tasked with creating an LLM-powered assistant for scientific research that must understand complex papers and also stay updated with the very latest pre-print articles published daily. Which combination of optimization techniques might you consider and why?\n",
    "    -   *Answer:* I would consider a combination of **fine-tuning** on a corpus of existing scientific papers to teach the model the fundamental concepts and jargon of the domain, and then use **RAG** with a vector database that is updated daily with new pre-print articles to ensure its knowledge is current without constant retraining.\n",
    "2.  **Teaching:** How would you explain the trade-off between RAG's \"lack of nuance\" and fine-tuning's \"deep expertise\" to a non-technical stakeholder deciding on an LLM project budget?\n",
    "    -   *Answer:* You could explain it like this: RAG is like giving an employee a detailed manual to look up answers—they'll be accurate based on the manual but might not grasp subtle implications. Fine-tuning is like sending that employee through extensive specialized training—they'll develop a deeper, more intuitive understanding, but this training is a bigger upfront investment in time and cost.\n",
    "3.  **Extension:** If a team is hesitant to fully fine-tune a large foundation model due to concerns about catastrophic forgetting and high training costs, what is an intermediate optimization strategy they could explore to adapt the model to their specific data?\n",
    "    -   *Answer:* The team could explore **Parameter-Efficient Fine-Tuning (PEFT)** methods, such as Low-Rank Adaptation (LoRA). These techniques involve training only a small number of new parameters or adapting existing ones slightly, significantly reducing computational cost and the risk of catastrophic forgetting while still allowing the model to learn from new domain-specific data.\n",
    "\n",
    "# Day 2 - Productionizing LLMs: Best Practices for Deploying AI Models at Scale\n",
    "\n",
    "### Summary\n",
    "This lecture segment outlines \"Productionize,\" the crucial fifth step in the five-step strategy for deploying Large Language Models, focusing on the activities required to take a developed model into a live, operational environment. Key aspects include defining an API for model access, managing hosting and deployment, addressing monitoring and security, scaling, measuring business impact, and establishing a continuous retraining and improvement loop. The speaker also provides a status update on their ongoing \"predict product prices\" project, noting they are currently in the \"Preparation\" phase (Step 2).\n",
    "\n",
    "### Highlights\n",
    "-   **Step 5: Productionization Defined**: This final step in the LLM strategy involves transitioning a model from development to real-world application. It encompasses defining an API for interaction, deciding on hosting and deployment, ensuring operational stability (monitoring, security, scalability), measuring business outcomes, and maintaining performance through continuous retraining and improvement.\n",
    "-   **API for Model Interaction**: A core component of productionization is creating an Application Programming Interface (API). This allows various applications or users to programmatically call the LLM solution—which might be an open-source model or a frontier model, potentially with RAG or prompt engineering logic—in a standardized way.\n",
    "-   **Operational Management**: Critical operational concerns must be addressed, including how the deployed model will be monitored for performance and errors, how information security will be maintained, and how the system will scale to handle varying loads. This ensures the LLM is reliable and trustworthy in production.\n",
    "-   **Measuring Business Value and Continuous Improvement**: Productionization involves tracking the business metrics identified in the initial \"Understand\" phase to assess the model's real-world impact. Furthermore, it establishes an ongoing cycle of performance measurement, retraining with new data, and iterative model improvement to maintain and enhance effectiveness over time.\n",
    "-   **Project Progress Update (\"Predict Product Prices\")**: The lecture situates the current project work within the five-step strategy: \"Understand\" (Step 1) is complete, and the team is actively engaged in \"Preparation\" (Step 2), specifically data preparation. The subsequent steps—\"Select,\" \"Customize,\" and \"Productionize\"—are planned future activities.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **API Definition for LLM Access**\n",
    "    1.  **Why is this concept important?** Defining an API is essential as it creates a formal contract for how other software systems or users can interact with the LLM. It abstracts the underlying complexity of the LLM, promoting modularity, easier integration into existing workflows, and allowing the LLM service to be updated or scaled independently of the applications that use it.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** For an e-commerce site using an LLM to generate product descriptions, the website's backend would call the LLM via an API, sending product features and receiving a description. This API ensures that if the LLM is updated or changed, the website's code doesn't necessarily need to change as long as the API contract is maintained.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Key areas include **RESTful API design principles**, **GraphQL**, **gRPC** (for high-performance microservices), API **security best practices** (e.g., authentication using API keys or OAuth, authorization), data interchange formats like **JSON**, and API documentation standards such as **OpenAPI (Swagger)**.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** Why is the \"measure business metrics\" aspect of productionization crucial for demonstrating the value of an LLM project to stakeholders?\n",
    "    -   *Answer:* Measuring business metrics links the LLM's technical performance directly to tangible business outcomes (e.g., reduced costs, increased sales, improved customer satisfaction) identified in Step 1, thereby providing clear evidence of its ROI and justifying the investment to stakeholders.\n",
    "2.  **Teaching:** How would you explain the importance of \"scalability\" in the productionization phase to a junior data scientist who has only built models on their laptop?\n",
    "    -   *Answer:* On your laptop, your model serves one user—you; in production, it might need to serve thousands simultaneously, so scalability means designing the system to handle this increased demand smoothly without crashing or becoming slow, ensuring a good experience for all users.\n",
    "\n",
    "# Day 2 - Optimizing Large Datasets for Model Training: Data Curation Strategies\n",
    "\n",
    "### Summary\n",
    "This technical lecture details the second and final stage of data curation for an LLM-based product price prediction project, emphasizing the efficient loading and strategic preparation of a substantially larger dataset. The speaker introduces a custom Python module, `ItemLoader`, which utilizes `concurrent.futures.ProcessPoolExecutor` for parallel data ingestion from Hugging Face, significantly speeding up the process by distributing work across multiple CPU cores. A key aspect of the preparation involves filtering products to a specific price range ($0.50 to $999.49) to enhance model training stability, manage error metrics, and focus the project's scope, with plans to load multiple product categories to create a comprehensive dataset.\n",
    "\n",
    "### Highlights\n",
    "-   **Final Stage of Data Curation**: This session focuses on expanding and meticulously preparing the dataset for optimal LLM training, involving both efficient loading mechanisms and strategic data refinement.\n",
    "-   **`ItemLoader` for Efficient Loading**: A custom Python module (`loaders.py`) featuring an `ItemLoader` class is presented. This tool is specifically designed to load datasets from the Hugging Face repository rapidly and is intended for reuse in various machine learning projects.\n",
    "-   **Parallel Processing with `ProcessPoolExecutor`**: The `ItemLoader` leverages Python's `concurrent.futures.ProcessPoolExecutor` to perform data loading in parallel using multiple CPU workers (e.g., 8 workers for an 8-core machine). This approach drastically cuts down data ingestion time, as demonstrated by loading an \"appliances\" dataset in 0.2 minutes compared to a previous ~1 minute.\n",
    "-   **Chunking Data with Generators**: To handle large datasets effectively, the loading mechanism processes data in manageable chunks (e.g., 1000 data points at a time) using generators. This technique is memory-efficient and well-suited for processing large volumes of data.\n",
    "-   **Strategic Price-Range Filtering**: A critical data preprocessing step is the filtering of products to include only those priced between $0.50 and $999.49. This decision aims to prevent statistical distortion from extreme price outliers, stabilize model evaluation metrics (particularly absolute error), and define a more focused scope for the price prediction task.\n",
    "-   **Reusable Data Loading Architecture**: The speaker highlights that the `ItemLoader` code is crafted for readability and reusability, encouraging its adaptation for other data-intensive machine learning tasks where efficient I/O is paramount.\n",
    "-   **Quantifiable Impact of Multi-Core Processing**: The significant speed improvement achieved by using multiple workers for data loading is demonstrated. Users are advised to configure the number of workers based on their specific hardware capabilities to balance performance and system load.\n",
    "-   **Expansion to Multiple Datasets**: The project plan includes loading and consolidating several product datasets from a Hugging Face repository (covering categories like appliances, automotive, electronics, etc.). This will create a large and diverse dataset, aiming to train a more robust and generalizable price prediction LLM.\n",
    "-   **Initial Download Time Consideration**: It's noted that the first-time execution of the loading script might be slower, as datasets need to be downloaded from Hugging Face. Subsequent runs will benefit from a local cache, speeding up the process.\n",
    "-   **Flexibility in Dataset Scope**: Learners are encouraged to adapt the process to their needs, such as by working with smaller data subsets (e.g., only the \"electronics\" category) if they face computational constraints or wish to experiment with model performance on different data scales.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Parallel Processing for Data Loading (using `ProcessPoolExecutor`)**\n",
    "    1.  **Why is this concept important?** In machine learning, data loading and initial preprocessing can be significant time sinks, especially with large datasets. Parallel processing distributes these tasks across multiple CPU cores, drastically reducing the overall time required. This accelerates the iterative cycle of model development and experimentation.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** This technique is vital in any scenario involving large-scale data ingestion, such as processing extensive logs for security analysis, loading millions of images for training computer vision models, or, as in this case, ingesting large product catalogs from various sources for e-commerce analytics or price prediction. Efficiently handling this data is key to timely insights and model deployment.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Key areas include understanding the differences and use cases for **multithreading vs. multiprocessing** in Python (especially concerning the Global Interpreter Lock - GIL), exploring other parallel and distributed computing libraries like **Dask** or **Ray** for even larger datasets, learning about **asynchronous programming (`asyncio`)** for I/O-bound operations, and optimizing data storage formats for faster reads.\n",
    "\n",
    "-   **Strategic Data Filtering (e.g., Price Range Cap)**\n",
    "    1.  **Why is this concept important?** Raw datasets are rarely perfect for direct use in model training. They often contain outliers, errors, or data points outside the intended scope, which can degrade model performance, skew evaluation metrics, or lead to a model that doesn't generalize well. Strategic filtering is a crucial step in cleaning and preparing data to align with the project's objectives.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** In a housing price prediction model, filtering out multi-million dollar listings might be necessary if the target is average homes. For a model predicting customer churn, filtering out newly acquired customers not representative of established patterns could be beneficial. In this lecture's context, limiting the product price range ensures the model focuses on a common segment and that extreme values don't disproportionately affect error calculations, leading to a more stable and interpretable model.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** This connects to **Exploratory Data Analysis (EDA)** for identifying sensible filtering criteria, **outlier detection methods** (e.g., Z-score, Interquartile Range), **feature engineering**, data **normalization and standardization** techniques (which are often applied after filtering), and understanding potential biases introduced or mitigated by filtering decisions.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** If you were using the described `ItemLoader` to load datasets with highly variable text lengths for an NLP task (e.g., some product descriptions are a few words, others are thousands), what potential issue related to \"chunking\" might arise, and how could you address it?\n",
    "    -   *Answer:* If chunking is purely by a fixed number of items (e.g., 1000 data points), some chunks processed by workers might contain vastly more total text data than others, leading to uneven processing times per worker. To address this, one might consider dynamic chunking based on approximate total token count, or ensure robust error handling and timeouts if some \"heavy\" chunks take significantly longer.\n",
    "2.  **Teaching:** How would you explain to a junior data scientist why setting a price cap (like $0.50 - $999.49) for training a price prediction model doesn't necessarily mean the business can *never* get predictions for items outside this range?\n",
    "    -   *Answer:* We're initially focusing the model on the most common price range to build a strong, reliable core predictor and make its performance easier to evaluate. Once this model is effective, we can explore strategies for higher-priced items, like training a separate specialized model or using different techniques, ensuring we don't compromise the accuracy for the bulk of products by trying to make one model do everything perfectly from the start.\n",
    "3.  **Extension:** The speaker's `ItemLoader` uses `ProcessPoolExecutor` for parallelism. What implications would using a `ThreadPoolExecutor` instead have, particularly considering Python's Global Interpreter Lock (GIL) and the nature of data loading/processing tasks?\n",
    "    -   *Answer:* If the tasks performed by each worker (loading from Hugging Face, then parsing with the `Item` class) are largely I/O-bound (waiting for data to download/read), `ThreadPoolExecutor` could be efficient as threads can release the GIL during I/O waits. However, if the parsing/object creation part is CPU-bound, `ThreadPoolExecutor` would offer limited true parallelism due to the GIL; `ProcessPoolExecutor` bypasses the GIL by using separate processes, making it better for CPU-bound portions of the task, though it incurs higher inter-process communication overhead.\n",
    "\n",
    "# Day 2 - How to Create a Balanced Dataset for LLM Training: Curation Techniques\n",
    "\n",
    "### Summary\n",
    "This lecture segment details the process of refining a large, 2.8 million-item product dataset down to a more manageable and balanced set of approximately 408,000 data points for effective LLM fine-tuning. After analyzing the initial distributions, which confirmed controlled token lengths but significant skews in price (favoring cheaper items) and category (favoring \"Automotive\"), a sophisticated sampling strategy was implemented. This strategy involved \"slotting\" items by their integer dollar price and then applying a stratified and weighted sampling approach—particularly for items under $240, giving higher selection weight to non-automotive categories—to improve the representation of higher-priced items and achieve a better, though still realistic, balance across product categories, ultimately creating a high-signal dataset for predicting product prices.\n",
    "\n",
    "### Highlights\n",
    "-   **Initial Large Dataset**: The aggregated dataset from multiple sources totals over 2.8 million data points, which is considered excessive and potentially imbalanced for the planned LLM training.\n",
    "-   **Controlled Prompt Length**: Analysis confirms that all generated training prompts remain under 180 tokens, a design choice made earlier to facilitate efficient fine-tuning with models like Llama and manage costs with API-based frontier models.\n",
    "-   **Persistent Price Skew**: Despite an initial price filter ($0.50-$999.49), the raw dataset's price distribution is heavily skewed towards lower-cost items, with higher-priced items being very sparse.\n",
    "-   **Category Imbalance**: The dataset also exhibits significant imbalance across product categories, with \"Automotive\" products being the most numerous by a large margin.\n",
    "-   **Refinement Goal**: The primary objective is to sample this large dataset down to approximately 400,000 data points, creating a smaller, more balanced, and higher-signal dataset conducive to effective model training.\n",
    "-   **Price \"Slotting\" for Granular Sampling**: A technique of \"slotting\" or bucketing products by their integer dollar price (from $1 to $999) is employed using a Python `defaultdict`. This allows for more targeted and controlled sampling within each specific price point.\n",
    "-   **Stratified and Weighted Sampling Implementation**:\n",
    "    * For products priced above $240, all items from their respective price slots are included in the final sample.\n",
    "    * For products priced at $240 or less, a maximum of 1200 items are sampled from each price slot.\n",
    "    * During this sampling, `numpy.random.choice` is used with custom weights: \"Automotive\" items are assigned a weight of 1, while items from all other categories are given a weight of 5. This aims to reduce the overrepresentation of automotive products and enhance the presence of other categories.\n",
    "-   **Iterative Parameter Tuning**: The specific sampling thresholds (e.g., $240 cut-off, 1200 items per slot) and the weighting scheme were determined through a process of trial and error, with the speaker adjusting them until satisfactory distributions and training outcomes were achieved.\n",
    "-   **Significantly Improved Price Distribution**: The sampling process results in a price distribution that, while still skewed towards affordable items (reflecting reality), shows a much better representation of products across all price points up to $999, including notable spikes at common price endings (e.g., $X99.xx).\n",
    "-   **Moderated Category Balance**: The weighted sampling strategy leads to a slightly improved balance among product categories. While \"Automotive\" remains the largest, its dominance is reduced, and other categories are better represented, providing a more diverse training set. \"Appliances\" constitute the smallest portion (1%).\n",
    "-   **Call for User Experimentation**: The audience is encouraged to critically review the provided sampling logic and experiment with different parameters, weights, or alternative strategies to potentially further optimize the dataset or model performance.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Price Slotting for Stratified Sampling**\n",
    "    1.  **Why is this concept important?** When a dataset has a continuous variable (like price) that is unevenly distributed, simple random sampling can lead to poor representation of less common ranges (e.g., high-priced items). \"Slotting\" items into discrete bins based on this variable (e.g., each whole dollar price) allows for stratified sampling. This means sampling can be controlled within each \"stratum\" or \"slot,\" ensuring that even sparsely populated segments are adequately included in the final dataset, leading to a more representative sample.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** This is broadly useful for ensuring fairness and robustness in models. For instance, when sampling a population for a survey, one might stratify by income brackets to ensure all economic groups are heard. In credit risk modeling, one might stratify by loan size or credit score to ensure the model learns from a diverse range of client profiles. Here, it ensures the price prediction model sees enough examples of both cheap and expensive items.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Key areas include **formal stratified sampling theory**, various **data binning and discretization methods**, techniques for **visualizing data distributions** (histograms, kernel density estimates) to inform slotting decisions, and general approaches to **handling imbalanced datasets**, such as oversampling minority classes or undersampling majority ones within strata.\n",
    "\n",
    "-   **Weighted Sampling for Category Balancing**\n",
    "    1.  **Why is this concept important?** Machine learning models trained on datasets with severe class or category imbalances can become biased towards the majority class, performing poorly on minority classes. Weighted sampling is a technique to counteract this by increasing the probability of selecting instances from underrepresented categories (or decreasing it for overrepresented ones). This helps create a training dataset where the model gets more balanced exposure to all categories, potentially improving its overall performance and fairness.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** This is critical in many domains: in medical diagnosis, to ensure models can detect rare diseases; in fraud detection, to identify less common types of fraudulent activities; in spam filtering, to correctly classify various types of spam messages. In this lecture, it aims to prevent the price model from being overly specialized in \"Automotive\" products at the expense of learning from other, less numerous categories.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Further study should include other **class imbalance learning techniques** (e.g., SMOTE for synthetic oversampling, Edited Nearest Neighbors for undersampling), **cost-sensitive learning** (where misclassifying minority classes incurs a higher penalty), ensemble methods designed for imbalanced data (like Balanced Random Forest or EasyEnsemble), and the use of appropriate **evaluation metrics** (like precision, recall, F1-score per class, or macro-averaged F1) that reflect performance on imbalanced data.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** If you were building a model to predict house prices and your dataset was heavily skewed towards 2-3 bedroom houses in a specific city, how might you adapt the \"slotting\" and \"weighted sampling\" techniques to ensure your model also learns effectively for larger houses or houses in less represented neighborhoods?\n",
    "    -   *Answer:* You could create \"slots\" based on a combination of factors like number of bedrooms and neighborhood (or zip code). Then, within these combined slots, you could apply weighted sampling to give higher selection probability to larger houses (e.g., 4+ bedrooms) or houses in underrepresented neighborhoods, thus ensuring these scarcer property types are adequately present in the training data.\n",
    "2.  **Teaching:** How would you explain the benefit of the described complex sampling strategy (slotting + weighting) over simply taking a smaller random sample of the initial 2.8 million data points to a non-technical manager?\n",
    "    -   *Answer:* Simply taking a random slice of a skewed dataset would likely give us a smaller, but still skewed, dataset, meaning our model wouldn't learn well about less common (e.g., expensive) items or smaller product groups. Our more complex sampling is like carefully hand-picking items to ensure we get a good variety of prices and types, making our final smaller dataset much richer and helping the model learn more effectively about the whole range of products.\n",
    "3.  **Extension:** The speaker used fixed weights (1 for automotive, 5 for others) in the sampling process. What could be a more adaptive or dynamic way to determine these weights to achieve a target distribution across categories?\n",
    "    -   *Answer:* A more dynamic approach could involve calculating weights inversely proportional to the current category frequencies in the `slots` being sampled, aiming for a target distribution (e.g., closer to uniform, or a specific desired skew). For example, if a category has `N_c` items and the total in the slot is `N_total`, its desired representation could influence its sampling weight, possibly adjusted iteratively or calculated based on an overall target proportion for each category in the final ~400k sample.\n",
    "\n",
    "# Day 2 - Finalizing Dataset Curation: Analyzing Price-Description Correlations\n",
    "\n",
    "### Summary\n",
    "This final lecture on dataset curation for the product price prediction project involves a last look at the ~400,000-item dataset, including an analysis of the correlation between product description length and price (found to be weak) and a detailed examination of how the Llama tokenizer handles prices, often mapping numerical values up to 999 to single tokens. The segment concludes with the critical steps of shuffling the dataset using a fixed random seed for reproducibility, splitting it into a 400,000-item training set and a 2,000-item test set, and detailing methods for saving the curated data locally using pickle files and preparing it for potential upload to the Hugging Face Hub, marking the completion of the data preparation phase.\n",
    "\n",
    "### Highlights\n",
    "-   **Final Dataset Review**: The session initiates with a final analytical look at the curated ~400,000-item dataset before it's finalized for model training.\n",
    "-   **Description Length vs. Price Analysis**: A scatter plot investigating the relationship between the length of product descriptions and their prices was generated. While some visual patterns emerged, no strong, clear correlation was identified, suggesting description length alone might not be a dominant price predictor for traditional ML models.\n",
    "-   **Detailed Price Tokenization Study**: A significant portion is dedicated to understanding how product prices are tokenized by the Llama tokenizer. A helper function demonstrates that prices (rounded to the nearest dollar, e.g., \"$34.00\", \"$765.00\") often have their numerical component (e.g., \"34\", \"765\") represented as a single token.\n",
    "-   **Llama Tokenizer's Number Encoding**: It's highlighted that the Llama tokenizer, akin to GPT's, possesses distinct tokens for many numbers, including those up to three digits. This is a model-specific characteristic and can differ from other tokenizers (e.g., Qwen, Gemma, Phi-3), potentially simplifying price prediction tasks for this specific model.\n",
    "-   **Dataset Shuffling for Unbiased Training**: The curated dataset, which had some order due to the sampling process, is thoroughly shuffled using `random.shuffle`. This step is crucial to ensure that the data fed into the model during training is not in any predictable sequence, which could bias learning.\n",
    "-   **Ensuring Reproducibility with Random Seed**: Before shuffling, `random.seed()` is set to a fixed value. This practice guarantees that the shuffling process, and consequently the train/test splits, are identical across different runs, enabling reproducible experimental results.\n",
    "-   **Train/Test Data Splitting**: The shuffled dataset (approximately 408,000 items) is divided into a primary training set of 400,000 items and a test set of 2,000 items. The speaker acknowledges the test set is relatively small but deems it adequate for the project's evaluation, noting diminishing returns from larger test sets for their specific goals.\n",
    "-   **Preparation for Hugging Face Hub**: The lecture outlines the steps and code structure for transforming the training and test prompts and prices into Hugging Face `Dataset` objects, encapsulated within a `DatasetDict`. This format is standard for easy sharing and utilization via the Hugging Face Hub.\n",
    "-   **Local Data Persistence via Pickling**: The finalized training and test data collections are serialized and saved locally as `train.pickle` and `test.pickle` files. This allows for convenient and fast reloading of the processed data in future work sessions, bypassing the need to repeat the entire curation pipeline.\n",
    "-   **Emphasis on Token Behavior Understanding**: Users are encouraged to independently explore how different data points, especially numerical values, are tokenized by their chosen LLM's tokenizer, as this can have implications for model behavior and performance.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Impact of Price Tokenization on LLM Task Performance**\n",
    "    1.  **Why is this concept important?** The way numerical values like prices are broken down into tokens can significantly affect how easily an LLM learns to interpret and predict them. If a price such as \"$765.00\" is tokenized into many sub-units (e.g., \"$\", \"7\", \"6\", \"5\", \".\", \"0\", \"0\"), the model has a more complex task associating these pieces than if \"765\" is a single token. A more \"atomic\" representation of numbers can simplify the learning process for numerical tasks.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** In the context of this price prediction project, the Llama tokenizer's ability to represent common price figures (1-999) as single tokens is beneficial. For any LLM application involving numerical reasoning, generation, or regression (e.g., financial forecasting, scientific data analysis), the efficiency and granularity of number tokenization can impact accuracy and the model's \"numerical sense.\"\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Understanding **tokenizer algorithms** (like BPE, WordPiece, Unigram), the concept of a tokenizer's **vocabulary size**, how different LLMs specifically handle and represent **numerical inputs/outputs**, and research into **numerically-aware LLM architectures** or pre-training strategies.\n",
    "\n",
    "-   **Reproducibility in ML: Seeding, Shuffling, and Data Splitting**\n",
    "    1.  **Why is this concept important?** Reproducibility is fundamental for credible scientific research and reliable engineering. In machine learning, ensuring that experiments can be consistently repeated to produce the same outcomes is vital for debugging, validating improvements, comparing models fairly, and building trust. Setting random seeds before stochastic operations like data shuffling or model weight initialization is a key practice to control randomness.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** If an ML development process lacks reproducibility, it becomes difficult to determine if changes in performance are due to actual model or data modifications or just random variations. Consistent shuffling and data splitting ensure that model training and evaluation always occur on identical data partitions, making A/B testing of different approaches meaningful. This is essential in academic publications, collaborative industry projects, and when deploying audited or regulated AI systems.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Best practices in **MLOps (Machine Learning Operations)**, including **experiment tracking tools** (e.g., MLflow, Weights & Biases) that log configurations and seeds, **data version control** (e.g., DVC, Git LFS), **model versioning**, standardized **project templating**, and understanding and managing all sources of randomness within the ML pipeline (data preprocessing, augmentation, model initialization, training algorithms).\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** If you were tasked with training an LLM to perform arithmetic operations (e.g., addition, subtraction) based on word problems, why would understanding the tokenization of numbers be even more critical than in this price prediction task, and what might be a concern if numbers are split into multiple tokens?\n",
    "    -   *Answer:* For arithmetic, the precise numerical values and their relationships are paramount. If numbers are split into multiple tokens (e.g., \"234\" becomes \"2\", \"3\", \"4\"), the model might struggle to recognize them as cohesive numerical entities, making it significantly harder to learn mathematical operations correctly, potentially leading to errors in calculation or place value understanding.\n",
    "2.  **Teaching:** How would you explain to a stakeholder why a seemingly small detail like \"setting a random seed\" before shuffling data is a professional standard in data science, even if the dataset is very large?\n",
    "    -   *Answer:* Setting a random seed ensures that our \"random\" shuffle is the *exact same* random shuffle every time we run our process. This is a professional standard because it allows us to reliably repeat our experiments; if we change something and the model improves, we know it's due to our change, not just a lucky different shuffle of the data, which is vital for making trustworthy progress.\n",
    "3.  **Extension:** The speaker decided a 2,000-item test set was sufficient. If this model were to be deployed for a system automatically setting prices on an e-commerce platform with millions of diverse products, what arguments would you make for a more extensive and continuously updated testing/validation strategy post-deployment?\n",
    "    -   *Answer:* For a live e-commerce pricing system, I'd argue for a much larger, stratified test set reflecting diverse product types and price points, plus ongoing A/B testing and monitoring of key business metrics (sales, conversion, profit margins) for prices set by the LLM. This is because model performance can drift, new product types emerge, market conditions change, and any systematic error could have significant financial impact, necessitating continuous validation beyond a static test set.\n",
    "\n",
    "# Day 2 - How to Create and Upload a High-Quality Dataset on HuggingFace\n",
    "\n",
    "### Summary\n",
    "This brief concluding segment celebrates the completion of the extensive data curation process, which culminated in a well-sampled dataset formatted for Hugging Face and uploaded to the Hub, ready for future use. It reinforces the key concepts covered, such as the five-step LLM strategy, different optimization techniques, and practical data curation skills, while also setting the stage for the next session which will focus on developing baseline models using traditional machine learning methods.\n",
    "\n",
    "### Highlights\n",
    "-   **Data Curation Milestone**: The core message is the successful completion of the data curation phase, resulting in a high-quality, representative dataset formatted as a Hugging Face `DatasetDict` (with training and test splits) and uploaded to the Hugging Face Hub. This marks a significant practical achievement in the project workflow.\n",
    "-   **Consolidation of Learning**: The segment serves to briefly recap the valuable knowledge acquired, including understanding the five-step strategy for applying LLMs to commercial problems, evaluating the three main optimization techniques, and gaining hands-on experience with detailed dataset curation, including complex sampling logic.\n",
    "-   **Preview of Next Session: Baseline Models**: The audience is informed that the next topic will involve creating baseline models using traditional machine learning techniques. This indicates a shift towards establishing initial performance benchmarks before proceeding with more advanced LLM-specific training."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
