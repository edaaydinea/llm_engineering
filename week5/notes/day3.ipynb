{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31fe6ba0",
   "metadata": {},
   "source": [
    "# Day 3 - Mastering Vector Embeddings: OpenAI and Chroma for LLM Engineering\n",
    "\n",
    "### Summary\n",
    "This lesson introduces the concept of converting text into numerical vectors, known as embeddings, which capture the semantic meaning of the text. It contrasts simplistic word-counting methods with more advanced deep learning models like Word2Vec, BERT, and the specific OpenAI embeddings model that will be used in the upcoming practical session. The lesson also introduces Chroma as an example of a popular open-source vector database for storing these embeddings, explaining its role in retrieving relevant vectorized data to augment prompts for AI applications, thereby setting the stage for hands-on work with text embeddings and vector stores.\n",
    "\n",
    "### Highlights\n",
    "-   **Text-to-Vector Conversion (Embeddings)**: The central theme is the process of transforming text into numerical vectors or embeddings. These vectors are designed to represent the semantic meaning of the text, enabling machines to understand and compare textual data based on meaning rather than just lexical matches.\n",
    "-   **Evolution of Embedding Models**: The lesson traces the progression from simplistic vectorization methods (like word counting or \"bag-of-words\") to more sophisticated deep learning-based models. Key milestones mentioned include Word2Vec, which famously demonstrated semantic relationships (e.g., \"king - man + woman = queen\"), and BERT, a transformer-based model. The practical exercises will use a recent OpenAI embeddings model.\n",
    "-   **Limitations of Simplistic Word Counting**: The \"bag-of-words\" approach to creating vectors is highlighted as overly simplistic because it only counts word occurrences. This method fails to capture crucial aspects like word order, grammar, and, importantly, the semantic context of words (e.g., a word like \"Java\" having multiple meanings).\n",
    "-   **OpenAI Embeddings for State-of-the-Art Conversion**: The lesson states that OpenAI's embedding models, specifically a recent version (updated 2024), will be used in the subsequent hands-on session to convert text chunks into vectors. This indicates a focus on using current, high-quality embedding techniques.\n",
    "-   **Chroma as an Open-Source Vector Database**: Chroma is introduced as a popular example of an open-source vector database. Vector databases are specialized systems designed to efficiently store, manage, index, and retrieve these dense vector embeddings, typically using similarity search.\n",
    "-   **Role of Vector Databases in AI Systems**: The utility of vector databases like Chroma is explained in the context of AI applications (often seen in Retrieval Augmented Generation - RAG). They allow an application to perform a query, retrieve semantically similar vectors (and their associated text) from a large corpus, and then use this retrieved data to provide relevant context to a large language model.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **Advanced Embedding Models vs. Simplistic Word Counting**\n",
    "    1.  **Why is this concept important?** Understanding the difference is key to appreciating why modern NLP has made significant strides. Simplistic word counting creates sparse, high-dimensional vectors that primarily capture word frequency, lacking nuanced semantic understanding. Advanced embedding models (like Word2Vec, BERT, OpenAI embeddings) learn dense, lower-dimensional vector representations where the position and distance between vectors reflect semantic relationships, context, and meaning.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** For tasks like semantic search, question answering, recommendation systems, and text clustering, a true understanding of meaning is crucial. Word counting might find documents with the same words but different meanings, or miss documents with different words but similar meanings. Advanced embeddings can identify synonymy, understand context (e.g., \"bank\" as a financial institution vs. \"bank\" of a river), and capture analogies, leading to far more accurate and relevant results in these applications.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Deep learning architectures (specifically neural networks, transformers), distributional semantics (the idea that words in similar contexts have similar meanings), and dimensionality reduction techniques are all relevant. Exploring different types of embeddings (word, sentence, document) and their specific training methods (e.g., CBOW, Skip-gram for Word2Vec; masked language modeling for BERT) would provide further depth.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Application:** How might the ability to turn text into meaningful vectors and then search for similar vectors be useful in a customer support system that has a large knowledge base of FAQs and troubleshooting guides?\n",
    "    -   *Answer:* When a customer asks a question in natural language, the system could convert the question into a vector and then search the vectorized knowledge base for FAQ entries or guide sections with the most semantically similar vectors. This would allow the system to retrieve the most relevant help documents even if the customer's wording doesn't exactly match the text in the knowledge base, leading to faster and more accurate support.\n",
    "2.  **Distinction:** Why is it critically important that modern embedding models like Word2Vec or OpenAI Embeddings go beyond simple word counts to capture the *meaning* and *context* of words in text, especially when dealing with polysemous words (words with multiple meanings)?\n",
    "    -   *Answer:* Capturing meaning and context is vital because language is ambiguous. A word like \"bank\" means different things in \"river bank\" versus \"savings bank.\" Simple word counts would treat both instances identically. Advanced models, by analyzing surrounding words and training on vast datasets, can generate different vectors (or contextualized representations) for \"bank\" depending on its usage, ensuring that similarity searches or downstream tasks operate on the correct interpretation, leading to more accurate and relevant outcomes.\n",
    "\n",
    "# Day 3 - Visualizing Embeddings: Exploring Multi-Dimensional Space with t-SNE\n",
    "\n",
    "### Summary\n",
    "This hands-on JupyterLab session demonstrates the practical application of creating text embeddings using OpenAI's model, storing them efficiently in a Chroma vector database via LangChain, and then visualizing these complex, high-dimensional vectors in both 2D and 3D. By employing t-SNE for dimensionality reduction and Plotly for interactive plotting, the lesson visually reveals how the OpenAI embeddings effectively cluster semantically similar text chunks in vector space, even without prior knowledge of their source document categories. This provides an intuitive understanding of how vector representations capture meaning and encourages users to experiment further, building a crucial foundation for understanding Retrieval Augmented Generation (RAG) systems.\n",
    "\n",
    "### Highlights\n",
    "-   **Practical Text Embedding Generation**: The core of the lesson is the hands-on creation of numerical vector representations (embeddings) from text chunks using the `OpenAIEmbeddings` model provided through LangChain. These embeddings capture the semantic meaning of the text.\n",
    "-   **Chroma Vector Database for Storage and Retrieval**: The session demonstrates how to use Chroma, a popular open-source vector database, to store these generated embeddings along with their source documents. LangChain simplifies this process, allowing database creation and population with a single command: `Chroma.from_documents()`.\n",
    "-   **High-Dimensional Nature of Embeddings**: It's highlighted that the embeddings generated by the OpenAI model used (e.g., `text-embedding-ada-002` or similar) have 1536 dimensions, meaning each text chunk is represented as a point in a 1536-dimensional space.\n",
    "-   **t-SNE for Dimensionality Reduction**: To make these high-dimensional vectors visualizable for humans (who struggle beyond 3D), the t-SNE (T-distributed Stochastic Neighbor Embedding) technique is employed. This method projects the vectors down to 2D or 3D while attempting to preserve the relative similarities and differences between them.\n",
    "-   **Interactive Visualization with Plotly**: The reduced-dimension vectors are plotted using Plotly, creating interactive 2D and 3D scatter plots. In these plots, individual points represent text chunks, are color-coded based on their original document type (e.g., contracts, products, employees, general company information), and feature hover-over text displaying a snippet of the chunk's content.\n",
    "-   **Emergent Semantic Clustering**: A key takeaway from the visualizations is that text chunks with similar semantic content naturally form distinct clusters in the vector space. This clustering occurs based purely on the meaning captured by the embedding model, without it being explicitly told the document categories during the vectorization process.\n",
    "-   **Nuanced Relationships Revealed by Proximity**: The visualizations also reveal finer-grained semantic relationships. For instance, text chunks from \"contracts\" that specifically describe product features are shown to be located closer to other \"product\"-related chunks in the vector space, indicating that the embeddings capture this shared subject matter.\n",
    "-   **Centrality of General or Broad-Topic Documents**: Documents with more general information about the company tend to appear more centrally in the visualized vector space, potentially acting as conceptual bridges between more specific topical clusters.\n",
    "-   **LangChain Simplifies Complex Workflows**: The lesson underscores how LangChain abstracts significant complexity, enabling operations like embedding generation, vector store population, and data retrieval with concise and high-level commands.\n",
    "-   **Call for Hands-on Experimentation**: Users are strongly encouraged to engage in \"homework\" by adding their own diverse text documents to the vector store and observing where these new vectors land in the visualized space. This experimentation is designed to help build a deeper, more intuitive understanding of how text embeddings represent meaning, which is vital for grasping advanced concepts like RAG.\n",
    "-   **Database Management Practice**: A practical tip demonstrated is the inclusion of code to delete an existing Chroma database directory before re-creating it. This is useful during development and experimentation to ensure a clean state and avoid accumulating duplicate entries.\n",
    "\n",
    "### Conceptual Understanding\n",
    "-   **t-SNE for Visualizing High-Dimensional Embeddings**\n",
    "    1.  **Why is this concept important?** Text embeddings like those from OpenAI models can have hundreds or thousands of dimensions (e.g., 1536 in the lesson). Humans cannot directly perceive or visualize data in such high-dimensional spaces. t-SNE is a dimensionality reduction algorithm that creates a low-dimensional (typically 2D or 3D) \"map\" of the high-dimensional data, primarily for visualization. It arranges points so that similar items in high-dimensional space are represented as nearby points in the low-dimensional map, and dissimilar items are typically further apart.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?** While t-SNE is mainly for visualization and exploratory data analysis, understanding the structure of an embedding space can provide insights into the quality of the embeddings, how well different topics are separated, or whether there are unexpected clusters. This can inform decisions about data preprocessing, model choice, or even identify issues in the source data. It helps build intuition about what the embedding model has \"learned.\"\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Other dimensionality reduction techniques like Principal Component Analysis (PCA) – which is often used as a pre-processing step for t-SNE – UMAP (Uniform Manifold Approximation and Projection), and understanding concepts like manifolds, local vs. global structure preservation, and the \"curse of dimensionality\" are relevant. It's also important to know that t-SNE plot interpretations should be done cautiously; cluster sizes and inter-cluster distances in t-SNE maps don't always have a direct, precise meaning corresponding to the original high-dimensional space.\n",
    "\n",
    "-   **Semantic Properties of Vector Space (Clustering and Proximity)**\n",
    "    1.  **Why is this concept important?** The fundamental idea behind using embeddings is that semantic similarity between texts translates into proximity in the vector space. If two pieces of text mean similar things, their vector representations will be close together; if they are dissimilar, their vectors will be far apart. This property is what enables semantic search, clustering, and other meaning-based operations.\n",
    "    2.  **How does it connect to real-world tasks, problems, or applications?**\n",
    "        * **Semantic Search:** Find documents most similar to a query by finding the nearest vectors.\n",
    "        * **Clustering:** Group similar documents together by identifying dense regions in the vector space.\n",
    "        * **Recommendation Systems:** Recommend items similar to what a user has liked by finding items close in the embedding space.\n",
    "        * **Anomaly Detection:** Identify documents that are far from any known clusters.\n",
    "        The observed clustering of document types (contracts, products, etc.) in the lesson's visualization, without the model being told these types, directly demonstrates this powerful semantic property.\n",
    "    3.  **Which related techniques or areas should be studied alongside this concept?** Cosine similarity and Euclidean distance (common metrics for measuring vector proximity), k-Nearest Neighbors (k-NN) algorithm, clustering algorithms (e.g., K-Means, DBSCAN), and the broader field of distributional semantics are all highly relevant.\n",
    "\n",
    "### Code Examples\n",
    "The lesson describes and implies the use of the following Python code snippets within a JupyterLab environment:\n",
    "\n",
    "1.  **Key Imports**:\n",
    "    ```python\n",
    "    # For embeddings and vector store\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "    from langchain_community.vectorstores import Chroma # Path may vary based on LangChain version\n",
    "\n",
    "    # For visualization\n",
    "    from sklearn.manifold import TSNE\n",
    "    import plotly.graph_objects as go\n",
    "    import numpy as np # For array manipulation\n",
    "    # (Also assumes loaders, text splitters, os, shutil for DB management are imported)\n",
    "    ```\n",
    "\n",
    "2.  **Initializing Embeddings and Loading Data (Conceptual)**:\n",
    "    ```python\n",
    "    # embeddings_model = OpenAIEmbeddings()\n",
    "    # # ... code to load documents and split into 'chunks' ...\n",
    "    # # num_chunks = len(chunks) # e.g., 123\n",
    "    ```\n",
    "\n",
    "3.  **Managing and Creating Chroma Vector Database**:\n",
    "    ```python\n",
    "    # import shutil\n",
    "    # db_name = \"vector_db\" # Or some constant\n",
    "    # if os.path.exists(db_name):\n",
    "    #     shutil.rmtree(db_name) # Delete if exists\n",
    "\n",
    "    # vector_store = Chroma.from_documents(\n",
    "    #     documents=chunks,\n",
    "    #     embedding=embeddings_model,\n",
    "    #     persist_directory=db_name\n",
    "    # )\n",
    "    # print(vector_store._collection.count()) # Should match num_chunks\n",
    "    ```\n",
    "\n",
    "4.  **Getting a Sample Embedding and its Dimensions**:\n",
    "    ```python\n",
    "    # sample = vector_store._collection.get(limit=1, include=['embeddings'])\n",
    "    # sample_embedding_vector = sample['embeddings'][0]\n",
    "    # print(len(sample_embedding_vector)) # e.g., 1536\n",
    "    # # print(sample_embedding_vector) # Prints the list of numbers\n",
    "    ```\n",
    "\n",
    "5.  **Preparing Data for Visualization**:\n",
    "    ```python\n",
    "    # all_data = vector_store._collection.get(include=['embeddings', 'documents', 'metadatas'])\n",
    "    # embeddings_array = np.array(all_data['embeddings'])\n",
    "    # metadatas_list = all_data['metadatas'] # List of dicts, e.g., [{'doc_type': 'employee'}, ...]\n",
    "    # documents_list = all_data['documents']\n",
    "\n",
    "    # # Example: Extracting document types for coloring\n",
    "    # doc_types = [metadata.get('doc_type', 'unknown') for metadata in metadatas_list]\n",
    "    # unique_doc_types = sorted(list(set(doc_types)))\n",
    "    # color_map = {dtype: i for i, dtype in enumerate(unique_doc_types)} # Map type to a number for color scale\n",
    "    # colors_for_plot = [color_map[dt] for dt in doc_types]\n",
    "    ```\n",
    "\n",
    "6.  **t-SNE Dimensionality Reduction**:\n",
    "    ```python\n",
    "    # For 2D\n",
    "    # tsne_2d = TSNE(n_components=2, random_state=42) # random_state for reproducibility\n",
    "    # reduced_vectors_2d = tsne_2d.fit_transform(embeddings_array)\n",
    "\n",
    "    # For 3D\n",
    "    # tsne_3d = TSNE(n_components=3, random_state=42)\n",
    "    # reduced_vectors_3d = tsne_3d.fit_transform(embeddings_array)\n",
    "    ```\n",
    "\n",
    "7.  **Plotly Visualization (Conceptual Structure for 2D)**:\n",
    "    ```python\n",
    "    # hover_texts = [doc[:100] + \"...\" for doc in documents_list] # First 100 chars for hover\n",
    "\n",
    "    # fig_2d = go.Figure(data=[go.Scatter(\n",
    "    #     x=reduced_vectors_2d[:, 0],\n",
    "    #     y=reduced_vectors_2d[:, 1],\n",
    "    #     mode='markers',\n",
    "    #     marker=dict(\n",
    "    #         color=colors_for_plot, # Apply colors based on doc_type\n",
    "    #         colorscale='Viridis', # Example color scale\n",
    "    #         showscale=True\n",
    "    #     ),\n",
    "    #     text=hover_texts, # Text to show on hover\n",
    "    #     hoverinfo='text'\n",
    "    # )])\n",
    "    # fig_2d.update_layout(title='2D t-SNE visualization of text embeddings')\n",
    "    # fig_2d.show()\n",
    "    ```\n",
    "\n",
    "8.  **Plotly Visualization (Conceptual Structure for 3D)**:\n",
    "    ```python\n",
    "    # fig_3d = go.Figure(data=[go.Scatter3d(\n",
    "    #     x=reduced_vectors_3d[:, 0],\n",
    "    #     y=reduced_vectors_3d[:, 1],\n",
    "    #     z=reduced_vectors_3d[:, 2],\n",
    "    #     mode='markers',\n",
    "    #     marker=dict(\n",
    "    #         color=colors_for_plot,\n",
    "    #         colorscale='Viridis',\n",
    "    #         showscale=True,\n",
    "    #         size=5 # Example marker size\n",
    "    #     ),\n",
    "    #     text=hover_texts,\n",
    "    #     hoverinfo='text'\n",
    "    # )])\n",
    "    # fig_3d.update_layout(title='3D t-SNE visualization of text embeddings', scene=dict(aspectmode='cube'))\n",
    "    # fig_3d.show()\n",
    "    ```\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Interpretation:** What does the observed clustering of same-colored points (representing text chunks from the same original document type like \"employee\" or \"product\") in the t-SNE visualization signify about the OpenAI embedding model's ability to understand text, especially since it wasn't told these types during embedding?\n",
    "    -   *Answer:* The clustering signifies that the OpenAI embedding model, based purely on the textual content of the chunks, is able to identify and group semantically similar information. The fact that chunks from \"employee\" documents cluster together implies the language used in those documents (terms, phrases, overall meaning) is distinct from, say, \"product\" documents, and the embedding model captures these distinctions as geometric proximity in the high-dimensional vector space.\n",
    "2.  **Practical Implication:** If two different text chunks are found to be very close to each other in the 1536-dimensional vector space (and consequently, also appear close in the 2D/3D t-SNE projection), what can you infer about their content, and how might this be useful in an application like semantic search?\n",
    "    -   *Answer:* You can infer that the two text chunks have very similar semantic meaning or discuss closely related topics, even if they don't use the exact same keywords. In semantic search, if a user's query vector is close to the vectors of these chunks, it means these chunks are highly relevant to the query, allowing the system to retrieve them as top search results, providing more accurate and contextually appropriate answers than simple keyword matching.\n",
    "3.  **Experimentation:** If you added a new text chunk describing \"annual performance review guidelines for marketing staff\" to the existing vector database, where might you expect its vector to appear in the 2D t-SNE visualization relative to the existing \"employee\" (green), \"product\" (blue), and \"company/about\" (yellow) clusters, and why?\n",
    "    -   *Answer:* You would expect this new chunk to appear within or very close to the \"employee\" (green) cluster. This is because its content (\"performance review guidelines,\" \"staff\") is semantically most related to employee information. It might be slightly pulled towards the \"company/about\" (yellow) cluster if it contains general company policies, or even have some affinity towards a \"contracts\" cluster if it discusses formal processes, but its primary association would be with employee-related topics.\n",
    "\n",
    "# Day 3 - Building RAG Pipelines: From Vectors to Embeddings with LangChain\n",
    "\n",
    "This lesson serves as a reflective summary of the hands-on work with text embeddings and vector databases, emphasizing LangChain's efficiency in these tasks and encouraging further user experimentation. It recaps how easily OpenAI embeddings were generated and stored in Chroma, highlights the possibility of trying other vector stores like FAISS with minimal code changes, and suggests exploring different data granularities (whole documents vs. chunks). The session concludes by setting the stage for the next major step: leveraging these concepts to build a complete Retrieval Augmented Generation (RAG) pipeline, which will include conversational memory.\n",
    "\n",
    "### Highlights\n",
    "-   **LangChain's Efficiency in Vector Operations**: A key takeaway is the power and simplicity LangChain offers. Complex tasks like initializing an embedding model (e.g., `embeddings = OpenAIEmbeddings()`) and creating/populating a vector database (e.g., `Chroma.from_documents(...)`) are accomplished with very few lines of code.\n",
    "-   **Experimentation with Different Vector Stores**: Users are encouraged to try alternative vector data stores. For example, **FAISS** (Facebook AI Similarity Search), an in-memory vector store, can be swapped in with only minor modifications to the existing code used for Chroma, demonstrating LangChain's consistent interface.\n",
    "-   **Exploring Embedding Granularity**: A suggestion for further learning is to experiment by vectorizing **entire documents** instead of the smaller text chunks used in the lesson. This would allow observation of how different levels of data granularity affect the resulting vector representations and their separation in visualizations.\n",
    "-   **Core Components Recap**: The lesson revisits the essential components for vector storage: an **embedding object** (like `OpenAIEmbeddings` instance) and the **documents or chunks** to be vectorized. These, along with a **database name/directory**, are the primary inputs to commands like `Chroma.from_documents`.\n",
    "-   **Consistency Across Vector Stores (with same embeddings)**: An important point made is that if the same embedding model (e.g., OpenAI Embeddings) is used, the semantic relationships and consistency of the vector representations should be maintained even if a different vector database (like FAISS) is employed.\n",
    "-   **Transition to Building a RAG Pipeline**: This lesson acts as a crucial stepping stone towards constructing a full **Retrieval Augmented Generation (RAG)** solution in the subsequent sessions. The understanding of embeddings and vector stores is foundational for building systems that can retrieve relevant information to augment LLM responses. This upcoming RAG system will also incorporate conversational chains and memory.\n",
    "\n",
    "### Code Examples\n",
    "The lesson refers to key lines of code from the previous hands-on session, emphasizing their conciseness:\n",
    "\n",
    "1.  **Initializing the Embedding Model**:\n",
    "    ```python\n",
    "    # from langchain_openai import OpenAIEmbeddings\n",
    "    # embeddings = OpenAIEmbeddings()\n",
    "    ```\n",
    "    This single line gives access to OpenAI's API for calculating embedding vectors.\n",
    "\n",
    "2.  **Creating and Populating the Chroma Vector Database**:\n",
    "    ```python\n",
    "    # from langchain_community.vectorstores import Chroma # Path may vary\n",
    "    # # Assuming 'chunks' is a list of Document objects,\n",
    "    # # 'embeddings' is the OpenAIEmbeddings instance,\n",
    "    # # and 'db_name' is a string for the directory name (e.g., \"vector_db\")\n",
    "    #\n",
    "    # vector_store = Chroma.from_documents(\n",
    "    #     documents=chunks, # Or 'documents' if vectorizing whole docs\n",
    "    #     embedding=embeddings,\n",
    "    #     persist_directory=db_name\n",
    "    # )\n",
    "    ```\n",
    "    This single line handles the vectorization of all chunks and their storage in the Chroma database.\n",
    "\n",
    "### Reflective Questions\n",
    "1.  **Adaptability:** How does LangChain's design, which facilitates easily swapping out components like vector databases (e.g., Chroma for FAISS) with minimal code alterations, benefit developers when building and iterating on AI applications?\n",
    "    -   *Answer:* This design provides immense flexibility and reduces vendor lock-in. Developers can experiment with different vector stores to find the one that best suits their application's performance, cost, or deployment needs without having to rewrite significant portions of their codebase, thereby accelerating development and iteration cycles.\n",
    "2.  **Granularity Impact:** What differences might you anticipate in a t-SNE visualization if you vectorized entire documents instead of smaller text chunks from those documents? Why might one approach be preferred over the other depending on the specific application?\n",
    "    -   *Answer:* Vectorizing entire documents would result in fewer vectors, each representing a broader range of information. In a t-SNE plot, these document vectors might still cluster by overall topic, but the fine-grained distinctions visible with chunks might be lost. Smaller chunks are often preferred for RAG because they provide more precise, targeted context to the LLM, whereas whole-document embeddings might be useful for tasks like document classification or finding broadly similar documents."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
