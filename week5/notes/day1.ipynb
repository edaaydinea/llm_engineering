{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea7afeb7",
   "metadata": {},
   "source": [
    "# Day 1 - RAG Fundamentals: Leveraging External Data to Improve LLM Responses\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This text introduces Retrieval Augmented Generation (RAG), a technique designed to significantly enhance Large Language Model (LLM) performance by dynamically incorporating specific, relevant information from external knowledge bases into prompts. The core purpose is to make LLM responses more accurate, context-aware, and grounded in factual or proprietary data, which is crucial for real-world data science applications such as building specialized AI assistants for enterprises or enabling queries over private datasets.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- **Core Principle of RAG**: RAG improves LLM outputs by enriching prompts with targeted data retrieved from an external knowledge base in real-time. This addresses LLM limitations like knowledge cut-offs and inability to access private data, making them more reliable for tasks requiring factual, up-to-date, or proprietary information.\n",
    "- **The \"Small Idea\" - Simple Implementation**: The foundational concept involves a user query triggering a search in a local knowledge base; any relevant information found is then directly inserted into the LLM prompt to provide immediate context. This allows data scientists to quickly prototype systems like Q&A bots for custom document sets.\n",
    "- **Knowledge Base Integration**: RAG systems leverage \"knowledge bases\" (e.g., company documents, databases, shared drives) to supply external information. This enables LLMs to utilize domain-specific or private data not present in their original training, essential for enterprise applications like internal search or tailored customer support.\n",
    "- **Dynamic Contextualization**: Unlike static prompt enrichment methods (e.g., few-shot examples or system prompts), RAG actively retrieves *new*, query-specific information from external sources for each interaction. This dynamic retrieval is highly effective for tasks needing current or very specific knowledge not easily embedded in fixed prompts.\n",
    "- **Building AI Knowledge Workers**: RAG can power \"AI knowledge workers\"—systems capable of answering questions and performing analysis based on an organization's internal documents. This is directly applicable to data science workflows by automating information retrieval and initial analysis, thus boosting productivity.\n",
    "- **Illustrative \"Toy\" Implementation**: The text describes a basic RAG implementation where, if an employee's name appears in a query, their entire record is \"stuffed\" into the prompt. This brute-force method serves to demonstrate the fundamental mechanics and immediate benefits of RAG without complex components.\n",
    "\n",
    "### **Conceptual Understanding**\n",
    "\n",
    "- **Core Principle of RAG**\n",
    "    1. **Why is this concept important?** It directly addresses fundamental LLM limitations such as knowledge cutoffs (models don't know about events past their training date), potential for hallucination (generating incorrect information), and lack of access to private or real-time data. By grounding the LLM with relevant, external facts for each query, RAG makes LLMs more factual, trustworthy, and useful for specific, real-world tasks.\n",
    "    2. **How does it connect to real-world tasks, problems, or applications?** This is vital for building chatbots that can accurately answer questions about internal company policies, summarize recent research papers not included in the LLM's training set, or provide customer support based on the latest product manuals. For example, an insurance company could use RAG to allow an LLM to answer customer queries about specific policy details by retrieving information from their internal policy documents.\n",
    "    3. **Which related techniques or areas should be studied alongside this concept?** To fully leverage RAG, one should explore vector databases (e.g., Pinecone, Weaviate) for efficient similarity search, embedding models (e.g., Sentence-BERT, OpenAI embeddings) to convert text into searchable vector representations, advanced prompt engineering techniques to best integrate retrieved context, and information retrieval (IR) principles. Understanding document chunking strategies is also key for preparing the knowledge base.\n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "1. **Application:** Which specific dataset or project could benefit from this \"small idea\" of RAG? Provide a one-sentence explanation.\n",
    "    - *Answer:* A project requiring an internal helpdesk bot to answer employee questions about HR policies could use this RAG approach by querying a knowledge base of HR documents and providing specific policy excerpts to the LLM for generating answers.\n",
    "2. **Teaching:** How would you explain the \"small idea\" of RAG to a junior colleague, using one concrete example? Keep the answer under two sentences.\n",
    "    - *Answer:* Think of RAG as giving an LLM (like a smart assistant) quick access to a specific cheat sheet before it answers a question; for instance, if asked about \"Project X,\" we first find the \"Project X summary document\" and give it to the LLM along with the question, so it gives a relevant, informed answer based on that document.\n",
    "\n",
    "# Day 1 - Building a DIY RAG System: Implementing Retrieval-Augmented Generation\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This text provides a step-by-step walkthrough of creating a \"Do-It-Yourself\" (DIY) Retrieval Augmented Generation (RAG) system using Python. It demonstrates how to load a local knowledge base of fictitious company documents (for \"InsureLM\") into a simple dictionary structure and then use basic keyword string matching to retrieve relevant documents based on user queries. This retrieved context is then injected into the prompt sent to an LLM (like GPT-4 Mini) via a Gradio interface, showcasing RAG's ability to provide more accurate, context-aware answers but also highlighting the inherent brittleness and lack of scalability of such a simplistic retrieval approach.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- **Practical DIY RAG Insight:** The primary goal is to offer hands-on understanding of RAG's fundamental mechanics by building a very basic version. This helps data science students grasp the core concept of augmenting LLM inputs with external data before tackling more sophisticated implementations.\n",
    "- **Structured Local Knowledge Base:** The system utilizes a \"Knowledge Base\" folder containing markdown documents organized into subfolders like `employees` and `products`. This simulates how real-world RAG systems might access proprietary information from company shared drives or document management systems.\n",
    "- **Simple Context Indexing:** Employee HR records and product descriptions are loaded into a Python dictionary (`context_dictionary`). The keys are derived from filenames (e.g., employee last names, product names), and the values are the full text content of these documents, serving as a rudimentary searchable index.\n",
    "- **Keyword-Based Retrieval Logic:** A Python function (`get_relevant_context`) performs context retrieval by iterating through the dictionary keys (e.g., \"Lancaster\", \"CarLM\") and checking if these literal strings appear anywhere in the user's input message. This is a direct but limited form of information retrieval.\n",
    "- **Dynamic Prompt Augmentation:** If relevant documents are found via keyword matching, their content is prepended to the original user query with an introductory phrase (e.g., \"The following additional context might be relevant...\"). This directly feeds the LLM specific information related to the query.\n",
    "- **System Prompt for Factual Grounding:** The LLM is guided by a system prompt that instructs it to act as an expert on \"InsureLM,\" provide brief and accurate answers, and explicitly state if it doesn't know an answer or lacks context, rather than fabricating information. This is emphasized as an effective method to curb LLM hallucinations.\n",
    "- **Interactive Testing with Gradio:** A Gradio web interface is quickly set up to allow interactive querying of the RAG system. This provides immediate visual feedback on how the retrieved context influences the LLM's responses to questions like \"Who is Avery Lancaster?\"\n",
    "- **Demonstrated Utility and Limitations:** The system successfully answers questions accurately when exact keywords are used (e.g., \"Avery Lancaster\" or \"CarLM\"), proving the benefit of contextual information. However, it fails with variations like using only a first name (\"Avery\"), incorrect capitalization (\"lancaster\"), or semantically similar but lexically different queries, exposing its brittleness.\n",
    "- **Scalability and Flexibility Issues:** The approach of loading all data into an in-memory dictionary and relying on naive string search is not scalable for large datasets and lacks the flexibility of advanced search techniques like semantic search.\n",
    "- **Educational Stepping Stone:** This exercise establishes a foundational understanding of the RAG pipeline (retrieve, augment, generate). It highlights the need for more robust retrieval mechanisms, which are to be covered in subsequent lessons.\n",
    "- **Fictitious Data Generation:** The knowledge base content was generated by LLMs (GPT-4, Claude), illustrating a modern way to create realistic-looking datasets for development and testing purposes.\n",
    "\n",
    "### **Conceptual Understanding**\n",
    "\n",
    "- **Keyword-Based Context Retrieval**\n",
    "    1. **Why is this concept important?** It's the most elementary form of the \"retrieval\" step in RAG, making the connection between a user's query and the knowledge base tangible and easy to understand. By seeing its directness and its flaws, students can better appreciate the necessity and complexity of more advanced retrieval methods.\n",
    "    2. **How does it connect to real-world tasks, problems, or applications?** While too rudimentary for most robust, scalable applications, this method is analogous to the basic \"Ctrl+F\" find functionality within a document or a very simple search feature in a small, highly controlled dataset. It helps illustrate the core challenge in information retrieval: accurately matching user intent (the query) with available data. For example, it might be a first pass in a multi-stage retrieval process or adequate for a small, internal list of commands where users know the exact terms.\n",
    "    3. **Which related techniques or areas should be studied alongside this concept?** Understanding the limitations of keyword-based retrieval directly motivates the study of more sophisticated techniques:\n",
    "        - **Lexical Search:** Algorithms like TF-IDF or BM25 that consider term frequency and document rarity.\n",
    "        - **Semantic Search:** Using word/sentence embeddings (from models like BERT, USE, or OpenAI embeddings) to find documents that are semantically similar, not just lexically identical. This requires learning about vector databases (e.g., FAISS, Pinecone, ChromaDB) for efficient similarity searching.\n",
    "        - **NLP Preprocessing:** Techniques such as case normalization, stemming, lemmatization, and stop-word removal to make keyword matching more resilient to minor variations.\n",
    "        - **Query Expansion/Rewriting:** Modifying the user's query to include synonyms or related terms to improve recall.\n",
    "\n",
    "### **Code Examples**\n",
    "\n",
    "The implementation involves several key Python components:\n",
    "\n",
    "1. Loading Knowledge Base into a Dictionary:\n",
    "    \n",
    "    The code iterates through files in specified subdirectories (e.g., Knowledge Base/employees, Knowledge Base/products). For each file:\n",
    "    \n",
    "    - It extracts a key (e.g., employee's last name or product name, often derived from the filename).\n",
    "    - It reads the entire content of the file.\n",
    "    - It stores this as a key-value pair in a dictionary named `context_dictionary`.\n",
    "    \n",
    "    ```python\n",
    "    # Pseudocode for loading data\n",
    "    context_dictionary = {}\n",
    "    # For employees\n",
    "    employee_files = list_files_in_directory(\"Knowledge Base/employees\")\n",
    "    for file_path in employee_files:\n",
    "        employee_last_name = extract_key_from_filename(file_path) # e.g., \"Lancaster\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "        context_dictionary[employee_last_name] = content\n",
    "    \n",
    "    # Similar logic for products\n",
    "    product_files = list_files_in_directory(\"Knowledge Base/products\")\n",
    "    for file_path in product_files:\n",
    "        product_name = extract_key_from_filename(file_path) # e.g., \"CarLM\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            content = f.read()\n",
    "        context_dictionary[product_name] = content\n",
    "    \n",
    "    ```\n",
    "    \n",
    "2. get_relevant_context(message, context_dictionary) Function:\n",
    "    \n",
    "    This function implements the brittle keyword search.\n",
    "    \n",
    "    ```python\n",
    "    # Pseudocode for get_relevant_context\n",
    "    def get_relevant_context(user_message, current_context_dictionary):\n",
    "        relevant_docs = []\n",
    "        for key, document_content in current_context_dictionary.items():\n",
    "            if key in user_message: # Simple substring match\n",
    "                relevant_docs.append(document_content)\n",
    "        return relevant_docs\n",
    "    \n",
    "    ```\n",
    "    \n",
    "    *Self-correction:* The transcript mentions checking \"if that text, like the word Lancaster, is anywhere in the message\".\n",
    "    \n",
    "3. add_context(message) Function:\n",
    "    \n",
    "    This function augments the user's message with the retrieved context.\n",
    "    \n",
    "    ```python\n",
    "    # Pseudocode for add_context\n",
    "    def add_context_to_message(user_message, current_context_dictionary):\n",
    "        retrieved_context_list = get_relevant_context(user_message, current_context_dictionary)\n",
    "        if retrieved_context_list:\n",
    "            context_string = \"\\n\\n\".join(retrieved_context_list)\n",
    "            augmented_message = f\"{user_message}\\n\\nThe following additional context might be relevant in answering this question:\\n{context_string}\"\n",
    "            return augmented_message\n",
    "        return user_message\n",
    "    \n",
    "    ```\n",
    "    \n",
    "    *Self-correction:* The example shows the original question first, then the context. The text states: \"it's going to add that into the message. It's going to say the following additional context might be relevant...\". The example output is: \"Who is Avery Lancaster? The following additional context might be relevant... [details]\". My pseudocode needs slight adjustment if the `user_message` is part of the `augmented_message`. The transcript implies the function *returns* the augmented message, and the example `Who is Avery Lancaster?` is the input to `add_context`, and the output is the augmented string. The provided video example output for `add_context` was `Who is Avery Lancaster? The following additional context might be relevant... [details]`. So, the original message seems to be prepended, or the `get_relevant_context` is called, and then the result is formatted into a string that is later added to the prompt. The `chat` function description says: \"We added extra context to our message, and that's what we send to OpenAI.\" This suggests the message is modified before being put into the chat history. The example call `add_context(question)` indeed shows it returns the full augmented string.\n",
    "    The example `print(add_context(\"Who is Avery Lancaster?\"))` outputs:\n",
    "    `Who is Avery Lancaster?The following additional context might be relevant in answering this question:[HR document for Avery Lancaster]`\n",
    "    \n",
    "    Corrected structure for `add_context`:\n",
    "    \n",
    "    ```python\n",
    "    # Pseudocode for add_context based on example output\n",
    "    def add_context_to_message(user_message_input, current_context_dictionary):\n",
    "        retrieved_context_list = get_relevant_context(user_message_input, current_context_dictionary)\n",
    "        if retrieved_context_list:\n",
    "            context_string = \"\\n\\n\".join(retrieved_context_list)\n",
    "            # The original message is implicitly part of the LLM's final prompt,\n",
    "            # this function primarily prepares the context string to be added.\n",
    "            # Or, it constructs the full message for the LLM.\n",
    "            # Based on \"added extra context TO our message\", it modifies the user message.\n",
    "            augmented_message = f\"{user_message_input}\\n\\nThe following additional context might be relevant in answering this question:\\n{context_string}\"\n",
    "            return augmented_message\n",
    "        return user_message_input # Return original if no context found\n",
    "    \n",
    "    ```\n",
    "    \n",
    "4. Chat Function for Gradio `(chat(message, history))`:\n",
    "    \n",
    "    This function orchestrates the interaction with the OpenAI API.\n",
    "    \n",
    "    ```python\n",
    "    # Pseudocode for Gradio chat function\n",
    "    import openai\n",
    "    \n",
    "    SYSTEM_MESSAGE = \"You are an expert in answering accurate questions about InsureLM...\" # as defined\n",
    "    \n",
    "    def chat_with_rag(user_input_message, chat_history):\n",
    "        # 1. Format chat_history for OpenAI API\n",
    "        formatted_history = []\n",
    "        for human_msg, ai_msg in chat_history:\n",
    "            formatted_history.append({\"role\": \"user\", \"content\": human_msg})\n",
    "            formatted_history.append({\"role\": \"assistant\", \"content\": ai_msg})\n",
    "    \n",
    "        # 2. Augment the current user message with context\n",
    "        # The text says \"We added extra context to our message\" implying the `user_input_message` passed to API call is augmented.\n",
    "        message_with_context = add_context_to_message(user_input_message, context_dictionary)\n",
    "    \n",
    "        # 3. Prepare messages for OpenAI API\n",
    "        messages_for_api = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_MESSAGE}\n",
    "        ] + formatted_history + [\n",
    "            {\"role\": \"user\", \"content\": message_with_context}\n",
    "        ]\n",
    "    \n",
    "        # 4. Call OpenAI API\n",
    "        response_stream = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4-turbo\", # Or \"gpt-3.5-turbo\" / \"gpt-4-mini\" as mentioned earlier\n",
    "            messages=messages_for_api,\n",
    "            stream=True\n",
    "        )\n",
    "    \n",
    "        # 5. Stream back the response\n",
    "        ai_response_content = \"\"\n",
    "        for chunk in response_stream:\n",
    "            # process chunk and yield for Gradio streaming\n",
    "            content_part = chunk.choices[0].delta.get(\"content\", \"\")\n",
    "            ai_response_content += content_part\n",
    "            yield ai_response_content\n",
    "    \n",
    "    ```\n",
    "    \n",
    "\n",
    "### **Reflective Questions**\n",
    "\n",
    "1. **Application:** Given the brittleness of this DIY RAG's keyword matching, for what specific small-scale, controlled dataset could it still be reasonably effective? Provide a one-sentence explanation.\n",
    "    - *Answer:* This DIY RAG could be reasonably effective for querying a small, internal company glossary of well-defined acronyms or product codes, where users are expected to input these exact terms and the knowledge base consists of their definitions or brief descriptions.\n",
    "2. **Teaching:** How would you explain to a non-technical colleague why simply matching keywords (like \"Lancaster\") from their question to document names is not enough for a robust company-wide Q&amp;A system? Use a concrete example.\n",
    "    - *Answer:* If an employee asks, \"Tell me about our CEO's start date,\" our simple keyword system might fail if they don't use the CEO's last name \"Lancaster,\" because it can't understand that \"CEO\" refers to her; a smarter system would recognize that connection even without the exact name, or if they misspelled \"Lancaster.\"\n",
    "3. **Extension:** What is the most immediate next step you would take to improve the \"brittle\" context retrieval demonstrated in this exercise, and why?\n",
    "    - *Answer:* The most immediate improvement would be to make the keyword matching case-insensitive and potentially add basic stemming (e.g., \"products\" matches \"product\"), because this would handle common minor variations in user queries with relatively little implementation effort, making retrieval slightly more flexible.\n",
    "\n",
    "# Day 1 - Understanding Vector Embeddings: The Key to RAG and LLM Retrieval\n",
    "\n",
    "### Summary\n",
    "\n",
    "This text introduces **vector embeddings** as the pivotal concept for building sophisticated Retrieval Augmented Generation (RAG) systems. It explains that **autoencoding LLMs** (like BERT or OpenAI embeddings) convert text into numerical vectors where proximity in the high-dimensional vector space signifies semantic similarity. This \"big idea\" behind RAG involves transforming user queries into vectors and then using these to find and retrieve the most semantically similar documents from a **vector data store** (a knowledge base containing text and corresponding embeddings), thus providing highly relevant context to a generative LLM for more accurate and nuanced answers.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- **Autoregressive vs. Autoencoding LLMs**: The text differentiates **autoregressive LLMs** (e.g., GPT-4, Claude), which predict the next token in a sequence and are used for generation, from **autoencoding LLMs** (e.g., BERT, OpenAI embeddings), which process an entire input to create a fixed-size representation. Autoencoding models are key for generating vector embeddings.\n",
    "- **Vector Embeddings Defined**: A vector embedding is a dense numerical representation (a list of numbers) that captures the semantic meaning of a piece of text—be it a character, token, word, sentence, paragraph, entire document, or even an abstract concept. This allows textual meaning to be processed mathematically in a high-dimensional space.\n",
    "- **High-Dimensional Meaning Space**: These embeddings typically consist of hundreds or thousands of numbers, positioning the text's meaning as a point in a high-dimensional vector space. The crucial property is that closeness in this space correlates with similarity in meaning.\n",
    "- **Semantic Similarity through Proximity**: Text segments with similar meanings will be mapped to vector embeddings that are close to each other in this vector space. This principle allows retrieval of relevant information even if the query and document use different phrasing but convey similar concepts.\n",
    "- **Vector Math and Analogies**: Vector embeddings can capture intricate semantic relationships, famously illustrated by the `vector('King') - vector('Man') + vector('Woman') ≈ vector('Queen')` example. This demonstrates that these vectors encode deep relational aspects of meaning.\n",
    "- **The \"Big Idea\" of RAG: Semantic Retrieval**: The advanced RAG approach leverages vector embeddings for information retrieval. A user's query is first converted into a query vector. This vector is then used to search a specialized **vector data store** to find document vectors (and their associated texts) that are most semantically similar.\n",
    "- **Vector Data Store (Vector Database)**: This is an enhanced knowledge base that stores not only the raw text of documents but also their pre-computed vector embeddings. These databases are optimized for efficient similarity searches among high-dimensional vectors.\n",
    "- **Upgraded RAG Workflow**: The process using vectors is: 1. User submits a query. 2. The query is converted into a vector embedding (vectorized). 3. The vector data store is searched for document vectors closest to the query vector. 4. The original text of these top-matching documents is retrieved. 5. This text is added to the prompt for a generative LLM. 6. The LLM generates a response using this semantically relevant context.\n",
    "- **Superiority over Keyword Matching**: Employing vector embeddings for retrieval is significantly more robust and flexible than the brittle keyword-based matching used in simpler RAG systems. It focuses on understanding the *meaning* and *intent* behind text rather than relying on exact string occurrences.\n",
    "- **Role of Encoding LLMs in RAG**: Specific models (often autoencoders like OpenAI's embedding models) serve as \"encoding LLMs.\" Their role is to generate the vector representations for all documents in the knowledge base (an offline process called indexing) and for incoming user queries (a real-time process).\n",
    "- **LangChain for Future Implementation**: The text concludes by noting that **LangChain**, a popular framework designed to simplify the development of LLM-powered applications like RAG, will be used in future sessions to practically implement these vector-based RAG systems.\n",
    "\n",
    "### Conceptual Understanding\n",
    "\n",
    "- **Semantic Similarity via Vector Embeddings**\n",
    "    1. **Why is this concept important?** It's the core mechanism that enables advanced RAG systems to overcome the limitations of literal keyword searching. By representing text in a way that captures meaning, the system can understand the *intent* or *concept* behind a user's query and retrieve documents that are relevant even if they use different vocabulary. This leads to more comprehensive context being fed to the generative LLM, resulting in higher quality, more accurate, and more nuanced answers.\n",
    "    2. **How does it connect to real-world tasks, problems, or applications?** This capability is critical for building:\n",
    "        - **Intelligent search systems** over private or specialized document collections (e.g., internal company wikis, libraries of research papers, legal case files).\n",
    "        - **Sophisticated Q&A chatbots** that can understand complex or ambiguously worded questions and provide relevant answers drawn from a knowledge base.\n",
    "        - **Content recommendation engines** that suggest articles, products, or media based on conceptual similarity to a user's interests or past behavior.\n",
    "        - For example, in a healthcare context, a doctor querying \"symptoms of early-onset cardiac distress in young adults\" could retrieve relevant medical literature even if the documents use terms like \"myocardial infarction precursors in individuals under 40\" because the underlying semantic meaning is similar.\n",
    "    3. **Which related techniques or areas should be studied alongside this concept?**\n",
    "        - **Embedding Models:** A deeper understanding of models that generate embeddings, such as Word2Vec, GloVe, FastText, and particularly transformer-based sentence encoders like Sentence-BERT (SBERT), Universal Sentence Encoder (USE), and those offered by OpenAI.\n",
    "        - **Distance/Similarity Metrics:** Learning about how \"closeness\" or \"similarity\" between vectors is mathematically quantified, primarily using **Cosine Similarity**, but also Euclidean Distance, Manhattan Distance, etc.\n",
    "        - **Vector Databases:** Exploring technologies specifically designed for efficient storage, indexing (e.g., using algorithms like HNSW, IVFADC), and querying of large volumes of high-dimensional vectors. Examples include Pinecone, Weaviate, Milvus, Chroma, Qdrant, and FAISS.\n",
    "        - **Natural Language Processing (NLP) Fundamentals:** Concepts like tokenization, text preprocessing, and understanding the architecture of transformer models are foundational.\n",
    "        - **Dimensionality Reduction (for analysis/visualization):** Techniques like PCA (Principal Component Analysis) and t-SNE (t-distributed Stochastic Neighbor Embedding) can be useful for visualizing high-dimensional embeddings in 2D or 3D to gain intuition, though they are not typically used directly in the retrieval pipeline itself for top-tier performance.\n",
    "\n",
    "### Reflective Questions\n",
    "\n",
    "1. **Application:** Which specific dataset or project could significantly benefit from RAG using vector embeddings compared to the keyword-based RAG previously discussed? Provide a one-sentence explanation.\n",
    "    - *Answer:* A large legal document archive used for case research would significantly benefit, as vector embeddings can identify relevant precedents or clauses based on conceptual legal arguments, even if the specific keywords used in a query don't exactly match the document text.\n",
    "2. **Teaching:** How would you explain the concept of \"semantic similarity\" achieved through vector embeddings to a colleague who only knows about keyword search, using a simple analogy?\n",
    "    - *Answer:* Imagine keyword search is like finding a book in a library by matching the exact title word-for-word; semantic similarity with vectors is like having a knowledgeable librarian who understands the *topic* you're interested in and can find relevant books even if their titles use completely different words but discuss the same underlying concepts.\n",
    "3. **Extension:** Given that vector embeddings represent meaning, what potential ethical consideration or bias should a data scientist be aware of when building a RAG system using pre-trained embedding models?\n",
    "    - *Answer:* Pre-trained embedding models can inadvertently learn and perpetuate societal biases (e.g., gender, racial, or cultural stereotypes) present in their vast training datasets, which could lead the RAG system to surface biased information or rank search results unfairly, necessitating careful model selection, bias auditing, and potentially fine-tuning or debiasing techniques."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
