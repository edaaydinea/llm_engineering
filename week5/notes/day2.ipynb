{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff60bc47",
   "metadata": {},
   "source": [
    "# Day 2 - Unveiling LangChain: Simplify RAG Implementation for LLM Applications\n",
    "\n",
    "### Summary\n",
    "\n",
    "This text introduces LangChain, a framework developed in late 2022 designed to simplify and accelerate the creation of applications powered by Large Language Models (LLMs), particularly for tasks like Retrieval Augmented Generation (RAG). LangChain achieves this by providing tools to \"stitch together\" various components, offering wrappers for different LLM APIs, and will be practically used here to load documents from a knowledge base, add metadata, and strategically break them into smaller chunks suitable for vectorization in a RAG pipeline.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- **LangChain's Core Purpose**: LangChain is a framework aimed at streamlining the development of LLM applications. It facilitates the chaining of different processing steps, simplifying complex workflows like RAG and AI assistant creation, thereby enabling quicker deployment.\n",
    "- **Simplified RAG Implementation**: A key advantage of LangChain is its ability to significantly reduce the coding effort required for RAG systems. It provides abstractions and tools for common RAG tasks such as document loading from various sources, text splitting (chunking), interaction with vector databases, and prompt management.\n",
    "- **LLM API Abstraction**: LangChain acts as a unifying wrapper around diverse LLM APIs (e.g., OpenAI, Claude). This allows developers to write application logic once and then easily switch between LLM providers or specific models with minimal code changes, which is convenient for experimentation and adapting to new models.\n",
    "- **Document Preprocessing for RAG**: The initial practical application of LangChain discussed involves crucial preprocessing steps for RAG: using its tooling to load documents from folders (the knowledge base), enrich them with metadata (like document type), and then divide them into optimized, semantically relevant \"chunks\" of text ready for vector embedding.\n",
    "- **Balancing Benefits and Alternatives**: The text notes that while LangChain provides a \"tremendous head start,\" the evolving maturity and similarity of LLM APIs, along with the rise of custom-built pipelines, means it's one of several viable approaches to building LLM applications today.\n",
    "- **LangChain Expression Language (LCL)**: LangChain includes its own declarative syntax, LCL, for composing chains. However, it also supports direct use through Python code, offering developers flexibility in how they leverage the framework.\n",
    "\n",
    "### Conceptual Understanding\n",
    "\n",
    "- **Document Preprocessing for RAG using LangChain**\n",
    "    1. **Why is this concept important?** The quality of a RAG system's output heavily depends on how well its knowledge base is prepared. Raw documents are often too long, too short, or lack clear structure for effective embedding and retrieval. LangChain offers specialized document loaders (for various file types like PDF, HTML, Markdown) and text splitters (e.g., recursive character splitters, token splitters) to ingest and segment these documents into appropriately sized, meaningful \"chunks.\" Good chunking ensures that the embeddings accurately represent distinct pieces of information and that retrieved context is focused and relevant. Adding metadata during this stage (e.g., source, date, chapter) can also be invaluable for filtering results or providing additional context to the LLM.\n",
    "    2. **How does it connect to real-world tasks, problems, or applications?** In any practical RAG application—be it for an internal knowledge search across company wikis, a customer support bot referencing product manuals, or an academic research tool sifting through papers—the source data is diverse and often messy. LangChain's preprocessing tools automate the ingestion and structuring of this data. For instance, when building a Q&A system over a company's extensive PDF policy documents, LangChain can load these PDFs, split them into logical paragraphs or sections, and tag them, making the information digestible for the subsequent vectorization and retrieval steps.\n",
    "    3. **Which related techniques or areas should be studied alongside this concept?**\n",
    "        - **Text Splitting Strategies:** Explore different methods like fixed-size chunking, recursive character splitting, token-based splitting (considering model-specific token limits), sentence splitting, and more advanced semantic chunking techniques that try to keep related ideas together.\n",
    "        - **Document Loaders:** Familiarize yourself with the variety of document loaders LangChain (and other libraries) offer for different file formats (e.g., `.txt`, `.md`, `.pdf`, `.html`, `.json`, `.csv`, SQL databases, NoSQL databases, Notion, Confluence).\n",
    "        - **Metadata Management:** Understand how to extract or generate useful metadata for each chunk and how this metadata can be stored and later utilized in the retrieval process (e.g., filtering search results based on metadata fields).\n",
    "        - **Chunk Size Optimization:** Learn about the impact of chunk size and overlap on the quality of embeddings, retrieval relevance, and the overall performance of the RAG system. This often involves experimentation.\n",
    "\n",
    "### Reflective Questions\n",
    "\n",
    "1. **Application:** For a startup aiming to quickly build a Q&A bot over their extensive internal documentation (mix of Confluence pages and PDFs), how would LangChain's described capabilities provide a \"tremendous head start\"?\n",
    "    - *Answer:* LangChain would offer a significant head start by providing pre-built document loaders for Confluence and PDFs, along with effective text splitters, enabling the startup to rapidly ingest and chunk their varied documentation into a usable format for a RAG system without needing to develop these ingestion and parsing components from scratch.\n",
    "2. **Teaching:** How would you explain the benefit of LangChain's \"LLM API Abstraction\" to a junior developer who has only used the OpenAI API directly for a project?\n",
    "    - *Answer:* Imagine you've built your project using OpenAI's API, but then your team wants to test if using a model from Claude or another provider might be more cost-effective or perform better for a specific task. LangChain's API abstraction acts like a universal remote; you write your main logic once, and LangChain handles the specific \"button presses\" (API calls) for each different LLM, so you can switch models with minimal code changes instead of rewriting significant parts of your API interaction code.\n",
    "\n",
    "[](https://lh3.googleusercontent.com/a/default-user=s64-c)\n",
    "\n",
    "# Day 2 - LangChain Text Splitter Tutorial: Optimizing Chunks for RAG Systems\n",
    "\n",
    "### Summary\n",
    "\n",
    "This text details a hands-on session with LangChain in a Jupyter notebook, focusing on the initial stages of a Retrieval Augmented Generation (RAG) pipeline: document ingestion and preparation. It demonstrates how to use LangChain's `DirectoryLoader` and `TextLoader` to load documents from various folders in a knowledge base, assign custom metadata like `doctype` to each document, and then employ `CharacterTextSplitter` to break these documents into smaller, overlapping text chunks (e.g., 123 chunks from 31 documents). This process prepares the data for subsequent vectorization and underscores the limitations of simple keyword searching, reinforcing the need for semantic retrieval in advanced RAG systems.\n",
    "\n",
    "### Highlights\n",
    "\n",
    "- **Practical LangChain Implementation**: The session provides a step-by-step walkthrough of using LangChain for crucial RAG preprocessing tasks: loading documents from a structured directory and then splitting them into chunks.\n",
    "- **Document Loading with `DirectoryLoader`**: LangChain's `DirectoryLoader` is utilized to efficiently ingest all files from specified subfolders of a knowledge base, using `TextLoader` internally for individual text/markdown files. This automates the collection of source documents.\n",
    "- **Metadata Assignment**: Custom metadata, specifically a `doctype` (e.g., 'company', 'contracts', 'employees', 'products'), is programmatically added to each loaded LangChain `Document`. This metadata is preserved during chunking and can be used later for filtering or providing context.\n",
    "- **Text Chunking with `CharacterTextSplitter`**: The `CharacterTextSplitter` from LangChain is employed to divide the loaded documents into smaller text segments. This splitter works by character count but aims to respect natural text boundaries.\n",
    "- **Configurable Chunk Parameters (`chunk_size`, `chunk_overlap`)**: The splitting process is controlled by parameters like `chunk_size` (target number of characters per chunk, e.g., 1000) and `chunk_overlap` (number of characters shared between adjacent chunks, e.g., 200). Overlap helps maintain context across chunk boundaries and improves retrieval robustness.\n",
    "- **LangChain `Document` Abstraction**: Both the initially loaded documents and the subsequent chunks are represented as LangChain `Document` objects, which contain the actual `page_content` (text) and associated `metadata` (like file source and custom `doctype`).\n",
    "- **Respecting Sensible Boundaries in Splitting**: It's noted that LangChain splitters attempt to break text at \"sensible boundaries\" (like newlines or spaces), which can lead to chunks slightly exceeding the defined `chunk_size` to preserve textual coherence.\n",
    "- **Demonstrating Limitations of Keyword Search**: The exercise involves searching for keywords (e.g., \"Lancaster,\" \"Avery,\" \"CEO\") within the generated chunks. This illustrates that simple text matching can easily miss relevant information if different terms are used for the same entity, thereby highlighting the need for semantic search capabilities inherent in vector-based RAG.\n",
    "- **Preparation for Vectorization**: The output of this document processing stage is a list of text chunks (as `Document` objects), which are now ready for the next crucial steps in the RAG pipeline: converting them into vector embeddings and storing them in a vector database.\n",
    "- **Emphasis on Hands-On Learning**: The instructor strongly advises learners to actively engage with the provided Jupyter notebooks—running the code, examining variables, and experimenting—to solidify their understanding of LangChain's operations and the concepts of document chunking.\n",
    "\n",
    "### Conceptual Understanding\n",
    "\n",
    "- **Configurable Chunk Size and Overlap in Text Splitting**\n",
    "    1. **Why is this concept important?**\n",
    "        - **Chunk Size**: This parameter dictates the granularity of information that gets embedded and subsequently retrieved. If chunks are too small, the semantic context might be too fragmented for the embedding model to capture rich meaning, or for the LLM to understand the retrieved piece. If chunks are too large, they might contain a mix of relevant and irrelevant information (diluting the context provided to the LLM), exceed the input limits of embedding models, or be too broad for precise answering.\n",
    "        - **Chunk Overlap**: This is crucial for ensuring continuity of information. Without overlap, a key piece of information or a sentence that naturally bridges two concepts might be split across two distinct chunks. If a user's query matches content near such a split, only one chunk might be retrieved, leading to incomplete context. Overlap allows a segment of text to exist in two adjacent chunks, increasing the likelihood that the full context surrounding a query's match point can be retrieved.\n",
    "    2. **How does it connect to real-world tasks, problems, or applications?**\n",
    "    Consider building a RAG system for querying a company's extensive technical manuals:\n",
    "        - Setting an appropriate `chunk_size` (e.g., a few paragraphs) ensures that when a user asks about a specific error code, the retrieved chunk provides the description of that code and its immediate troubleshooting steps, not just a sentence fragment or an entire irrelevant chapter.\n",
    "        - Using `chunk_overlap` would be beneficial if, for instance, a safety warning related to a procedure is mentioned at the very end of one section (chunk A) and the procedure itself starts at the beginning of the next section (chunk B). Overlap ensures that a query about the procedure's safety aspects could potentially retrieve both the end of chunk A and the beginning of chunk B, or a chunk that fully contains the critical transition due to overlap.\n",
    "    3. **Which related techniques or areas should be studied alongside this concept?**\n",
    "        - **Advanced Text Splitters:** LangChain offers various splitters beyond `CharacterTextSplitter`, such as `RecursiveCharacterTextSplitter` (often a good default as it tries a hierarchy of separators), `TokenTextSplitter` (splits based on LLM token counts), `SentenceTransformersTokenTextSplitter`, and splitters aware of document structure (e.g., for Markdown or HTML).\n",
    "        - **Embedding Model Limitations:** Understanding the maximum sequence length (in tokens) that the chosen embedding model can process is vital for setting an upper bound on chunk size.\n",
    "        - **LLM Context Window:** The final context provided to the generative LLM (often composed of multiple retrieved chunks) must fit within its context window.\n",
    "        - **Chunking Evaluation:** Experimenting with different chunking strategies and parameters, and evaluating their impact on retrieval performance using metrics or frameworks (like RAGAS, TruLens), is often necessary to find the optimal setup for a specific dataset and use case.\n",
    "        - **Semantic Chunking:** More advanced techniques that try to divide text based on semantic shifts or topics, rather than just character/token counts or simple separators.\n",
    "\n",
    "### Code Examples\n",
    "\n",
    "The text describes using the following LangChain components and methods:\n",
    "\n",
    "1. **Imports:**\n",
    "    \n",
    "    ```python\n",
    "    from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "    from langchain.text_splitter import CharacterTextSplitter\n",
    "    \n",
    "    ```\n",
    "    \n",
    "2. **Loading Documents from Multiple Directories and Adding Metadata:**\n",
    "    \n",
    "    ```python\n",
    "    # Pseudocode/Conceptual representation based on description\n",
    "    knowledge_base_path = \"./Knowledge Base/\" # Example path\n",
    "    folders = [\"company\", \"contracts\", \"employees\", \"products\"] # Subdirectories\n",
    "    all_documents = []\n",
    "    \n",
    "    for folder_name in folders:\n",
    "        doc_type = folder_name # e.g., \"company\"\n",
    "        directory_path = f\"{knowledge_base_path}{folder_name}/\"\n",
    "    \n",
    "        # Instantiate DirectoryLoader with TextLoader for .md files (implied from context)\n",
    "        loader = DirectoryLoader(\n",
    "            path=directory_path,\n",
    "            glob=\"**/*.md\", # Assuming markdown files as per description\n",
    "            loader_cls=TextLoader,\n",
    "            show_progress=True # Optional: to see loading progress\n",
    "        )\n",
    "    \n",
    "        loaded_docs_for_folder = loader.load() # Returns a list of Document objects\n",
    "    \n",
    "        for doc in loaded_docs_for_folder:\n",
    "            doc.metadata['doctype'] = doc_type # Add custom metadata\n",
    "            all_documents.append(doc)\n",
    "    \n",
    "    print(f\"Loaded {len(all_documents)} documents.\")\n",
    "    # Example: print(all_documents[0].page_content)\n",
    "    # Example: print(all_documents[0].metadata)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "3. **Splitting Documents into Chunks:**Python\n",
    "    \n",
    "    ```python\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        chunk_size=1000,  # Target character size for each chunk\n",
    "        chunk_overlap=200   # Number of characters to overlap between chunks\n",
    "    )\n",
    "    \n",
    "    document_chunks = text_splitter.split_documents(all_documents)\n",
    "    \n",
    "    print(f\"Created {len(document_chunks)} chunks.\")\n",
    "    # Example: print(document_chunks[0].page_content)\n",
    "    # Example: print(document_chunks[0].metadata)\n",
    "    \n",
    "    ```\n",
    "    \n",
    "    *Note: The text mentions a warning about a chunk being larger than 1000 characters (e.g., 1088), illustrating LangChain's attempt to respect boundaries.*\n",
    "    \n",
    "4. **Inspecting Document Types in Chunks:**\n",
    "    \n",
    "    ```python\n",
    "    # Conceptual: How to check doctypes across chunks\n",
    "    doc_types_in_chunks = set()\n",
    "    for chunk in document_chunks:\n",
    "        if 'doctype' in chunk.metadata:\n",
    "            doc_types_in_chunks.add(chunk.metadata['doctype'])\n",
    "    # print(f\"Found doctypes in chunks: {doc_types_in_chunks}\")\n",
    "    \n",
    "    ```\n",
    "    \n",
    "5. **Searching for Keywords in Chunks (for demonstration):**\n",
    "    \n",
    "    ```python\n",
    "    # Conceptual: How to find chunks containing a specific keyword\n",
    "    chunks_with_lancaster = []\n",
    "    for chunk in document_chunks:\n",
    "        if \"Lancaster\" in chunk.page_content:\n",
    "            chunks_with_lancaster.append(chunk)\n",
    "    # print(f\"Found {len(chunks_with_lancaster)} chunks containing 'Lancaster'.\")\n",
    "    \n",
    "    ```\n",
    "    \n",
    "\n",
    "### Reflective Questions\n",
    "\n",
    "1. **Application:** If you were tasked with ingesting a knowledge base consisting of both very short FAQ entries (often just a few sentences) and long technical manuals for a RAG system, how might you adjust the `CharacterTextSplitter` parameters (`chunk_size`, `chunk_overlap`) or choose a different LangChain splitter for each type of document, and why?\n",
    "    - *Answer:* For short FAQs, I might use a very small `chunk_size` with no `chunk_overlap` to ensure each FAQ remains a distinct unit, or ideally use a sentence splitter if available to treat each question/answer pair as a chunk. For long technical manuals, I'd use a larger `chunk_size` (e.g., 500-1000 characters) and a significant `chunk_overlap` (e.g., 100-200 characters) with a `RecursiveCharacterTextSplitter` to better preserve context across paragraphs and sections while respecting natural document structure.\n",
    "2. **Teaching:** How would you explain to a junior data scientist why simply setting a `chunk_size` of 500 characters isn't always optimal and why LangChain's splitters trying to find \"sensible boundaries\" is beneficial? Use an example.\n",
    "    - *Answer:* If you rigidly cut text every 500 characters, you could easily slice a sentence—or worse, a word—in half (e.g., \"The machine requires careful maintenan...\" instead of \"maintenance\"). This makes the chunk confusing and less useful. LangChain's attempt to find \"sensible boundaries,\" like a period or paragraph break, helps keep complete thoughts or ideas within a single chunk, making the information more coherent and valuable when the LLM later uses it to answer a question.\n",
    "3. **Extension:** The text mentions adding `doctype` metadata. What other types of metadata might be useful to extract or assign to document chunks during the loading phase, and how could that metadata enhance a RAG system's capabilities beyond simple semantic retrieval?\n",
    "    - *Answer:* Other useful metadata could include creation/modification dates (for filtering by recency), specific source filenames or URLs (for precise referencing), author information, chapter/section titles (for structural context), or even pre-extracted keywords or summaries. This enhanced metadata allows for more sophisticated RAG strategies, such as filtering retrieved chunks based on date ranges or source types, prioritizing information from specific authors, or even providing the LLM with explicit structural context about the retrieved information to improve the coherence of its generated answer.\n",
    "\n",
    "# Day 2 - Preparing for Vector Databases: OpenAI Embeddings and Chroma in RAG\n",
    "\n",
    "### **Summary**\n",
    "\n",
    "This text serves as a progress checkpoint, confirming that learners now understand the role of vectors in Retrieval Augmented Generation (RAG) and can use LangChain to load, split, and add metadata to documents. The upcoming lesson will transition to converting these prepared text chunks into vector embeddings using OpenAI's embedding models and then storing and visualizing these vectors in Chroma, a popular open-source vector database.\n",
    "\n",
    "### **Highlights**\n",
    "\n",
    "- **Current RAG Skillset**: Learners are now equipped with the foundational understanding of vectors for RAG and practical skills in using LangChain for document preprocessing, including loading, splitting into chunks, and adding metadata. This prepares the groundwork for building the retrieval mechanism.\n",
    "- **Impending Vectorization**: The immediate next step in the learning journey is to convert the text chunks (created using LangChain) into numerical vector embeddings. This will be performed using OpenAI's embedding models, which are a type of \"encoding LLM.\"\n",
    "- **Introduction to Chroma Vector Database**: Following vectorization, the embeddings will be stored in Chroma, a widely-used open-source vector data store. This will also include an exercise to visualize the vectors within the database to provide a tangible understanding of their structure and relationships."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
